2020 10 13 (Tue)

    - angular_separate_trial15
        Policy: Beta
        ==== Training progress 100.00% ====
        Epoch 450
        Training loss: 0.474153, Validation loss: 0.472563
        -------------------------------------------
        | current_lr              | 4.5e-07       |
        | ent_coef                | 1.0043164     |
        | ent_coef_loss           | -0.0024440344 |
        | entropy                 | -1.5266002    |
        | episodes                | 440           |
        | fps                     | 219           |
        | mean 100 episode reward | 0.92          |
        | n_updates               | 9949          |
        | policy_loss             | 2.203872      |
        | qf1_loss                | 0.023535093   |
        | qf2_loss                | 0.0214932     |
        | time_elapsed            | 458           |
        | total timesteps         | 100511        |
        | value_loss              | 1.2701457     |
        -------------------------------------------
        -------------------------------------------
        | current_lr              | 1.75e-10      |
        | ent_coef                | 1.0040003     |
        | ent_coef_loss           | 0.00033442373 |
        | entropy                 | -0.93191975   |
        | episodes                | 2808          |
        | fps                     | 266           |
        | mean 100 episode reward | 0.78          |
        | n_updates               | 99863         |
        | policy_loss             | 24.311306     |
        | qf1_loss                | 3.518248      |
        | qf2_loss                | 3.8738937     |
        | time_elapsed            | 3746          |
        | total timesteps         | 999659        |
        | value_loss              | 0.32423455    |
        -------------------------------------------
        Num episodes:  2807 	Success rate: 82.37%
    
    - linear_separate_trial75
        Policy: Beta
        ==== Training progress 100.00% ====
        Epoch 450
        Training loss: 0.020438, Validation loss: 0.020475
        ------------------------------------------
        | current_lr              | 8.99e-08     |
        | ent_coef                | 1.0009283    |
        | ent_coef_loss           | -0.000368347 |
        | entropy                 | -1.4098951   |
        | episodes                | 256          |
        | fps                     | 278          |
        | mean 100 episode reward | 0.9          |
        | n_updates               | 9972         |
        | policy_loss             | 1.4506013    |
        | qf1_loss                | 0.0029765775 |
        | qf2_loss                | 0.0027535474 |
        | time_elapsed            | 361          |
        | total timesteps         | 100740       |
        | value_loss              | 1.2169098    |
        ------------------------------------------
        --------------------------------------------
        | current_lr              | 3.21e-11       |
        | ent_coef                | 1.0048677      |
        | ent_coef_loss           | -0.00037705468 |
        | entropy                 | -1.0982203     |
        | episodes                | 1784           |
        | fps                     | 273            |
        | mean 100 episode reward | 0.52           |
        | n_updates               | 99866          |
        | policy_loss             | 1.7835755      |
        | qf1_loss                | 0.0066446015   |
        | qf2_loss                | 0.0059192907   |
        | time_elapsed            | 3650           |
        | total timesteps         | 999683         |
        | value_loss              | 0.82774407     |
        --------------------------------------------
        Num episodes:  1783 	Success rate: 62.25%
    
    - linear_separate_trial78
        Policy: Gaussian
        ==== Training progress 100.00% ====
        Epoch 500
        Training loss: 0.045013, Validation loss: 0.045024
        -------------------------------------------
        | current_lr              | 8.99e-07      |
        | ent_coef                | 0.9973579     |
        | ent_coef_loss           | -0.0024807393 |
        | entropy                 | 1.1784866     |
        | episodes                | 104           |
        | fps                     | 391           |
        | mean 100 episode reward | 0.7           |
        | n_updates               | 9992          |
        | policy_loss             | 0.78643835    |
        | qf1_loss                | 0.0024392277  |
        | qf2_loss                | 0.0024410982  |
        | time_elapsed            | 257           |
        | total timesteps         | 100944        |
        | value_loss              | 1.6504693     |
        -------------------------------------------
        -----------------------------------------
        | current_lr              | 1.07e-09    |
        | ent_coef                | 0.9573729   |
        | ent_coef_loss           | -0.07187698 |
        | entropy                 | 1.429819    |
        | episodes                | 636         |
        | fps                     | 381         |
        | mean 100 episode reward | 0.32        |
        | n_updates               | 99791       |
        | policy_loss             | -54.583008  |
        | qf1_loss                | 0.064368464 |
        | qf2_loss                | 0.07137745  |
        | time_elapsed            | 2615        |
        | total timesteps         | 998936      |
        | value_loss              | 0.08441347  |
        -----------------------------------------
        Num episodes:  635 	Success rate: 45.67%

    - angular_adj_separate_trial2
        Policy: Beta
        -------------------------------------------
        | current_lr              | 4.96e-06      |
        | ent_coef                | 1.0029414     |
        | ent_coef_loss           | -0.0024531025 |
        | entropy                 | -1.8388793    |
        | episodes                | 52            |
        | fps                     | 131           |
        | mean 100 episode reward | 1             |
        | n_updates               | 630           |
        | policy_loss             | 1.9426062     |
        | qf1_loss                | 0.0049558384  |
        | qf2_loss                | 0.0050073643  |
        | time_elapsed            | 55            |
        | total timesteps         | 7326          |
        | value_loss              | 3.1696966     |
        -------------------------------------------
        Num episodes:  52 	Success rate: 99.99%

2020 10 14 (Wed)

    - fused_auxilary_trial32
        auxiliary: weight * 0.1
        BC: Adam
        Learning: Adam
        -------------------------------------------
        | current_lr              | 3.88e-09      |
        | ent_coef                | 1.0250524     |
        | ent_coef_loss           | -0.0076833563 |
        | entropy                 | -1.1443769    |
        | episodes                | 432           |
        | fps                     | 125           |
        | mean 100 episode reward | 38.6          |
        | n_updates               | 99123         |
        | policy_loss             | 35.268112     |
        | qf1_loss                | 0.4174384     |
        | qf2_loss                | 0.46016067    |
        | time_elapsed            | 7890          |
        | total timesteps         | 992259        |
        | value_loss              | 2.0085697     |
        -------------------------------------------
        Num episodes:  434 	Success rate: 10.14%

    - fused_auxilary_trial33
        auxiliary: weight * 0.1
        BC: RAdam
        Learning: Adam
        ------------------------------------------
        | current_lr              | 5.11e-10     |
        | ent_coef                | 1.025032     |
        | ent_coef_loss           | -0.007936968 |
        | entropy                 | -1.1574274   |
        | episodes                | 444          |
        | fps                     | 135          |
        | mean 100 episode reward | -5.49        |
        | n_updates               | 99796        |
        | policy_loss             | 29.939817    |
        | qf1_loss                | 2.7700684    |
        | qf2_loss                | 1.876455     |
        | time_elapsed            | 7391         |
        | total timesteps         | 998987       |
        | value_loss              | 1.2694768    |
        ------------------------------------------
        Num episodes:  443 	Success rate: 9.71%

    - fused_auxilary_trial34
        auxiliary: weight * 0.1
        BC: Adam
        Learning: RAdam -> WRONG!!
        -----------------------------------------
        | current_lr              | 4.27e-08    |
        | ent_coef                | 0.6641457   |
        | ent_coef_loss           | -0.16365853 |
        | entropy                 | -0.7960597  |
        | episodes                | 404         |
        | fps                     | 96          |
        | mean 100 episode reward | 9.29        |
        | n_updates               | 198090      |
        | policy_loss             | 196.75035   |
        | qf1_loss                | 0.32494348  |
        | qf2_loss                | 0.32608828  |
        | time_elapsed            | 10241       |
        | total timesteps         | 991474      |
        | value_loss              | 0.17325646  |
        -----------------------------------------
        Num episodes:  406 	Success rate: 1.97%

    - fused_auxilary_trial35
        auxiliary: weight * 0.1
        BC: RAdam
        Learning: RAdam -> WRONG!!
        {
            Used dense reward
            RAdam at Learning time WRONG
        }
        ------------------------------------------
        | current_lr              | 1.51e-08     |
        | ent_coef                | 0.8478695    |
        | ent_coef_loss           | -0.046509862 |
        | entropy                 | -0.86510396  |
        | episodes                | 412          |
        | fps                     | 97           |
        | mean 100 episode reward | 5.21         |
        | n_updates               | 198788       |
        | policy_loss             | 281.17612    |
        | qf1_loss                | 0.9335018    |
        | qf2_loss                | 0.92602193   |
        | time_elapsed            | 10194        |
        | total timesteps         | 994962       |
        | value_loss              | 0.35661218   |
        ------------------------------------------
        Num episodes:  412 	Success rate: 2.18%

    - fused_auxilary_trial36
        auxiliary: weight * 0.1
        BC: Adam
        BC lr: 5e-7        
        Learning: RAdam
        policy: Beta
        ==== Training progress 100.00% ====
        Epoch 200
        Training loss: 0.858433, Validation loss: 0.835708
        -----------------------------------------
        | current_lr              | 2.7e-06     |
        | ent_coef                | 1.006733    |
        | ent_coef_loss           | 0.000764064 |
        | entropy                 | -0.9190245  |
        | episodes                | 68          |
        | fps                     | 111         |
        | mean 100 episode reward | 0           |
        | n_updates               | 19909       |
        | policy_loss             | 92.58181    |
        | qf1_loss                | 8.5768175   |
        | qf2_loss                | 8.612754    |
        | time_elapsed            | 898         |
        | total timesteps         | 100567      |
        | value_loss              | 0.426668    |
        -----------------------------------------
        Num episodes:  67 	Success rate: 0.00%
        ------------------------------------------
        | current_lr              | 1.48e-08     |
        | ent_coef                | 0.7905688    |
        | ent_coef_loss           | -0.055340298 |
        | entropy                 | -0.8851103   |
        | episodes                | 664          |
        | fps                     | 109          |
        | mean 100 episode reward | 0            |
        | n_updates               | 198809       |
        | policy_loss             | 262.71988    |
        | qf1_loss                | 0.05949334   |
        | qf2_loss                | 0.056128595  |
        | time_elapsed            | 9085         |
        | total timesteps         | 995065       |
        | value_loss              | 0.2789746    |
        ------------------------------------------
        Num episodes:  665 	Success rate: 0.00%
    
    - fused_auxilary_trial37
        auxiliary: weight * 0.1
        BC: X
        Learning: RAdam
        policy: Beta
        ------------------------------------------
        | current_lr              | 2.71e-06     |
        | ent_coef                | 1.0181488    |
        | ent_coef_loss           | 0.0010040924 |
        | entropy                 | -0.9354493   |
        | episodes                | 72           |
        | fps                     | 109          |
        | mean 100 episode reward | 0.0563       |
        | n_updates               | 19410        |
        | policy_loss             | 102.035805   |
        | qf1_loss                | 0.18616351   |
        | qf2_loss                | 0.21022919   |
        | time_elapsed            | 893          |
        | total timesteps         | 98070        |
        | value_loss              | 0.59591925   |
        ------------------------------------------
        Num episodes:  71 	Success rate: 11.27%
        -----------------------------------------
        | current_lr              | 9.53e-09    |
        | ent_coef                | 0.79594237  |
        | ent_coef_loss           | -0.12115268 |
        | entropy                 | -0.7587354  |
        | episodes                | 692         |
        | fps                     | 109         |
        | mean 100 episode reward | -0.01       |
        | n_updates               | 199161      |
        | policy_loss             | 217.41293   |
        | qf1_loss                | 20.737486   |
        | qf2_loss                | 20.792116   |
        | time_elapsed            | 9116        |
        | total timesteps         | 996827      |
        | value_loss              | 0.34202003  |
        -----------------------------------------
        Num episodes:  693 	Success rate: 3.32%
    
    - fused_auxilary_trial38
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        Learning: RAdam
        policy: Beta
        {
            BC with RAdam, Learning with RAdam -> To see the impact of BC with RAdam
        }
        ==== Training progress 100.00% ====
        Epoch 200
        Training loss: 0.845428, Validation loss: 0.865947
        ------------------------------------------
        | current_lr              | 2.69e-06     |
        | ent_coef                | 1.0228208    |
        | ent_coef_loss           | 0.0017174174 |
        | entropy                 | -0.9610003   |
        | episodes                | 72           |
        | fps                     | 73           |
        | mean 100 episode reward | -0.127       |
        | n_updates               | 20402        |
        | policy_loss             | 116.968094   |
        | qf1_loss                | 0.07448702   |
        | qf2_loss                | 0.06764215   |
        | time_elapsed            | 1396         |
        | total timesteps         | 103031       |
        | value_loss              | 0.47029006   |
        ------------------------------------------
        Num episodes:  71 	Success rate: 0.00%
        ------------------------------------------
        | current_lr              | 4.04e-09     |
        | ent_coef                | 0.8019654    |
        | ent_coef_loss           | -0.074894674 |
        | entropy                 | -0.8374023   |
        | episodes                | 768          |
        | fps                     | 56           |
        | mean 100 episode reward | -0.43        |
        | n_updates               | 199527       |
        | policy_loss             | 253.70178    |
        | qf1_loss                | 0.045595445  |
        | qf2_loss                | 0.048193388  |
        | time_elapsed            | 17599        |
        | total timesteps         | 998658       |
        | value_loss              | 0.2787872    |
        ------------------------------------------
        Num episodes:  767 	Success rate: 0.26%

    - fused_auxilary_trial39
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        Learning: Adam
        policy: Beta
        {
            BC with RAdam, Learning with Adam -> To see the impact of BC with RAdam
        }
        ==== Training progress 100.00% ====
        Epoch 200
        Training loss: 0.861261, Validation loss: 0.855977
        -----------------------------------------
        | current_lr              | 2.7e-06     |
        | ent_coef                | 1.0222019   |
        | ent_coef_loss           | 0.002676031 |
        | entropy                 | -0.94356483 |
        | episodes                | 68          |
        | fps                     | 72          |
        | mean 100 episode reward | -0.0149     |
        | n_updates               | 19850       |
        | policy_loss             | 105.64279   |
        | qf1_loss                | 3.3865917   |
        | qf2_loss                | 3.333793    |
        | time_elapsed            | 1383        |
        | total timesteps         | 100270      |
        | value_loss              | 0.55020326  |
        -----------------------------------------
        Num episodes:  67 	Success rate: 0.00%
        -----------------------------------------
        | current_lr              | 1.54e-08    |
        | ent_coef                | 0.80188626  |
        | ent_coef_loss           | -0.04398638 |
        | entropy                 | -0.88533926 |
        | episodes                | 664         |
        | fps                     | 56          |
        | mean 100 episode reward | 0           |
        | n_updates               | 198769      |
        | policy_loss             | 274.14893   |
        | qf1_loss                | 0.09345032  |
        | qf2_loss                | 0.094446376 |
        | time_elapsed            | 17567       |
        | total timesteps         | 994866      |
        | value_loss              | 0.27756098  |
        -----------------------------------------
        Num episodes:  663 	Success rate: 0.00%
    
    - fused_auxilary_trial40
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        pre_n_epochs: 400
        Learning: Adam
        policy: Beta
        {
            More pre_n_epochs than 38, 39 -> To see the impact of less BC loss
            Learning with Adam
        }
        ==== Training progress 100.00% ====
        Epoch 400
        Training loss: 0.783282, Validation loss: 0.775527
        --------------------------------------------
        | current_lr              | 2.7e-06        |
        | ent_coef                | 0.9993473      |
        | ent_coef_loss           | -0.00013733667 |
        | entropy                 | -0.9000516     |
        | episodes                | 68             |
        | fps                     | 63             |
        | mean 100 episode reward | -0.0597        |
        | n_updates               | 19658          |
        | policy_loss             | 105.92889      |
        | qf1_loss                | 0.056913782    |
        | qf2_loss                | 0.061873272    |
        | time_elapsed            | 1564           |
        | total timesteps         | 99313          |
        | value_loss              | 0.48812574     |
        --------------------------------------------
        Num episodes:  67 	Success rate: 0.00%
        -----------------------------------------
        | current_lr              | 5.33e-09    |
        | ent_coef                | 0.78288203  |
        | ent_coef_loss           | -0.10080008 |
        | entropy                 | -0.80118126 |
        | episodes                | 712         |
        | fps                     | 56          |
        | mean 100 episode reward | -0.14       |
        | n_updates               | 199441      |
        | policy_loss             | 238.66019   |
        | qf1_loss                | 0.030632451 |
        | qf2_loss                | 0.03212556  |
        | time_elapsed            | 17716       |
        | total timesteps         | 998225      |
        | value_loss              | 0.2767639   |
        -----------------------------------------
        Num episodes:  711 	Success rate: 0.14%

    - fused_auxilary_trial41
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        pre_n_epochs: 400
        Learning: RAdam
        policy: Beta
        {
            More pre_n_epochs than 38, 39 -> To see the impact of less BC loss
            Learning with RAdam
        }
        ==== Training progress 100.00% ====
        Epoch 400
        Training loss: 0.772019, Validation loss: 0.765919
        ------------------------------------------
        | current_lr              | 2.69e-06     |
        | ent_coef                | 1.0113238    |
        | ent_coef_loss           | 0.0027768628 |
        | entropy                 | -0.90546006  |
        | episodes                | 72           |
        | fps                     | 63           |
        | mean 100 episode reward | -0.155       |
        | n_updates               | 20139        |
        | policy_loss             | 97.00701     |
        | qf1_loss                | 7.186699     |
        | qf2_loss                | 7.1670003    |
        | time_elapsed            | 1611         |
        | total timesteps         | 101718       |
        | value_loss              | 0.52501      |
        ------------------------------------------
        Num episodes:  71 	Success rate: 0.00%
        -----------------------------------------
        | current_lr              | 6.27e-09    |
        | ent_coef                | 0.79275066  |
        | ent_coef_loss           | -0.0870548  |
        | entropy                 | -0.799674   |
        | episodes                | 692         |
        | fps                     | 56          |
        | mean 100 episode reward | -0.23       |
        | n_updates               | 199378      |
        | policy_loss             | 241.45258   |
        | qf1_loss                | 0.081505075 |
        | qf2_loss                | 0.07650563  |
        | time_elapsed            | 17720       |
        | total timesteps         | 997913      |
        | value_loss              | 0.2572199   |
        -----------------------------------------
        Num episodes:  691 	Success rate: 0.00%
    
    - fused_auxilary_trial42
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        pre_n_epochs: 400
        Learning: RAdam
        policy: Beta
        {
            Use dense reward: 
                reward = 1/(l+0.5) * 0.002 + (np.pi+2)/(angle_diff+1) * 0.001
        }
        ==== Training progress 100.00% ====
        Epoch 400
        Training loss: 0.786143, Validation loss: 0.782892
        -------------------------------------------
        | current_lr              | 2.69e-06      |
        | ent_coef                | 0.9994635     |
        | ent_coef_loss           | -6.177092e-05 |
        | entropy                 | -0.9271079    |
        | episodes                | 72            |
        | fps                     | 58            |
        | mean 100 episode reward | -6.83         |
        | n_updates               | 20315         |
        | policy_loss             | 108.76815     |
        | qf1_loss                | 0.8196725     |
        | qf2_loss                | 0.8400145     |
        | time_elapsed            | 1752          |
        | total timesteps         | 102597        |
        | value_loss              | 0.42523885    |
        -------------------------------------------
        Num episodes:  71 	Success rate: 1.41%
        ------------------------------------------
        | current_lr              | 8.37e-09     |
        | ent_coef                | 0.7835367    |
        | ent_coef_loss           | -0.117379844 |
        | entropy                 | -0.7436243   |
        | episodes                | 680          |
        | fps                     | 57           |
        | mean 100 episode reward | 5.46         |
        | n_updates               | 199238       |
        | policy_loss             | 214.71423    |
        | qf1_loss                | 28.67083     |
        | qf2_loss                | 28.660355    |
        | time_elapsed            | 17219        |
        | total timesteps         | 997212       |
        | value_loss              | 0.24159686   |
        ------------------------------------------
        Num episodes:  679 	Success rate: 0.59%
        Problem:
            {
                Too much dependency on aux networks @ training time.
                -> No good result obtained during those stages
            }

    - fused_auxilary_trial43
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        pre_n_epochs: 1000
        Learning: RAdam
        policy: Beta
        {
            Dense reward with even more BC epochs
        }
        ==== Training progress 100.00% ====
        Epoch 1000
        Training loss: 0.630477, Validation loss: 0.634888
        -------------------------------------------
        | current_lr              | 2.7e-06       |
        | ent_coef                | 0.9906271     |
        | ent_coef_loss           | -0.0032340572 |
        | entropy                 | -0.83602786   |
        | episodes                | 68            |
        | fps                     | 49            |
        | mean 100 episode reward | 1.74          |
        | n_updates               | 19506         |
        | policy_loss             | 86.10309      |
        | qf1_loss                | 0.7904922     |
        | qf2_loss                | 0.80388767    |
        | time_elapsed            | 2000          |
        | total timesteps         | 98550         |
        | value_loss              | 0.5235663     |
        -------------------------------------------
        Num episodes:  67 	Success rate: 1.49%
        -----------------------------------------
        | current_lr              | 1.35e-08    |
        | ent_coef                | 0.77580905  |
        | ent_coef_loss           | -0.11882399 |
        | entropy                 | -0.75410104 |
        | episodes                | 680         |
        | fps                     | 106         |
        | mean 100 episode reward | -1.69       |
        | n_updates               | 198896      |
        | policy_loss             | 227.53873   |
        | qf1_loss                | 33.263424   |
        | qf2_loss                | 33.24225    |
        | time_elapsed            | 9379        |
        | total timesteps         | 995502      |
        | value_loss              | 0.2273885   |
        -----------------------------------------
        Num episodes:  679 	Success rate: 0.29%


2020 10 15 (Thu)

    - Linear primitive, baseline acc: 50%
    - Tried State-Action coupling by estimating the next state from the last layers of policy given state and action.
    - 1. Predict: x, y, phi with action: lin from policy + ang from noise
        ------------------------------------------
        | current_lr              | 7.49e-09     |
        | ent_coef                | 0.76749134   |
        | ent_coef_loss           | -0.012358234 |
        | entropy                 | -0.94481575  |
        | episodes                | 1664         |
        | fps                     | 132          |
        | mean 100 episode reward | 0.1          |
        | n_updates               | 199297       |
        | policy_loss             | 164.76471    |
        | qf1_loss                | 0.06117493   |
        | qf2_loss                | 0.058522306  |
        | time_elapsed            | 7551         |
        | total timesteps         | 997507       |
        | value_loss              | 0.12089803   |
        ------------------------------------------
        Num episodes:  1666 	Success rate: 50.84%
        sa coupler loss:  0.007889251
        obs:		 [ 1.41860894 -4.11809422  2.62856023 -0.13740125  0.07864352  1.41075706]
        GT next obs:	 [ 1.42523083 -4.12182463  2.62958227]
        Pred next obs:	 [ 1.3439953 -3.9926527  2.4744987]
        sa coupler loss:  0.0041199396
        obs:		 [ 1.92841527 -4.4100945   2.61833652 -0.13740125  0.07864352  1.41075706]
        GT next obs:	 [ 1.93334118 -4.41293623  2.61680923]
        Pred next obs:	 [ 1.8876604 -4.309787   2.507296 ]
        action: -0.278 , mode: 0.178 , mu: 0.218 , std: 0.10000 , alpha: 3.511 , beta: 12.561
        sa coupler loss:  0.0016889866
        obs:		 [ 2.397715   -4.67849928  2.62366218 -0.13740125  0.07864352  1.41075706]
        GT next obs:	 [ 2.40013436 -4.67987788  2.62440659]
        Pred next obs:	 [ 2.384603  -4.606096   2.5577064]
        sa coupler loss:  0.0004198793
        obs:		 [ 2.86901036 -4.9481978   2.61879649 -0.13740125  0.07864352  1.41075706]
        GT next obs:	 [ 2.8745766  -4.95140551  2.61860181]
        Pred next obs:	 [ 2.8772616 -4.906147   2.5970674]
        -> low sa coupler loss at last (sudden?) but, 10e-5 ~ 10e-6 till near 1500 episodes
        -> Tested, always heading backward. alpha ~ 2.**, beta ~ 12.**
    - 2-1. Predict: x, y with action: lin from policy (lr: lr)
    - 2-2. Predict: x, y with action: lin from policy (lr: lr * 5)
    - 3. Predict: del x, del y with action: lin from policy (lr: lr * 5)
        -> Not a good idea, since what we'd prefer our policy to have a good understanding of the current state.
         By reducing the state error given current state may result in more accurate estimation of the increment, but will less likely
         to adjust policy parameters to the current state.
    - 4. Predict: x, y with action: lin from policy (lr: lr * 5), update SAC grad with half the frequency of sa step
         (changed from 1 -> int(freq/2))
    
    Always heading backward. alpha ~ 2.**, beta ~ 12.**
    All of the above achieved success rate of near 50% -> action always head backward. Need to fix it

    - 5. Maybe lack of exploration? try with more samplings + random exploration
    - N. How about training with Alpha/Beta directly? ... really??
    - 7. How about mu with tanh, sigma with sigmoid? -> NEVER
    - 8. Adjust mu value (median mapped to 0.004 - Problem!) -> will it attenuate the problem?
    - 9. How about adding BC?
    - 10. Is it underfitted? Do we need more parameters for the policy?
        - deeper layer?
        - wider layer?
        -> No... lr too small, suboptimal...
    - 11. SA coupling results in biased action?
        -> Nope, with/without SA coupling resulted in biased action in absolute obs
    - 12. MU BIASING FIGURED OUT: sieving nan value of logp_pi to 1 results in bias.
        -> Thought I figured out, but actually not. Cartpole with/without sieving performed nicely.
        -> Relative obs does not show the biased actions...
    - 13. training with high lr 5e-4 (without SA coup, absolute obs)
    - 14. training with high lr 5e-4 (with SA coup, absolute obs) - exploded
    - 15. learning with 5e-4, but SA coupling with 1e-6
    - 16. training with lr 5e-5 (with SA coup, absolute obs)
    - 17. training with lr 5e-5, SA coup lr 5e-6
    - 18. gradient flow to var
    - 19. gradient flow to var + SA coup update same with SAC update
    - 20. gradient flow to var + SA coup update same with SAC update + additional wide layer added to the last level of the policy network
    - 21. SA coup update same with SAC update + additional wide layer added to the last level of the policy network
    - 22. With even deeper network + SA coup update(with sumed) before SAC update, 
            + additional layer added to SAcoupler, and the last layer has same # of neurons with coupled state to compensate the roll of action better
            + dense reward
        - fused_separate_trial79
            -----------------------------------------
            | current_lr              | 4.49e-05    |
            | ent_coef                | 0.63408554  |
            | ent_coef_loss           | -0.04201517 |
            | entropy                 | -0.8960402  |
            | episodes                | 96          |
            | fps                     | 102         |
            | mean 100 episode reward | 35.8        |
            | n_updates               | 10115       |
            | policy_loss             | 28.948477   |
            | qf1_loss                | 0.34185708  |
            | qf2_loss                | 0.34412962  |
            | time_elapsed            | 999         |
            | total timesteps         | 102177      |
            | value_loss              | 0.14038643  |
            -----------------------------------------
            Num episodes:  95 	Success rate: 28.42%
            ------------------------------------------
            | current_lr              | 1.57e-07     |
            | ent_coef                | 0.09099196   |
            | ent_coef_loss           | -0.15502071  |
            | entropy                 | -0.93784124  |
            | episodes                | 1124         |
            | fps                     | 131          |
            | mean 100 episode reward | 50.8         |
            | n_updates               | 99584        |
            | policy_loss             | 22.21339     |
            | qf1_loss                | 0.0167036    |
            | qf2_loss                | 0.014223535  |
            | time_elapsed            | 7582         |
            | total timesteps         | 996862       |
            | value_loss              | 0.0034751603 |
            ------------------------------------------
            Num episodes:  1126 	Success rate: 47.07%
        {
            SA coupler: Useless -> Didn't converge to a meaningful level.
            Movement shown at the starting phase (Wheter correct or not)
                -> as time goes, it reduces its speed
        }
    - 23. " + sparse reward + SA coup update with mean
        - fused_separate_trial80
            ------------------------------------------
            | current_lr              | 4.49e-05     |
            | ent_coef                | 0.6358355    |
            | ent_coef_loss           | -0.042796638 |
            | entropy                 | -0.88639617  |
            | episodes                | 76           |
            | fps                     | 94           |
            | mean 100 episode reward | 0.16         |
            | n_updates               | 10023        |
            | policy_loss             | 27.685257    |
            | qf1_loss                | 0.0008432417 |
            | qf2_loss                | 0.0010583058 |
            | time_elapsed            | 1074         |
            | total timesteps         | 101256       |
            | value_loss              | 0.08373366   |
            ------------------------------------------
            Num episodes:  75 	Success rate: 17.33%
            -------------------------------------------
            | current_lr              | 4.66e-08      |
            | ent_coef                | 0.088483095   |
            | ent_coef_loss           | -0.1801542    |
            | entropy                 | -0.9159876    |
            | episodes                | 1060          |
            | fps                     | 136           |
            | mean 100 episode reward | 0.07          |
            | n_updates               | 99805         |
            | policy_loss             | 19.760448     |
            | qf1_loss                | 0.00039794482 |
            | qf2_loss                | 0.00028243283 |
            | time_elapsed            | 7332          |
            | total timesteps         | 999073        |
            | value_loss              | 0.001683588   |
            -------------------------------------------
            Num episodes:  1060 	Success rate: 38.30%
        {
            SA coupler also useless.
            Always moves forward.
            Policy loss decreasing - Maybe a bit more training?
        }
    - 24. " + SA coup updates more frequently than SAC. 
        - fused_separate_trial81
            ------------------------------------------
            | current_lr              | 4.48e-05     |
            | ent_coef                | 0.63078946   |
            | ent_coef_loss           | -0.05352938  |
            | entropy                 | -0.886851    |
            | episodes                | 76           |
            | fps                     | 64           |
            | mean 100 episode reward | 0.04         |
            | n_updates               | 10275        |
            | policy_loss             | 28.018255    |
            | qf1_loss                | 0.0011327865 |
            | qf2_loss                | 0.0012332982 |
            | time_elapsed            | 1612         |
            | total timesteps         | 103771       |
            | value_loss              | 0.07914214   |
            ------------------------------------------
            Num episodes:  75 	Success rate: 10.67%
            -------------------------------------------
            | current_lr              | 9.35e-08      |
            | ent_coef                | 0.088621005   |
            | ent_coef_loss           | -0.25222063   |
            | entropy                 | -0.9065951    |
            | episodes                | 1020          |
            | fps                     | 109           |
            | mean 100 episode reward | 0.19          |
            | n_updates               | 99711         |
            | policy_loss             | 20.885864     |
            | qf1_loss                | 0.0003092753  |
            | qf2_loss                | 0.00038747277 |
            | time_elapsed            | 9082          |
            | total timesteps         | 998137        |
            | value_loss              | 0.0018869314  |
            -------------------------------------------
            Num episodes:  1020 	Success rate: 38.73%
        {
            SA coupling doesn't seem to be useful as lr decreases to near 1e-9, and still not converging.
            Looks like a bit biased forwardly, but not exactly.
        }
    -25. logp_pi multiple samples from state
        - fused_separate_trial82
            ------------------------------------------
            | current_lr              | 4.5e-05      |
            | ent_coef                | 0.6379396    |
            | ent_coef_loss           | -0.048355225 |
            | entropy                 | -0.8865482   |
            | episodes                | 80           |
            | fps                     | 83           |
            | mean 100 episode reward | 0.253        |
            | n_updates               | 9854         |
            | policy_loss             | 27.669382    |
            | qf1_loss                | 0.0016267912 |
            | qf2_loss                | 0.001808959  |
            | time_elapsed            | 1187         |
            | total timesteps         | 99561        |
            | value_loss              | 0.016244117  |
            ------------------------------------------
            Num episodes:  79 	Success rate: 26.58%
            -------------------------------------------
            | current_lr              | 7.71e-08      |
            | ent_coef                | 0.084628955   |
            | ent_coef_loss           | -0.23121975   |
            | entropy                 | -0.9044304    |
            | episodes                | 1052          |
            | fps                     | 152           |
            | mean 100 episode reward | 0.3           |
            | n_updates               | 99744         |
            | policy_loss             | 20.096495     |
            | qf1_loss                | 0.16304855    |
            | qf2_loss                | 0.16253597    |
            | time_elapsed            | 6557          |
            | total timesteps         | 998463        |
            | value_loss              | 0.00016107864 |
            -------------------------------------------
            Num episodes:  1052 	Success rate: 40.97%
        {
            SA coupler doesn't seem to converge as well.
            Forward biased. WHY!!!!
        }
    
    Q. Why is std always fixed to the maximum value?

2020 10 16 (Fri)
    -26. Is the depth/width of the policy network short to represent state-action mapping?
        Q. Can policy network properly represent the received state signal?
            Test1. Deepen the SA Coupler to give sufficient rooms for latent features of the last policy layer to be fitted to the state estimation.
                - fused_separate_trial83

            Test2. See if SA Coupler can estimate next state given current state and action.
                1. RL fashion - Estimate the effect of action given state from latent vector (No need to estimate the state, but only the residual value)
                -> fused_separate_trial84

                2. No policy fashion - Get rid of the policy network (Receive info from state directly)
                -> fused_separate_trial85

                
