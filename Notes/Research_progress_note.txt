2020 10 13 (Tue)

    - angular_separate_trial15
        Policy: Beta
        ==== Training progress 100.00% ====
        Epoch 450
        Training loss: 0.474153, Validation loss: 0.472563
        -------------------------------------------
        | current_lr              | 4.5e-07       |
        | ent_coef                | 1.0043164     |
        | ent_coef_loss           | -0.0024440344 |
        | entropy                 | -1.5266002    |
        | episodes                | 440           |
        | fps                     | 219           |
        | mean 100 episode reward | 0.92          |
        | n_updates               | 9949          |
        | policy_loss             | 2.203872      |
        | qf1_loss                | 0.023535093   |
        | qf2_loss                | 0.0214932     |
        | time_elapsed            | 458           |
        | total timesteps         | 100511        |
        | value_loss              | 1.2701457     |
        -------------------------------------------
        -------------------------------------------
        | current_lr              | 1.75e-10      |
        | ent_coef                | 1.0040003     |
        | ent_coef_loss           | 0.00033442373 |
        | entropy                 | -0.93191975   |
        | episodes                | 2808          |
        | fps                     | 266           |
        | mean 100 episode reward | 0.78          |
        | n_updates               | 99863         |
        | policy_loss             | 24.311306     |
        | qf1_loss                | 3.518248      |
        | qf2_loss                | 3.8738937     |
        | time_elapsed            | 3746          |
        | total timesteps         | 999659        |
        | value_loss              | 0.32423455    |
        -------------------------------------------
        Num episodes:  2807 	Success rate: 82.37%
    
    - linear_separate_trial75
        Policy: Beta
        ==== Training progress 100.00% ====
        Epoch 450
        Training loss: 0.020438, Validation loss: 0.020475
        ------------------------------------------
        | current_lr              | 8.99e-08     |
        | ent_coef                | 1.0009283    |
        | ent_coef_loss           | -0.000368347 |
        | entropy                 | -1.4098951   |
        | episodes                | 256          |
        | fps                     | 278          |
        | mean 100 episode reward | 0.9          |
        | n_updates               | 9972         |
        | policy_loss             | 1.4506013    |
        | qf1_loss                | 0.0029765775 |
        | qf2_loss                | 0.0027535474 |
        | time_elapsed            | 361          |
        | total timesteps         | 100740       |
        | value_loss              | 1.2169098    |
        ------------------------------------------
        --------------------------------------------
        | current_lr              | 3.21e-11       |
        | ent_coef                | 1.0048677      |
        | ent_coef_loss           | -0.00037705468 |
        | entropy                 | -1.0982203     |
        | episodes                | 1784           |
        | fps                     | 273            |
        | mean 100 episode reward | 0.52           |
        | n_updates               | 99866          |
        | policy_loss             | 1.7835755      |
        | qf1_loss                | 0.0066446015   |
        | qf2_loss                | 0.0059192907   |
        | time_elapsed            | 3650           |
        | total timesteps         | 999683         |
        | value_loss              | 0.82774407     |
        --------------------------------------------
        Num episodes:  1783 	Success rate: 62.25%
    
    - linear_separate_trial78
        Policy: Gaussian
        ==== Training progress 100.00% ====
        Epoch 500
        Training loss: 0.045013, Validation loss: 0.045024
        -------------------------------------------
        | current_lr              | 8.99e-07      |
        | ent_coef                | 0.9973579     |
        | ent_coef_loss           | -0.0024807393 |
        | entropy                 | 1.1784866     |
        | episodes                | 104           |
        | fps                     | 391           |
        | mean 100 episode reward | 0.7           |
        | n_updates               | 9992          |
        | policy_loss             | 0.78643835    |
        | qf1_loss                | 0.0024392277  |
        | qf2_loss                | 0.0024410982  |
        | time_elapsed            | 257           |
        | total timesteps         | 100944        |
        | value_loss              | 1.6504693     |
        -------------------------------------------
        -----------------------------------------
        | current_lr              | 1.07e-09    |
        | ent_coef                | 0.9573729   |
        | ent_coef_loss           | -0.07187698 |
        | entropy                 | 1.429819    |
        | episodes                | 636         |
        | fps                     | 381         |
        | mean 100 episode reward | 0.32        |
        | n_updates               | 99791       |
        | policy_loss             | -54.583008  |
        | qf1_loss                | 0.064368464 |
        | qf2_loss                | 0.07137745  |
        | time_elapsed            | 2615        |
        | total timesteps         | 998936      |
        | value_loss              | 0.08441347  |
        -----------------------------------------
        Num episodes:  635 	Success rate: 45.67%

    - angular_adj_separate_trial2
        Policy: Beta
        -------------------------------------------
        | current_lr              | 4.96e-06      |
        | ent_coef                | 1.0029414     |
        | ent_coef_loss           | -0.0024531025 |
        | entropy                 | -1.8388793    |
        | episodes                | 52            |
        | fps                     | 131           |
        | mean 100 episode reward | 1             |
        | n_updates               | 630           |
        | policy_loss             | 1.9426062     |
        | qf1_loss                | 0.0049558384  |
        | qf2_loss                | 0.0050073643  |
        | time_elapsed            | 55            |
        | total timesteps         | 7326          |
        | value_loss              | 3.1696966     |
        -------------------------------------------
        Num episodes:  52 	Success rate: 99.99%

2020 10 14 (Wed)

    - fused_auxilary_trial32
        auxiliary: weight * 0.1
        BC: Adam
        Learning: Adam
        -------------------------------------------
        | current_lr              | 3.88e-09      |
        | ent_coef                | 1.0250524     |
        | ent_coef_loss           | -0.0076833563 |
        | entropy                 | -1.1443769    |
        | episodes                | 432           |
        | fps                     | 125           |
        | mean 100 episode reward | 38.6          |
        | n_updates               | 99123         |
        | policy_loss             | 35.268112     |
        | qf1_loss                | 0.4174384     |
        | qf2_loss                | 0.46016067    |
        | time_elapsed            | 7890          |
        | total timesteps         | 992259        |
        | value_loss              | 2.0085697     |
        -------------------------------------------
        Num episodes:  434 	Success rate: 10.14%

    - fused_auxilary_trial33
        auxiliary: weight * 0.1
        BC: RAdam
        Learning: Adam
        ------------------------------------------
        | current_lr              | 5.11e-10     |
        | ent_coef                | 1.025032     |
        | ent_coef_loss           | -0.007936968 |
        | entropy                 | -1.1574274   |
        | episodes                | 444          |
        | fps                     | 135          |
        | mean 100 episode reward | -5.49        |
        | n_updates               | 99796        |
        | policy_loss             | 29.939817    |
        | qf1_loss                | 2.7700684    |
        | qf2_loss                | 1.876455     |
        | time_elapsed            | 7391         |
        | total timesteps         | 998987       |
        | value_loss              | 1.2694768    |
        ------------------------------------------
        Num episodes:  443 	Success rate: 9.71%

    - fused_auxilary_trial34
        auxiliary: weight * 0.1
        BC: Adam
        Learning: RAdam -> WRONG!!
        -----------------------------------------
        | current_lr              | 4.27e-08    |
        | ent_coef                | 0.6641457   |
        | ent_coef_loss           | -0.16365853 |
        | entropy                 | -0.7960597  |
        | episodes                | 404         |
        | fps                     | 96          |
        | mean 100 episode reward | 9.29        |
        | n_updates               | 198090      |
        | policy_loss             | 196.75035   |
        | qf1_loss                | 0.32494348  |
        | qf2_loss                | 0.32608828  |
        | time_elapsed            | 10241       |
        | total timesteps         | 991474      |
        | value_loss              | 0.17325646  |
        -----------------------------------------
        Num episodes:  406 	Success rate: 1.97%

    - fused_auxilary_trial35
        auxiliary: weight * 0.1
        BC: RAdam
        Learning: RAdam -> WRONG!!
        {
            Used dense reward
            RAdam at Learning time WRONG
        }
        ------------------------------------------
        | current_lr              | 1.51e-08     |
        | ent_coef                | 0.8478695    |
        | ent_coef_loss           | -0.046509862 |
        | entropy                 | -0.86510396  |
        | episodes                | 412          |
        | fps                     | 97           |
        | mean 100 episode reward | 5.21         |
        | n_updates               | 198788       |
        | policy_loss             | 281.17612    |
        | qf1_loss                | 0.9335018    |
        | qf2_loss                | 0.92602193   |
        | time_elapsed            | 10194        |
        | total timesteps         | 994962       |
        | value_loss              | 0.35661218   |
        ------------------------------------------
        Num episodes:  412 	Success rate: 2.18%

    - fused_auxilary_trial36
        auxiliary: weight * 0.1
        BC: Adam
        BC lr: 5e-7        
        Learning: RAdam
        policy: Beta
        ==== Training progress 100.00% ====
        Epoch 200
        Training loss: 0.858433, Validation loss: 0.835708
        -----------------------------------------
        | current_lr              | 2.7e-06     |
        | ent_coef                | 1.006733    |
        | ent_coef_loss           | 0.000764064 |
        | entropy                 | -0.9190245  |
        | episodes                | 68          |
        | fps                     | 111         |
        | mean 100 episode reward | 0           |
        | n_updates               | 19909       |
        | policy_loss             | 92.58181    |
        | qf1_loss                | 8.5768175   |
        | qf2_loss                | 8.612754    |
        | time_elapsed            | 898         |
        | total timesteps         | 100567      |
        | value_loss              | 0.426668    |
        -----------------------------------------
        Num episodes:  67 	Success rate: 0.00%
        ------------------------------------------
        | current_lr              | 1.48e-08     |
        | ent_coef                | 0.7905688    |
        | ent_coef_loss           | -0.055340298 |
        | entropy                 | -0.8851103   |
        | episodes                | 664          |
        | fps                     | 109          |
        | mean 100 episode reward | 0            |
        | n_updates               | 198809       |
        | policy_loss             | 262.71988    |
        | qf1_loss                | 0.05949334   |
        | qf2_loss                | 0.056128595  |
        | time_elapsed            | 9085         |
        | total timesteps         | 995065       |
        | value_loss              | 0.2789746    |
        ------------------------------------------
        Num episodes:  665 	Success rate: 0.00%
    
    - fused_auxilary_trial37
        auxiliary: weight * 0.1
        BC: X
        Learning: RAdam
        policy: Beta
        ------------------------------------------
        | current_lr              | 2.71e-06     |
        | ent_coef                | 1.0181488    |
        | ent_coef_loss           | 0.0010040924 |
        | entropy                 | -0.9354493   |
        | episodes                | 72           |
        | fps                     | 109          |
        | mean 100 episode reward | 0.0563       |
        | n_updates               | 19410        |
        | policy_loss             | 102.035805   |
        | qf1_loss                | 0.18616351   |
        | qf2_loss                | 0.21022919   |
        | time_elapsed            | 893          |
        | total timesteps         | 98070        |
        | value_loss              | 0.59591925   |
        ------------------------------------------
        Num episodes:  71 	Success rate: 11.27%
        -----------------------------------------
        | current_lr              | 9.53e-09    |
        | ent_coef                | 0.79594237  |
        | ent_coef_loss           | -0.12115268 |
        | entropy                 | -0.7587354  |
        | episodes                | 692         |
        | fps                     | 109         |
        | mean 100 episode reward | -0.01       |
        | n_updates               | 199161      |
        | policy_loss             | 217.41293   |
        | qf1_loss                | 20.737486   |
        | qf2_loss                | 20.792116   |
        | time_elapsed            | 9116        |
        | total timesteps         | 996827      |
        | value_loss              | 0.34202003  |
        -----------------------------------------
        Num episodes:  693 	Success rate: 3.32%
    
    - fused_auxilary_trial38
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        Learning: RAdam
        policy: Beta
        {
            BC with RAdam, Learning with RAdam -> To see the impact of BC with RAdam
        }
        ==== Training progress 100.00% ====
        Epoch 200
        Training loss: 0.845428, Validation loss: 0.865947
        ------------------------------------------
        | current_lr              | 2.69e-06     |
        | ent_coef                | 1.0228208    |
        | ent_coef_loss           | 0.0017174174 |
        | entropy                 | -0.9610003   |
        | episodes                | 72           |
        | fps                     | 73           |
        | mean 100 episode reward | -0.127       |
        | n_updates               | 20402        |
        | policy_loss             | 116.968094   |
        | qf1_loss                | 0.07448702   |
        | qf2_loss                | 0.06764215   |
        | time_elapsed            | 1396         |
        | total timesteps         | 103031       |
        | value_loss              | 0.47029006   |
        ------------------------------------------
        Num episodes:  71 	Success rate: 0.00%
        ------------------------------------------
        | current_lr              | 4.04e-09     |
        | ent_coef                | 0.8019654    |
        | ent_coef_loss           | -0.074894674 |
        | entropy                 | -0.8374023   |
        | episodes                | 768          |
        | fps                     | 56           |
        | mean 100 episode reward | -0.43        |
        | n_updates               | 199527       |
        | policy_loss             | 253.70178    |
        | qf1_loss                | 0.045595445  |
        | qf2_loss                | 0.048193388  |
        | time_elapsed            | 17599        |
        | total timesteps         | 998658       |
        | value_loss              | 0.2787872    |
        ------------------------------------------
        Num episodes:  767 	Success rate: 0.26%

    - fused_auxilary_trial39
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        Learning: Adam
        policy: Beta
        {
            BC with RAdam, Learning with Adam -> To see the impact of BC with RAdam
        }
        ==== Training progress 100.00% ====
        Epoch 200
        Training loss: 0.861261, Validation loss: 0.855977
        -----------------------------------------
        | current_lr              | 2.7e-06     |
        | ent_coef                | 1.0222019   |
        | ent_coef_loss           | 0.002676031 |
        | entropy                 | -0.94356483 |
        | episodes                | 68          |
        | fps                     | 72          |
        | mean 100 episode reward | -0.0149     |
        | n_updates               | 19850       |
        | policy_loss             | 105.64279   |
        | qf1_loss                | 3.3865917   |
        | qf2_loss                | 3.333793    |
        | time_elapsed            | 1383        |
        | total timesteps         | 100270      |
        | value_loss              | 0.55020326  |
        -----------------------------------------
        Num episodes:  67 	Success rate: 0.00%
        -----------------------------------------
        | current_lr              | 1.54e-08    |
        | ent_coef                | 0.80188626  |
        | ent_coef_loss           | -0.04398638 |
        | entropy                 | -0.88533926 |
        | episodes                | 664         |
        | fps                     | 56          |
        | mean 100 episode reward | 0           |
        | n_updates               | 198769      |
        | policy_loss             | 274.14893   |
        | qf1_loss                | 0.09345032  |
        | qf2_loss                | 0.094446376 |
        | time_elapsed            | 17567       |
        | total timesteps         | 994866      |
        | value_loss              | 0.27756098  |
        -----------------------------------------
        Num episodes:  663 	Success rate: 0.00%
    
    - fused_auxilary_trial40
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        pre_n_epochs: 400
        Learning: Adam
        policy: Beta
        {
            More pre_n_epochs than 38, 39 -> To see the impact of less BC loss
            Learning with Adam
        }
        ==== Training progress 100.00% ====
        Epoch 400
        Training loss: 0.783282, Validation loss: 0.775527
        --------------------------------------------
        | current_lr              | 2.7e-06        |
        | ent_coef                | 0.9993473      |
        | ent_coef_loss           | -0.00013733667 |
        | entropy                 | -0.9000516     |
        | episodes                | 68             |
        | fps                     | 63             |
        | mean 100 episode reward | -0.0597        |
        | n_updates               | 19658          |
        | policy_loss             | 105.92889      |
        | qf1_loss                | 0.056913782    |
        | qf2_loss                | 0.061873272    |
        | time_elapsed            | 1564           |
        | total timesteps         | 99313          |
        | value_loss              | 0.48812574     |
        --------------------------------------------
        Num episodes:  67 	Success rate: 0.00%
        -----------------------------------------
        | current_lr              | 5.33e-09    |
        | ent_coef                | 0.78288203  |
        | ent_coef_loss           | -0.10080008 |
        | entropy                 | -0.80118126 |
        | episodes                | 712         |
        | fps                     | 56          |
        | mean 100 episode reward | -0.14       |
        | n_updates               | 199441      |
        | policy_loss             | 238.66019   |
        | qf1_loss                | 0.030632451 |
        | qf2_loss                | 0.03212556  |
        | time_elapsed            | 17716       |
        | total timesteps         | 998225      |
        | value_loss              | 0.2767639   |
        -----------------------------------------
        Num episodes:  711 	Success rate: 0.14%

    - fused_auxilary_trial41
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        pre_n_epochs: 400
        Learning: RAdam
        policy: Beta
        {
            More pre_n_epochs than 38, 39 -> To see the impact of less BC loss
            Learning with RAdam
        }
        ==== Training progress 100.00% ====
        Epoch 400
        Training loss: 0.772019, Validation loss: 0.765919
        ------------------------------------------
        | current_lr              | 2.69e-06     |
        | ent_coef                | 1.0113238    |
        | ent_coef_loss           | 0.0027768628 |
        | entropy                 | -0.90546006  |
        | episodes                | 72           |
        | fps                     | 63           |
        | mean 100 episode reward | -0.155       |
        | n_updates               | 20139        |
        | policy_loss             | 97.00701     |
        | qf1_loss                | 7.186699     |
        | qf2_loss                | 7.1670003    |
        | time_elapsed            | 1611         |
        | total timesteps         | 101718       |
        | value_loss              | 0.52501      |
        ------------------------------------------
        Num episodes:  71 	Success rate: 0.00%
        -----------------------------------------
        | current_lr              | 6.27e-09    |
        | ent_coef                | 0.79275066  |
        | ent_coef_loss           | -0.0870548  |
        | entropy                 | -0.799674   |
        | episodes                | 692         |
        | fps                     | 56          |
        | mean 100 episode reward | -0.23       |
        | n_updates               | 199378      |
        | policy_loss             | 241.45258   |
        | qf1_loss                | 0.081505075 |
        | qf2_loss                | 0.07650563  |
        | time_elapsed            | 17720       |
        | total timesteps         | 997913      |
        | value_loss              | 0.2572199   |
        -----------------------------------------
        Num episodes:  691 	Success rate: 0.00%
    
    - fused_auxilary_trial42
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        pre_n_epochs: 400
        Learning: RAdam
        policy: Beta
        {
            Use dense reward: 
                reward = 1/(l+0.5) * 0.002 + (np.pi+2)/(angle_diff+1) * 0.001
        }
        ==== Training progress 100.00% ====
        Epoch 400
        Training loss: 0.786143, Validation loss: 0.782892
        -------------------------------------------
        | current_lr              | 2.69e-06      |
        | ent_coef                | 0.9994635     |
        | ent_coef_loss           | -6.177092e-05 |
        | entropy                 | -0.9271079    |
        | episodes                | 72            |
        | fps                     | 58            |
        | mean 100 episode reward | -6.83         |
        | n_updates               | 20315         |
        | policy_loss             | 108.76815     |
        | qf1_loss                | 0.8196725     |
        | qf2_loss                | 0.8400145     |
        | time_elapsed            | 1752          |
        | total timesteps         | 102597        |
        | value_loss              | 0.42523885    |
        -------------------------------------------
        Num episodes:  71 	Success rate: 1.41%
        ------------------------------------------
        | current_lr              | 8.37e-09     |
        | ent_coef                | 0.7835367    |
        | ent_coef_loss           | -0.117379844 |
        | entropy                 | -0.7436243   |
        | episodes                | 680          |
        | fps                     | 57           |
        | mean 100 episode reward | 5.46         |
        | n_updates               | 199238       |
        | policy_loss             | 214.71423    |
        | qf1_loss                | 28.67083     |
        | qf2_loss                | 28.660355    |
        | time_elapsed            | 17219        |
        | total timesteps         | 997212       |
        | value_loss              | 0.24159686   |
        ------------------------------------------
        Num episodes:  679 	Success rate: 0.59%
        Problem:
            {
                Too much dependency on aux networks @ training time.
                -> No good result obtained during those stages
            }

    - fused_auxilary_trial43
        auxiliary: weight * 0.1
        BC: RAdam
        BC lr: 5e-7
        pre_n_epochs: 1000
        Learning: RAdam
        policy: Beta
        {
            Dense reward with even more BC epochs
        }
        ==== Training progress 100.00% ====
        Epoch 1000
        Training loss: 0.630477, Validation loss: 0.634888
        -------------------------------------------
        | current_lr              | 2.7e-06       |
        | ent_coef                | 0.9906271     |
        | ent_coef_loss           | -0.0032340572 |
        | entropy                 | -0.83602786   |
        | episodes                | 68            |
        | fps                     | 49            |
        | mean 100 episode reward | 1.74          |
        | n_updates               | 19506         |
        | policy_loss             | 86.10309      |
        | qf1_loss                | 0.7904922     |
        | qf2_loss                | 0.80388767    |
        | time_elapsed            | 2000          |
        | total timesteps         | 98550         |
        | value_loss              | 0.5235663     |
        -------------------------------------------
        Num episodes:  67 	Success rate: 1.49%
        -----------------------------------------
        | current_lr              | 1.35e-08    |
        | ent_coef                | 0.77580905  |
        | ent_coef_loss           | -0.11882399 |
        | entropy                 | -0.75410104 |
        | episodes                | 680         |
        | fps                     | 106         |
        | mean 100 episode reward | -1.69       |
        | n_updates               | 198896      |
        | policy_loss             | 227.53873   |
        | qf1_loss                | 33.263424   |
        | qf2_loss                | 33.24225    |
        | time_elapsed            | 9379        |
        | total timesteps         | 995502      |
        | value_loss              | 0.2273885   |
        -----------------------------------------
        Num episodes:  679 	Success rate: 0.29%


2020 10 15 (Thu)

    - Linear primitive, baseline acc: 50%
    - Tried State-Action coupling by estimating the next state from the last layers of policy given state and action.
    - 1. Predict: x, y, phi with action: lin from policy + ang from noise
        - test result
            ------------------------------------------
            | current_lr              | 7.49e-09     |
            | ent_coef                | 0.76749134   |
            | ent_coef_loss           | -0.012358234 |
            | entropy                 | -0.94481575  |
            | episodes                | 1664         |
            | fps                     | 132          |
            | mean 100 episode reward | 0.1          |
            | n_updates               | 199297       |
            | policy_loss             | 164.76471    |
            | qf1_loss                | 0.06117493   |
            | qf2_loss                | 0.058522306  |
            | time_elapsed            | 7551         |
            | total timesteps         | 997507       |
            | value_loss              | 0.12089803   |
            ------------------------------------------
            Num episodes:  1666 	Success rate: 50.84%
            sa coupler loss:  0.007889251
            obs:		 [ 1.41860894 -4.11809422  2.62856023 -0.13740125  0.07864352  1.41075706]
            GT next obs:	 [ 1.42523083 -4.12182463  2.62958227]
            Pred next obs:	 [ 1.3439953 -3.9926527  2.4744987]
            sa coupler loss:  0.0041199396
            obs:		 [ 1.92841527 -4.4100945   2.61833652 -0.13740125  0.07864352  1.41075706]
            GT next obs:	 [ 1.93334118 -4.41293623  2.61680923]
            Pred next obs:	 [ 1.8876604 -4.309787   2.507296 ]
            action: -0.278 , mode: 0.178 , mu: 0.218 , std: 0.10000 , alpha: 3.511 , beta: 12.561
            sa coupler loss:  0.0016889866
            obs:		 [ 2.397715   -4.67849928  2.62366218 -0.13740125  0.07864352  1.41075706]
            GT next obs:	 [ 2.40013436 -4.67987788  2.62440659]
            Pred next obs:	 [ 2.384603  -4.606096   2.5577064]
            sa coupler loss:  0.0004198793
            obs:		 [ 2.86901036 -4.9481978   2.61879649 -0.13740125  0.07864352  1.41075706]
            GT next obs:	 [ 2.8745766  -4.95140551  2.61860181]
            Pred next obs:	 [ 2.8772616 -4.906147   2.5970674]
        -> low sa coupler loss at last (sudden?) but, 10e-5 ~ 10e-6 till near 1500 episodes
        -> Tested, always heading backward. alpha ~ 2.**, beta ~ 12.**
    - 2-1. Predict: x, y with action: lin from policy (lr: lr)
    - 2-2. Predict: x, y with action: lin from policy (lr: lr * 5)
    - 3. Predict: del x, del y with action: lin from policy (lr: lr * 5)
        -> Not a good idea, since what we'd prefer our policy to have a good understanding of the current state.
         By reducing the state error given current state may result in more accurate estimation of the increment, but will less likely
         to adjust policy parameters to the current state.
    - 4. Predict: x, y with action: lin from policy (lr: lr * 5), update SAC grad with half the frequency of sa step
         (changed from 1 -> int(freq/2))
    
    1~4: Always heading backward. alpha ~ 2.**, beta ~ 12.**
    All of the above achieved success rate of near 50% -> actions always head backward. Need to fix it

    - 5. Maybe lack of exploration? try with more samplings + random exploration
    - N. How about training with Alpha/Beta directly? ... really??
    - 7. How about mu with tanh, sigma with sigmoid? -> NEVER!!!
    - 8. Adjust mu value (median mapped to 0.004 - Problem!) -> will it attenuate the problem?
    - 9. How about adding BC?
    - 10. Is it underfitted? Do we need more parameters for the policy?
        - deeper layer?
        - wider layer?
        -> No... lr too small, suboptimal...
    - 11. SA coupling results in biased action?
        -> Nope, with/without SA coupling resulted in biased action in absolute obs
    - 12. MU BIASING FIGURED OUT: sieving nan value of logp_pi to 1 results in bias.
        -> But actually not... Cartpole with/without sieving performed nicely without any bias.
        -> Relative obs does not show the biased actions... why...?
    - 13. training with high lr 5e-4 (without SA coup, absolute obs)
    - 14. training with high lr 5e-4 (with SA coup, absolute obs) - exploded
    - 15. learning with 5e-4, but SA coupling with 1e-6
    - 16. training with lr 5e-5 (with SA coup, absolute obs)
    - 17. training with lr 5e-5, SA coup lr 5e-6
    - 18. gradient flow to var
    - 19. gradient flow to var + SA coup update same with SAC update
    - 20. gradient flow to var + SA coup update same with SAC update + additional wide layer added to the last level of the policy network
    - 21. SA coup update same with SAC update + additional wide layer added to the last level of the policy network
    - 22. With even deeper network + SA coup update(with sumed) before SAC update, 
            + additional layer added to SAcoupler, and the last layer has same # of neurons with coupled state to compensate the roll of action better
            + dense reward
        - linear_separate_trial79
            -----------------------------------------
            | current_lr              | 4.49e-05    |
            | ent_coef                | 0.63408554  |
            | ent_coef_loss           | -0.04201517 |
            | entropy                 | -0.8960402  |
            | episodes                | 96          |
            | fps                     | 102         |
            | mean 100 episode reward | 35.8        |
            | n_updates               | 10115       |
            | policy_loss             | 28.948477   |
            | qf1_loss                | 0.34185708  |
            | qf2_loss                | 0.34412962  |
            | time_elapsed            | 999         |
            | total timesteps         | 102177      |
            | value_loss              | 0.14038643  |
            -----------------------------------------
            Num episodes:  95 	Success rate: 28.42%
            ------------------------------------------
            | current_lr              | 1.57e-07     |
            | ent_coef                | 0.09099196   |
            | ent_coef_loss           | -0.15502071  |
            | entropy                 | -0.93784124  |
            | episodes                | 1124         |
            | fps                     | 131          |
            | mean 100 episode reward | 50.8         |
            | n_updates               | 99584        |
            | policy_loss             | 22.21339     |
            | qf1_loss                | 0.0167036    |
            | qf2_loss                | 0.014223535  |
            | time_elapsed            | 7582         |
            | total timesteps         | 996862       |
            | value_loss              | 0.0034751603 |
            ------------------------------------------
            Num episodes:  1126 	Success rate: 47.07%
        {
            SA coupler: Useless -> Didn't converge to a meaningful level.
            Movement shown at the starting phase (Wheter correct or not)
                -> as time goes, it reduces its speed
        }
    - 23. " + sparse reward + SA coup update with mean
        - linear_separate_trial80
            ------------------------------------------
            | current_lr              | 4.49e-05     |
            | ent_coef                | 0.6358355    |
            | ent_coef_loss           | -0.042796638 |
            | entropy                 | -0.88639617  |
            | episodes                | 76           |
            | fps                     | 94           |
            | mean 100 episode reward | 0.16         |
            | n_updates               | 10023        |
            | policy_loss             | 27.685257    |
            | qf1_loss                | 0.0008432417 |
            | qf2_loss                | 0.0010583058 |
            | time_elapsed            | 1074         |
            | total timesteps         | 101256       |
            | value_loss              | 0.08373366   |
            ------------------------------------------
            Num episodes:  75 	Success rate: 17.33%
            -------------------------------------------
            | current_lr              | 4.66e-08      |
            | ent_coef                | 0.088483095   |
            | ent_coef_loss           | -0.1801542    |
            | entropy                 | -0.9159876    |
            | episodes                | 1060          |
            | fps                     | 136           |
            | mean 100 episode reward | 0.07          |
            | n_updates               | 99805         |
            | policy_loss             | 19.760448     |
            | qf1_loss                | 0.00039794482 |
            | qf2_loss                | 0.00028243283 |
            | time_elapsed            | 7332          |
            | total timesteps         | 999073        |
            | value_loss              | 0.001683588   |
            -------------------------------------------
            Num episodes:  1060 	Success rate: 38.30%
        {
            SA coupler also useless.
            Always moves forward.
            Policy loss decreasing - Maybe a bit more training?
        }
    - 24. " + SA coup updates more frequently than SAC. 
        - linear_separate_trial81
            ------------------------------------------
            | current_lr              | 4.48e-05     |
            | ent_coef                | 0.63078946   |
            | ent_coef_loss           | -0.05352938  |
            | entropy                 | -0.886851    |
            | episodes                | 76           |
            | fps                     | 64           |
            | mean 100 episode reward | 0.04         |
            | n_updates               | 10275        |
            | policy_loss             | 28.018255    |
            | qf1_loss                | 0.0011327865 |
            | qf2_loss                | 0.0012332982 |
            | time_elapsed            | 1612         |
            | total timesteps         | 103771       |
            | value_loss              | 0.07914214   |
            ------------------------------------------
            Num episodes:  75 	Success rate: 10.67%
            -------------------------------------------
            | current_lr              | 9.35e-08      |
            | ent_coef                | 0.088621005   |
            | ent_coef_loss           | -0.25222063   |
            | entropy                 | -0.9065951    |
            | episodes                | 1020          |
            | fps                     | 109           |
            | mean 100 episode reward | 0.19          |
            | n_updates               | 99711         |
            | policy_loss             | 20.885864     |
            | qf1_loss                | 0.0003092753  |
            | qf2_loss                | 0.00038747277 |
            | time_elapsed            | 9082          |
            | total timesteps         | 998137        |
            | value_loss              | 0.0018869314  |
            -------------------------------------------
            Num episodes:  1020 	Success rate: 38.73%
        {
            SA coupling doesn't seem to be useful as lr decreases to near 1e-9, and still not converging.
            Looks like a bit biased forwardly, but not exactly.
        }
    -25. logp_pi multiple samples from state
        - linear_separate_trial82
            ------------------------------------------
            | current_lr              | 4.5e-05      |
            | ent_coef                | 0.6379396    |
            | ent_coef_loss           | -0.048355225 |
            | entropy                 | -0.8865482   |
            | episodes                | 80           |
            | fps                     | 83           |
            | mean 100 episode reward | 0.253        |
            | n_updates               | 9854         |
            | policy_loss             | 27.669382    |
            | qf1_loss                | 0.0016267912 |
            | qf2_loss                | 0.001808959  |
            | time_elapsed            | 1187         |
            | total timesteps         | 99561        |
            | value_loss              | 0.016244117  |
            ------------------------------------------
            Num episodes:  79 	Success rate: 26.58%
            -------------------------------------------
            | current_lr              | 7.71e-08      |
            | ent_coef                | 0.084628955   |
            | ent_coef_loss           | -0.23121975   |
            | entropy                 | -0.9044304    |
            | episodes                | 1052          |
            | fps                     | 152           |
            | mean 100 episode reward | 0.3           |
            | n_updates               | 99744         |
            | policy_loss             | 20.096495     |
            | qf1_loss                | 0.16304855    |
            | qf2_loss                | 0.16253597    |
            | time_elapsed            | 6557          |
            | total timesteps         | 998463        |
            | value_loss              | 0.00016107864 |
            -------------------------------------------
            Num episodes:  1052 	Success rate: 40.97%
        {
            SA coupler doesn't seem to converge as well.
            Forward biased. WHY!!!!
        }
    
    Q. Why is std always fixed to the maximum value?

2020 10 16 (Fri)
    -26. Is the depth/width of the policy network short to represent state-action mapping?
        Q. Can policy network properly represent the received state signal?
            Test1. Deepen the SA Coupler to give sufficient rooms for latent features of the last policy layer to be fitted to the state estimation.
                - linear_separate_trial83 -> 80 (typo)
                    -------------------------------------------
                    | current_lr              | 1.2e-08       |
                    | ent_coef                | 0.0005405881  |
                    | ent_coef_loss           | -0.029572416  |
                    | entropy                 | -0.992943     |
                    | episodes                | 6288          |
                    | fps                     | 125           |
                    | mean 100 episode reward | 0.46          |
                    | n_updates               | 499778        |
                    | policy_loss             | 0.1920619     |
                    | qf1_loss                | 5.3218123e-06 |
                    | qf2_loss                | 7.050609e-06  |
                    | time_elapsed            | 39908         |
                    | total timesteps         | 4998805       |
                    | value_loss              | 2.6477792e-06 |
                    -------------------------------------------
                    Num episodes:  6287 	Success rate: 49.77%
                    sa coupler loss:  0.2385518
                {
                    SA coupler doesn't seem to converge.
                }
            Test2. See if SA Coupler can estimate next state given current state and action.
                1. RL fashion - Estimate the effect of action given state from latent vector (No need to estimate the state, but only the residual value)
                -> linear_separate_trial84
                    -------------------------------------------
                    | current_lr              | 3.31e-08      |
                    | ent_coef                | 0.0003846674  |
                    | ent_coef_loss           | -0.115235984  |
                    | entropy                 | -0.9885119    |
                    | episodes                | 5716          |
                    | fps                     | 125           |
                    | mean 100 episode reward | 0.25          |
                    | n_updates               | 499567        |
                    | policy_loss             | 0.19074601    |
                    | qf1_loss                | 0.00028179405 |
                    | qf2_loss                | 0.0002797141  |
                    | time_elapsed            | 39884         |
                    | total timesteps         | 4996695       |
                    | value_loss              | 2.7005296e-06 |
                    -------------------------------------------
                    Num episodes:  5717 	Success rate: 41.33%
                    FIRST TIMESTEP
                        sa coupler loss:  3.1030752e-06
                        obs:		     [-2.52226206  2.96752719]
                        GT next obs:	 [-2.52085524  2.969679  ]
                        Pred next obs:	 [-2.52269266  2.96667325]
                    FIRST TIMESTEP
                        sa coupler loss:  2.6768277e-05
                        obs:		     [-1.63020711 -1.70137338]
                        GT next obs:	 [-1.63185512 -1.71035756]
                        Pred next obs:	 [-1.62948842 -1.70028429]
                     FIRST TIMESTEP
                        sa coupler loss:  1.6480793e-05
                        obs:		     [-2.31425233 -1.34178904]
                        GT next obs:	 [-2.31640369 -1.35059552]
                        Pred next obs:	 [-2.31465105 -1.34266762]
                    FIRST TIMESTEP
                        sa coupler loss:  2.0656358e-05
                        obs:		     [-0.99604904  2.44866421]
                        GT next obs:	 [-0.99421233  2.43890213]
                        Pred next obs:	 [-0.99648107  2.44770427]
                {
                    Residual scheme doesn't seem to work well at all -> hidden pi may not feature out the state very well
                }

                2. No policy fashion - Get rid of the policy network (Receive info from state directly)
                -> linear_separate_trial85
                    -------------------------------------------
                    | current_lr              | 2.68e-08      |
                    | ent_coef                | 0.00019060548 |
                    | ent_coef_loss           | -0.19666965   |
                    | entropy                 | -0.98011875   |
                    | episodes                | 8208          |
                    | fps                     | 139           |
                    | mean 100 episode reward | 0.14          |
                    | n_updates               | 499630        |
                    | policy_loss             | 0.23279147    |
                    | qf1_loss                | 3.9357375e-05 |
                    | qf2_loss                | 3.686829e-05  |
                    | time_elapsed            | 35723         |
                    | total timesteps         | 4997322       |
                    | value_loss              | 3.6967806e-06 |
                    -------------------------------------------
                    Num episodes:  8210 	Success rate: 49.37%
                    FIRST TIMESTEP
                        sa coupler loss:  0.00021752415
                        obs:		     [-1.94468483  0.94174427]
                        GT next obs:	 [-1.93676107  0.93746489]
                        Pred next obs:	 [-1.922635   0.9115699]
                    FIRST TIMESTEP
                        sa coupler loss:  0.00083253434
                        obs:		     [-2.82702304  1.28220331]
                        GT next obs:	 [-2.82937201  1.28662125]
                        Pred next obs:	 [-2.781733   1.2540532]
                    FIRST TIMESTEP
                        sa coupler loss:  0.0011367932
                        obs:     		 [ 0.21149025 -1.04442638]
                        GT next obs:	 [ 0.21056559 -1.04236096]
                        Pred next obs:	 [ 0.19428182 -0.97692394]
                    FIRST TIMESTEP
                        sa coupler loss:  0.0016270675
                        obs:    		 [-0.45371863  2.69732278]
                        GT next obs:	 [-0.45400202  2.69745396]
                        Pred next obs:	 [-0.4324392  2.6197152]
                    FIRST TIMESTEP
                        sa coupler loss:  1.9849816e-05
                        obs:    		 [0.34079533 1.06428713]
                        GT next obs:	 [0.34904111 1.06167522]
                        Pred next obs:	 [0.34566417 1.0699211 ]
                    FIRST TIMESTEP
                        sa coupler loss:  0.0005642412
                        obs:    		 [ 1.07459795 -1.30249878]
                        GT next obs:	 [ 1.07104801 -1.30837407]
                        Pred next obs:	 [ 1.0863827 -1.2634095]
                {
                    First timestep SA coupler error seems to be reduced fairly well compared to the previous pi_hidden layers, but still not enough to represent state well.
                }
    
    -27. Entropy coeff too large compared to the sparse reward setting. 
        -> Compare Q value with different reward setting.
            1. dense reward low value
                - 
            2. dense reward high value
                - 
            3. sparse reward low value
                - Have done a lot
            4. sparse reward high value
                - linear_separate_trial88
                    -----------------------------------------
                    | current_lr              | 1.66e-05    |
                    | ent_coef                | 0.037199    |
                    | ent_coef_loss           | 0.019487422 |
                    | entropy                 | -1.0017076  |
                    | episodes                | 4404        |
                    | fps                     | 100         |
                    | mean 100 episode reward | 47.9        |
                    | n_updates               | 333801      |
                    | policy_loss             | 9.971277    |
                    | qf1_loss                | 2.3674643   |
                    | qf2_loss                | 2.4746666   |
                    | time_elapsed            | 33374       |
                    | total timesteps         | 3339032     |
                    | value_loss              | 0.060025256 |
                    -----------------------------------------
                    Num episodes:  4403 	Success rate: 48.81%
                    ------------------------------------------
                    | current_lr              | 2.3e-08      |
                    | ent_coef                | 0.040467385  |
                    | ent_coef_loss           | 0.0053986846 |
                    | entropy                 | -1.0090866   |
                    | episodes                | 6532         |
                    | fps                     | 114          |
                    | mean 100 episode reward | 52.9         |
                    | n_updates               | 499668       |
                    | policy_loss             | 14.83703     |
                    | qf1_loss                | 0.43515944   |
                    | qf2_loss                | 0.21010816   |
                    | time_elapsed            | 43485        |
                    | total timesteps         | 4997708      |
                    | value_loss              | 0.3179015    |
                    ------------------------------------------
                    Num episodes:  6533 	Success rate: 50.38%
                {
                    Sucess mostly from front action, but no sign of bias
                    SA coupler loss doesn't seem to decrease
                }

2020 10 17 (Sat)
    -28. Target network copy from source (by SAC paper)
        1. with SA Coupler
            - linear_separate_trial86
                ------------------------------------------
                | current_lr              | 1.7e-05      |
                | ent_coef                | 0.0004905583 |
                | ent_coef_loss           | -0.11591902  |
                | entropy                 | -0.9953968   |
                | episodes                | 4080         |
                | fps                     | 101          |
                | mean 100 episode reward | 0.51         |
                | n_updates               | 329589       |
                | policy_loss             | 0.16421838   |
                | qf1_loss                | 0.0008705767 |
                | qf2_loss                | 0.0008689374 |
                | time_elapsed            | 32510        |
                | total timesteps         | 3296910      |
                | value_loss              | 4.848305e-06 |
                ------------------------------------------
                Num episodes:  4079 	Success rate: 43.79%
                -------------------------------------------
                | current_lr              | 5.11e-09      |
                | ent_coef                | 0.000531978   |
                | ent_coef_loss           | -0.028741807  |
                | entropy                 | -0.99673676   |
                | episodes                | 6064          |
                | fps                     | 105           |
                | mean 100 episode reward | 0.53          |
                | n_updates               | 499847        |
                | policy_loss             | 0.20795879    |
                | qf1_loss                | 8.7328386e-05 |
                | qf2_loss                | 8.787915e-05  |
                | time_elapsed            | 47542         |
                | total timesteps         | 4999499       |
                | value_loss              | 3.3874783e-06 |
                -------------------------------------------
                Num episodes:  6064 	Success rate: 46.16%
                //////////////DIFFERENT LR///////////////////
                -------------------------------------------
                | current_lr              | 1.69e-05      |
                | ent_coef                | 0.0004593251  |
                | ent_coef_loss           | -0.041431274  |
                | entropy                 | -0.99726385   |
                | episodes                | 4436          |
                | fps                     | 99            |
                | mean 100 episode reward | 0.46          |
                | n_updates               | 330978        |
                | policy_loss             | 0.11592258    |
                | qf1_loss                | 4.4819084e-05 |
                | qf2_loss                | 4.5848505e-05 |
                | time_elapsed            | 33207         |
                | total timesteps         | 3310807       |
                | value_loss              | 7.059043e-06  |
                -------------------------------------------
                Num episodes:  4435 	Success rate: 50.48%
                -------------------------------------------
                | current_lr              | 3.7e-08       |
                | ent_coef                | 0.00044710157 |
                | ent_coef_loss           | -0.098227374  |
                | entropy                 | -0.99105674   |
                | episodes                | 6460          |
                | fps                     | 104           |
                | mean 100 episode reward | 0.45          |
                | n_updates               | 499528        |
                | policy_loss             | 0.19365123    |
                | qf1_loss                | 7.619441e-06  |
                | qf2_loss                | 6.548571e-06  |
                | time_elapsed            | 47655         |
                | total timesteps         | 4996304       |
                | value_loss              | 2.8445897e-06 |
                -------------------------------------------
                Num episodes:  6461 	Success rate: 50.83%
            {
                SA Coupler loss not decreasing.
                Action not biased. No OB, but time up.
                Doesn't seem to get better at achieving goal - Success rate slowly, slowly increases.
            }


        2. without SA Coupler
            - linear_separate_trial87
                -------------------------------------------
                | current_lr              | 1.7e-05       |
                | ent_coef                | 0.00040760695 |
                | ent_coef_loss           | -0.022687973  |
                | entropy                 | -1.0013275    |
                | episodes                | 4504          |
                | fps                     | 183           |
                | mean 100 episode reward | 0.58          |
                | n_updates               | 329725        |
                | policy_loss             | 0.14374875    |
                | qf1_loss                | 0.0007662733  |
                | qf2_loss                | 0.00077279273 |
                | time_elapsed            | 17928         |
                | total timesteps         | 3298274       |
                | value_loss              | 7.0510678e-06 |
                -------------------------------------------
                Num episodes:  4503 	Success rate: 50.81%
                -------------------------------------------
                | current_lr              | 3.46e-08      |
                | ent_coef                | 0.0005752122  |
                | ent_coef_loss           | 0.047651496   |
                | entropy                 | -1.0043137    |
                | episodes                | 6500          |
                | fps                     | 176           |
                | mean 100 episode reward | 0.54          |
                | n_updates               | 499552        |
                | policy_loss             | 0.21169686    |
                | qf1_loss                | 1.3582756e-05 |
                | qf2_loss                | 1.4727514e-05 |
                | time_elapsed            | 28376         |
                | total timesteps         | 4996546       |
                | value_loss              | 4.1647754e-06 |
                -------------------------------------------
                Num episodes:  6499 	Success rate: 51.13%
            {
                Everything same with SA Coupler
            }

2020 10 23 (Fri)
    -29. Weight regularization term added to the reward
        1. 2,1,1,1
            - fused_auxilary_trial44
                -----------------------------------------
                | current_lr              | 3.96e-05    |
                | ent_coef                | 0.011189235 |
                | ent_coef_loss           | -0.5705854  |
                | entropy                 | -0.9167373  |
                | episodes                | 696         |
                | fps                     | 212         |
                | mean 100 episode reward | -196        |
                | n_updates               | 103437      |
                | policy_loss             | 32.202618   |
                | qf1_loss                | 0.2631174   |
                | qf2_loss                | 0.26299334  |
                | time_elapsed            | 4865        |
                | total timesteps         | 1035392     |
                | value_loss              | 0.013994224 |
                -----------------------------------------
                Num episodes:  695 	Success rate: 0.86%
            {
                Still aux network producing dominant action
                weight: [[8.8941997e-01 1.1057547e-01 5.0556244e-13 4.5854958e-06]]
            }
        2. 3,0.1,0.1,0.1
            - fused_auxilary_trial45
        
        3. 3,0.01,0.01,0.01
            - fused_auxilary_trial46
        
        4. 1, 0.001, 0.001, 0.001
            - fused_auxilary_trial47

2020 10 27 (Tue)
    - 29 (cont) 
        5. 0.1, 0.001, 0.001, 0.001 -> too large reward results in NAN?
            - fused_auxilary_trial48    

2020 11 3 (Tue)
    - 29 (cont)
        5. Identify the reason for the occurance of NAN
            - fused_auxilary_trial49
            {
                NAN from input batch?
            }

2020 11 17 (Tue)
    - 30 Natural Gradient scheme added to the SAC (Emperical Fisher inv multiplied to the gradient of SAC objective)

2020 12 13 (Sun)
    - Should timesteps be added???
        -> if yes, then how to train each primitives and fuse them?