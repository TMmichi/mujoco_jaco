Note

    Problem_specification:
        Keywords:
            1. Dexterous manipulation
            2. Strategic (time-dependent) manipulation
            3. Human data learning
            4. Multi-modal sensory data
            5. Safe to interact (torque control)
            6. Robot independent (workspace independent)
            7. Dual (multi) arm
            8. Reaching
            9. Grasping
            10. State representation
            11. Sim-to-Real
            12. Learning from Demonstration
            13. Reusable policies
            14. Explainable AI
            15. Sample efficient learning


        Tasks:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                Keywords:
                Goal: Make policies explainable (and mimic human-like behavior - if incorporating human data while TRAINING PRIMITIVES)
                Motivation:
                    - Shall let the users know what it's doing with human-interpretable information
                    - Behave in a way human brain does -> composing complicated behavior with basic structures of primitives in a hierarchical manner
                    - MCPs uses primitives that are not identifiable
                Contributions:
                    - making policy explainable
                    - Increase sample efficiency (incorporate human data while TRAINING WHOLE TASK)
                    - learn auxiliary primitives that have not been told/trained (if possible)
                    - fuse policies with different action space into a whole
                    - reduce learning time compared to primitives that has same, whole action space
                    - weights coordinate each primitives: not concurrent -> execute multiple primitives at once // no latent embedding -> interpretable
                    - can be applied to policies with different action space
                    - configure proper states from observation to each primitives
                
                To_do: 
                    - Train primitives that are identified for certain tasks (from either expert trajectory or reward engineered RL scheme)
                    - Set task which requires multiple primitives:
                        Options: 
                        {
                            1. 
                            {
                                Task: Grasp a glass of water and pour to a specific destination
                                Required features (primitive):
                                    - Reaching to a glass (Reaching)
                                    - Make a good grip when holding a glass (Reaching + Grasping)
                                    - Take it to a goal position when holding a glass (Reaching + Grasping + Auxiliary - stabilizing glass?)
                                    - Pouring it to a goal position (Reaching + Grasping + Auxiliary - Pouring)
                                Thoughts:
                                    - Pouring: 
                                    - Moveit! -> variance = 0인 distribution
                                    - Optimal state representation -> No need to do these shits
                            }
                            2. 
                            {
                                Task: 
                                Goal: 
                            }
                        }
                    - State representation learning of required & orthogonal sensor data: image, pressure, joint angles
                        -> want to show that those data has some information that does not overlap with each other
                        -> check orthogonality of each sensors using [PCA? suggestions????]
                    - Provide auxiliary dummy primitive networks when training in an End-to-End fashion -> do a PCA analysis for actions under states to validate the need for an extra primitive.
                        -> How to check / avoid a single auxiliary network from taking over all the auxiliary primitives?
                             - by training policy with enough of aux policies and then optimize the numbers?
                        -> time: 언제 primitive가 activate 될 지??
                        -> Non-Markovian 
                    - Identify unknown primitives via (examining state/action sets from forward propagations)
                    - primitives with same action space -> MCP // primitives with different action space -> ?
                References:
                    Compositive_policy:
                        Additive:
                            - Mixture Density Networks (Bishop, 1994)
                        Multiplicative:
                            - MCP: Learning composable hierarchical control with multiplicative compositional policies (Peng, 2019)    
                    Hierarchical_policy:
                        Pure_Hierarchy
                            - FeUdal Networks for Hierarchical Reinforcement Learning (Vezhnevets, 2017)
                            - Options (Sutten, 1999)
                            - Hierarchical and interpretable skill acquisition in multi-task reinforcement learning (Shu, 2017)
                            - Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards (Li, 2019)
                            BT-RL
                                - Integrating Reinforcement Learning into Behavior Trees by Hierarchical Composition (Kartasev, 2019)
                                - QL-BT: Enhancing behavior tree design and implementation with Q-learning (Dey, 2013)
                        Meta_Learning
                            - Learning to Coordinate Manipulation Skills Via Skill Behavior Diversification (Lee, 2020)
                            - Modular Multitask Reinforcement Learning with Policy Sketches (Andreas, 2017)
                            - Chealsea Finn paper of meta-learning
                            - MLSH (Frans, 2017)
                    Explainable_RL:
                        - Hierarchical and interpretable skill acquisition in multi-task reinforcement learning (Shu, 2017)
                    Brain_Functionality:
                        Pure_Brain:
                            - The Architecture of Complexity (Simon, 1962)
                            - Hierarchical modularity in human brain functional networks (Meunier, 2009)
                            - Functional Network Organization of the Human Brain (Power, 2011)
                        Deep_Learning_Related:
                            - Overcoming catastrophic forgetting in neural networks (Kirkpatrick, 2017)
                            - Progressive neural networks (Rusu, 2016)
                            - Sim-to-real robot learning from pixels with progressive nets (Rusu, 2017)
            }
            1-1. Sub_task: test with simpler environment
            {
                Contributions:
                    - making policy explainable by incorporating known primitives
                    - can be applied to policies with different state/action space
                    - configure proper states from observation to each primitives
                    - Increase sample efficiency (incorporate human data while TRAINING WHOLE TASK)
                    - learn auxiliary primitives that have not been told/trained (if possible) -> auxiliary primitive can also achieve auxiliary loss/goals to improve performance
                    - reduce learning time compared to primitives that has same, whole action space
                    - weights coordinate each primitives: not concurrent -> execute multiple primitives at once // no latent embedding -> interpretable
                        -> explicit embedding: leads to explainable AI
                    # - Presents correlation on std of SAC and MCP in vanilla and Hierarchical structure
                    - Fuse sub-goal, selector-based HRL method, but does not leverage on option framework where a single policy network learns and operates over different levels of temporal abstaction given options from the meta-controller. Although options may abstract the action space, it is not interpretable by human in a physical terms. Instead, we produce subgoals from the meta-controller by which has been manually designated to certain primitive as a semantic state.
                    - Effectiveness of imploiting entropies within the policy has been proved to be useful by many works regarding the policy optimization algorithm. Soft Q-Learning ~~~ . Precedent of soft Q-Learning is SAC, where it ~~~. Not only restricted to the policy optimization scheme, but reward setting of the learning algorithm has also proposed notion of uncertainties to be implemented. Frank et al. (curiosity driven RL for motion planning on humanoids) leverages on information maximization as a curiosity, and [other uncertainties]. However, performance of simplified primitives is not highly dependent on the uncertainties of the environment, and shows slower convergence than that of the ~~, and thus we do not use uncertainties scheme on learning simple primitives. (Nevertheless, we propose the usefulness of uncertainties within the meta-controller where exploration should be taken upon)
                    - Proposes gradients of meta-controller wrt. subgoals/weight selection.

                To Do:
                    - create tasks which needs manipulator to get involved -> different action space
                Ideation:
                    - Do we really need a manipulator to be involved? -> yes! 원래 하려던게 이거였음.
                    - + 원래 또 하려고 했던 것: 사람의 데이터를 수집하여 Behavior Cloning을 한 다음, 보상함수 검출
                    - 보상함수 검출은 적당히 함수 형상 정해놓고, 파라미터 optimizing 하는 정도로만?
                    - 일단 task 정해서, policy update하고, 그거 맞춰서 reward는 다시 정하는 정도로만 하자.
                    - 그럼 대체 왜 manipulator의 task에 hierarchical policy가 필요한가?
                    - 복잡하니까, 좀 더 간단하게 풀어보고 싶어서.
                    - 복잡함의 대상: task / action space / action planning
                    - Aux Net의 Action bound를 정해버린다. 진짜 조그마한 값만 내어놓아서 기존에 train 된 network를 사용할 때 보정만 하도록.
                    - meta-controlle의 action: weights/subgoals
                        - 
                
                Done (moved from ToDO):
                    - 

                References:
                    1. Compositive_policy:
                        1.1. Additive:
                            1.1.1. Mixture Density Networks (Bishop, 1994)
                        1.2. Multiplicative:
                            1.2.1. MCP: Learning composable hierarchical control with multiplicative compositional policies (Peng, 2019)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                    2. Hierarchical_policy:
                        2.1 Pure_Hierarchy
                            2.1.1. Hierarchical and interpretable skill acquisition in multi-task reinforcement learning (Shu, 2017)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        - Goal condition is sampled from sets of goals, so the goal is explicitly comprehensible.
                                    Cons:
                                        -
                                }
                            2.1.2. Data-Efficient Hierarchical Reinforcement Learning (Nachum, 2018)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        - to remedy the challenge from off-policy learning of HRL (only sequential) by off-policy correction by re-labeling
                                    Cons:
                                        - 
                                        - same state space for high/low policy + low policy taking additional goal as a state from high
                                }
                            2.1.3. Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards (Li, 2019)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            2.1.4. Hierarchical Visuomotor Control of Humanoids (Merel, 2019)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                        - same state space for high/low policy + low policy taking additional goal as a state from high
                                }
                        2.2 BT-RL
                            2.2.1. QL-BT: Enhancing behavior tree design and implementation with Q-learning (Dey, 2013)
                            2.2.2. Integrating Reinforcement Learning into Behavior Trees by Hierarchical Composition (Kartasev, 2019)
                        2.3 Meta_Learning
                            2.3.1. Options (Sutten, 1999)
                                {
                                    Key:
                                        - abstractions over the space of actions
                                        - agent chooses one step primitive action vs multi-step action policy
                                        - terminates upon stochastic function beta (MDP -> SMDP)
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            2.3.2. Bacon et al 2017
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            2.3.3. Modular Multitask Reinforcement Learning with Policy Sketches (Andreas, 2017)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            2.3.4. Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning (Dilokthanakul, 2017) 
                                {
                                    Key:
                                        - Meta-controller provides sub-goal as a latent and an intrinsic reward at learning
                                        - Meta-controller select which layer of CNN (feature) to be used @ intrinsic reward calculation
                                    Pros:
                                        - Using sub-goals from meta-controller can work better in sparse-reward environment
                                    Cons: 
                                        - learned features from feature controlling encoder cannot be explicitly interpreted -> options from meta-controller is hard to be 
                                        - end-to-end training: can be handy but do not know its options/sub-goals explicitly.
                                        - same state space for high/low policy + low policy taking additional goal as a state from high
                                }
                            2.3.5. FeUdal Networks for Hierarchical Reinforcement Learning (Vezhnevets, 2017)
                                {
                                    Key:
                                        - Manager and Worker setting where the goal from the manager is a estimated direction for the encoded state @ Manager's side
                                        - the Manager is updated independently from the Worker in a way that the gradient of the goal is updated aligned to the direction for maximizing the Adventage of the manager
                                    Pros:
                                        - manager provides a meaningful and explicit goal for the workers (?)
                                    Cons:
                                        - if action space is large, fitting U_t from RNN of the Worker side can be hard
                                        - Still do not have a clear understanding of what the goal intends to do in a human language
                                        - same state space for high/low policy + low policy taking additional goal as a state from high
                                }
                            2.3.6. MLSH (Frans, 2017)
                            2.3.7. Chealsea Finn paper of meta-learning
                            2.3.8. Learning Actionable Representation with Goal-Conditioned Policies (Ghosh, 2019)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            2.3.9. Learning to Coordinate Manipulation Skills Via Skill Behavior Diversification (Lee, 2020)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            2.3.10. Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation (Kulkarni, 2016)
                            {
                                key:
                                    - Uses h-DQN, meta learning (sub-goal based)
                                    - Difference from the dense reward environment: 
                                        Dense reward: 1. Subgoals should be engineered manually
                                                      2. There is no sequential order of the subtask. 
                                        HRL subgoal:  1. Subgoals can be learned
                                                      2. Reasonings on the actions can be identified based on the subgoals
                                Pros:
                                    - shared learning between different options
                                    - model is scalable to a large number of options
                                Cons:
                                    - Automatically forming options from data is challenging
                            }
                            2.3.11. The Option-Critic Architecture
                            {
                                Key:
                                    - policy gradient theorem
                                    - option-critic architecture
                                Pros:
                                    - No need to provide any additional rewards or subgoals
                                    - intra-option policy: no lines separating 'discovering' options from 'learning' options -> 이게 automatic option finding 아님..?
                                    - by providing number of desired options, it automatically learns options...?
                                    - no discount factors -> unbiased policy gradient estimator?
                                    - learn: option policies, termination function
                                        - option policies: how to choose good option
                                            -> e-greedy over options!
                                            -> numbers of options given... can argmax over options
                                        - intra-option policy: given option, learns to produce good actions
                                        - terminatation: goodness of termination upon entering the next state
                                    - gradient over both meta-controller, controller
                                    - state-option transition probability also exists

                                    -> Algorithm Intuition:
                                        when the option choice is suboptimal wrt. the expected value over all options, A is negative, so increases the odds of terminating given option.
                                        After termination, the agent has the opportunity to pick a better option using option policy
                                        (Interrupting Bellman Operator)
                                    
                                Cons:
                                    - 
                            }
                    3. Explainable_RL:
                        3.1.1. Hierarchical and interpretable skill acquisition in multi-task reinforcement learning (Shu, 2017)
                        -
                    4. Brain_Functionality:
                        4.1. Pure_Brain:
                            4.1.1. The Architecture of Complexity (Simon, 1962)
                            4.1.2. Hierarchical modularity in human brain functional networks (Meunier, 2009)
                            4.1.3. Functional Network Organization of the Human Brain (Power, 2011)
                        4.2. Deep_Learning_Related:
                            4.2.1. Overcoming catastrophic forgetting in neural networks (Kirkpatrick, 2017)
                            4.2.2. Progressive neural networks (Rusu, 2016)
                            4.2.3. Sim-to-real robot learning from pixels with progressive nets (Rusu, 2017)
                    5. Optimization_Algorithm:
                        5.1. SAC
                            {
                                Key:
                                    - 
                                Pros:
                                    -
                                Cons:
                                    -
                                Questions:
                                    - Why use log_std clipping?
                            }
                        5.2. Learning Multi-Level Hierarchies with Hindsight (Levy, 2019)
                            -> Useful for hierarchical policies in a sequential manner, but not in the case of ours
                    6. Manipulation via RL
                        6.1 Human Demonstrations
                            6.1.1. Learning Complex Dexterous Manipulation with Deep Reinforcement Learning and Demonstrations (Rajeswaran, 2018)
                            {
                                Key: Used Human Demonstrations
                                Pros:
                                Cons:
                            }
                        6.2 Manipulation
                            6.2.2 
            }
            2. Title: Who cares who I am: Policy transfer between robots without fine-tuning
            {
                Keywords: 
                
                Goal: Make policies that can be transferred to different robot models without fine-tuning
                Motivation:
                    - Most of the techniques uses joint angles -> restricted to the specific configuration of a robot
                Contributions:
                    - Can transfer policy without the need of fine-tuning within the simulated env
                    - Can transfer policy to a real robot (if possible)
                Limitation:
                    - IK algorithm should be embedded
                To Do:
                    - Use actions as ∆p ∆v of an End Effecor instead of joint angles or joint velocities
                    - Use action/observation scaling with respect to the workbound of each robots (NOT FINETUNING! CAN EASILY BE DONE)
                        (if the scale of jaco : mini jaco = 10 : 1, multiply 10 to each obs/action when using the policy trained from mini jaco to apply to a jaco)
                    - transfer policy without fine-tuning and compare episode rewards at test time
                Thoughts:
                    - Put vision/PC to EE
                    - How to handle vision/PC
                    - If DOF gets to be differ?
            }   
            3. Title: Dexterous Dual-Arm Manipulation via Compositive Primitives
            {
                Keywords: 
                Goal: TBD
                Motivation: TBD
                References:
                    Hierarchical_MARL:
                        - Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real (Nachum, 2019)
            }
            4. Title: Screw-fixing
            {
                Keywords:     
                Goal: TBD
                Motivation: TBD
                To Do:
                    - time-dependent 
            }
            5. Title: Plugging with sample-efficient policy by composing primitives
            {
                Keywords:     
                Goal: TBD
                Motivation: TBD
            }
            6. Title: Fully express the intention
            {
                Keywords:     
                Goal: Fully express the intention while doing a task by proper / mixture of methods
                Motivation: 
                Notes:
                    - Feature Importance, Permutation:
                        - Determine which feature is the most import one by randomly replacing values for each feature
                        - Importance by error measurement
                        - Cons: No direction, Scale independent, Feature independent
                    - Partial Dependence Plots
                        - Determine the most import feature by 'linearly' replacing values
                        - Pros: Directions, Scale dependent
                        - Cons: Computationally expensive
                    - Global Surrogate
                    - Local Interpretable Model-agnostic Explanations (LIME)
                    - Shapley Additive exPlanations (SHAP)
                    - Filter Visualization
                    - Layer-wise Relevant Propagation (LRP)
            }
            7. State-Action Coupling to focus on states that really matters - specify elements of latent variables to focus
            {
            }
            8. ?????????
            {
                Sampling algorithm -> uses utility
                1. does utility has some sort of coefficient to be determined?

                Dexterous manipulation -> 
            }
            1-2. IRL for problem solving!!! // grasping
            {
                1. 저번주 이후로 상당히 많은 state/reward shaping 과정이 있었음


                2. 교수님께서 주신 MaxEnt IRL paper:
                    - 일단 expert trajectory가 존재. 나도 있음
                    - 어떤 task에 대해 optimal인지 유추함. 근데 난 딱 grasping에 쓰일건데? "어떻게"가 아니라, "얼마나 잘"이 문제라...
                    - 방법:
                        1. trajectory를 relable하고, 이걸로 off-policy RL을 돌림
                            의의: 기존의 hindsight relable method를 generalize 해서 비단 goal-reaching 뿐만 아니라 task distribution에 적용할 수 있게 함.
                        2. Behavior Cloning을 relabled experience에 사용함.
                            의의: policy search를 multi-task setting으로, goal-conditioned imitation learning을 임의의 task distribution으로 확장시킴.


                Apprenticeship learning paper를 보고 혹시 state의 non-linear 

                3. 일단, 재현형님 연구에서 GP로 reward function의 parameter 및 state feature 추정한 것을 어떻게 내 연구에 접목시킬까?
                    3-1. 특징
                        - reward function의 each term 형태가 정해져있고, 한 term의 coefficient에 대해 expert trajectory로 GP realization을 시킴
                        - trajectory의 state feature 중 어느 것이 coefficient를 결정하는 데에 dominant 한 영향을 미치나 알아봄
                    3-2.
                        내 state: 15 dimensions
                            - touch index : 1 dimension
                            - EE pose (normalized into range (-1 ~ 1)) : 6 dimensions
                            - gripper angle (normalized into range (-0.5 ~ 0.5)) : 2 dimensions
                            - object pose (normalized into range (-1 ~ 1)) : 6 dimensions
                        내 reward:
                            - distance btw EE and the object
                            - difference btw EE angle and the desired angle
                            - touch index (grasp, in-touch, out-touch)
                            - object height
                                의 linear combination으로 이루어져 있음
                        내 action: 8 dimensions
                            - del_pose : 6 dimensions
                            - del_gripper angle : 2 dimensions
                        
                        Q. state 또한
                
                4. 내 HPC 또한 curriculum learning 중 하나. 그럼 grasping을 진짜 더 세부화 시켜서 curriculum화 하면 어떨까?
                
                Way of constituting a curriculum: order-wise
                How can tasks/experience samples be sequenced into a curriculum?
                    -> manually
                    -> automated methods have been proposed.
                How can an agent transfer knowledge between tasks of curriculum?
                    !!Assume: MDPs share state/action spaces or task mapping(manual/automatic) occurs.
                    -> transfer learning. It is important to transfer relevant, reusable info from each task, and combine them.
                    -> knowledge can be: samples, options, policies, models, value functions
                    -> Transfer can be one-to-one or many-to-one
                Evaluation Metric:
                    - just step-value graph
                    - time to expected return threshold
                    - asymptotic performance: final performance after convergence
                    - jumpstart: initial performance increase
                    - total reward ratio: total reward accumulated by the agent during training till same stopping point
                    Weak transfer: time spent @ source task is not accounted
                    Strong transfer: source task time added
            }
            


    Research_Details:
        Task:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                1-1. Making proper observations 
                {
                    Note: Target position of a reaching primitive - Should explicitly be known to the user
                        -> User do know where it's reaching to
                }
                1-2. Sub-primitives
                {
                    Questions:
                        1. Why have sub-primitives?
                            - To construct primitives, observation spaces for each should be matched among all.
                                Reason: each primitives shall have an idea with the full knowledge of a current state
                                Question: Doesn't weight function determine how to composite primitives?
                                Defence: 
                            - But there is no need to train each primitives with full observation spaces where they can acheive their goal with much simpler observations.
                            - pretrain sub-primitives with simple observation spaces, and modify them with full observation placeholders
                        
                        2. Did X.Peng pretrained their policies with full observation spaces?
                            - Yes (Did not trained their primitives via RL, but by LofD - simplification not required)
                    Do:
                        -> Need to select:
                            1. RL-trained sub-primitives and finetune them with full obs space
                            2. Not using sub-primitives and policy transfer with expert data directly with full obs space
                    Consequences:
                        1. if select sub-primitives:
                            Csq: Finetune twice for sub-primitives -> primitives and for primitives -> composite policy
                            Result: Complicated/Time consuming to pretrain and finetune twice
                        2.  
                    Defences for Consequences:
                        Def for 1: To avoid complicated / time consumption problem
                            - May not have to finetune for primitives -> composite policies
                            Reason: 
                                1. 
                }
                1-3. Primitive fusing 
                {    
                    Questions:
                        1. How to fuse primitives with different action space?
                            - maybe weight is all responsible of fusing action spaces.
                            - if actions from both arm and wheels are important, then the weight will equally be distributed
                            - if actions from arm is more important than that of the wheels, then the weight will focus more on arms
                            - since weight incorporates total states, it can fully observe the space and provide decisions
                            - DRAWBACK: If one primitive gets majoritic value, then actions from other primitives (-> from other action spaces) will not be effective
                            -           Total policy is not a 'probability distribution' anymore.
                    References:
                        - Peng et al. 2019 (MCP)
                        - Shu et al. 2017 (Hierarchical and interpretable skill acquisition in multi-task reinforcement learning)
                        - Li et al. 2019 (Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards)
                }
                1-4. Brain Functionality
                {
                    References:
                        - Hierarchical modularity in human brain functional networks (Meunier, 2009)
                        {
                            - modular decomposition, measured using fMRI
                            - human brain functional networks have a hierarchical modular organization
                            - 5-largest, highest: medial occipital (visual), lateral occipital, central, parieto-frontal, fronto-temporal
                            ** Simon’s original hypothesis that hierarchy or near-decomposability of physical symbol systems is a critical design feature for their fast adaptivity to changing environmental conditions.
                            ** stable intermediate forms (component modules) -> allow the system to adapt one module at a time without risking loss of function in other, already-adapted modules
                            -> Need of auxilary network, not to destruct already-trained primitive 
                                - Overcoming catastrophic forgetting in neural networks (Kirkpatrick, 2017)
                                - Progressive neural networks (Rusu, 2016)
                                - Sim-to-real robot learning from pixels with progressive nets (Rusu, 2017)
                            ** modularity -> Can it measure similarity btw NN primitives? -> btw pretrained primitives and aux networks
                                - how well a given partition concentrates the edges within the modules based on the topological connectivity
                                - For Neural Network, ...?
                            - 
                        }
                        - The Architecture of Complexity (Simon, 1962)
                        {
                            - Nearly-decomposable systems
                            - most interactions with subsets close to them, and less with elements outside this subset
                            -> then, behavior part should located at the center - require fastest response with all the other processes
                        }
                        - Functional Network Organization of the Human Brain (Power, 2011)
                        {

                        }
                        - Multi-scale brain networks (Betzel, 2017)
                        - The economy of brain network organization (Bullmore, 2012)
                        - Dynamic reconfiguration of human brain networks during learning (Bassett, 2011)
                        - Structural and Functional Brain Networks: From Connections to Cognition (Park, 2013)
                        - Organization and hierarchy of the human functional brain network lead to a chain-like core (Mastrandrea, 2017)
                        - Modeling functional resting-state brain networks through neural message passing on the human connectome (Peraza-Goicolea, 2019)
                        - Deep Learning on Graph-structured Data (Lee, 2019)
                }
                1-5. Value selection
                {
                    Questions:
                        1. Does each of the pretrained primitives requires its value function to be preserved?
                            - 
                        2. Can the value function of the total policy be another new chunk of network?
                            - Maybe
                            - or, Maybe not
                    Do:
                        1. Do all:
                            - 1. Easiest: make whole new value function from scratch as a chunk of parameters
                            - 2. Hard: construct same value function for each pretrained primitives to get total sum of values, but trained from scratch
                            - 3. Harder: construct same value function with total sum of values and import parameters to train for complicated task (Fine-tunable)
                            - 4. Also Harder: same with 3 but parameter be fixed                            
                        2. For 1, get:
                            - total episodic reward
                            - convergence time
                            - environment timesteps
                }
                1-6. Algorithm Formulation
                {
                    Question:
                        1. How to formulate entropy? Just add them all by primitives?
                        2. How to initialize policy since it only guarantees the local maximum?
                }
                1-7. Baselines
                {
                    - Vanilla RL
                    - MCP with goal conditioned 1-leveled primitives
                    - HMCP
                }
                1-8. Variances from Probability Distribution
                {
                    Question:
                        1. What is the meaning of standard deviation of a primitive distribution while learning?
                            - In SAC: d
                            - In MCP: To formulate the gaussian primitive, independently influcence on the composite distribution. No info on how to optimize std, used PPO to optimize both mean and std.
                    Do:
                        1. restrict the minimum value of std
                }
            }
            2. Title: Who cares who I am: Policy transfer between robots without fine-tuning
            {
            }
            6. Title: Fully express the intention
            {
                XAI
            }
    
    Conflicts:
        Task:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                Primitives - control variables can be differ wrt. tasks when using same hardware
                (e.g. reaching tasks given position is redundant for balancing while walking
                    -> motion of arm is required, but not regarding the position of the hand, but to stabilize inertia by moving shoulder joint)
                Task specific primitives:
                    - Controlling arm when reaching / balancing [Q: uses different part of the brain -> and thus, should be trained separately.]
                -> Is our brain made out of cascading weight layers of neurons? -> then, by visualizing each weights to a series of layers can explain the intention of the AI
                -> How does it incorporate external information to the existing layers of neurons?
                -> Physical memory vs Simulated memory
            } 
            
        