Note

    Problem_specification:
        Keywords:
            1. Dexterous manipulation
            2. Strategic (time-dependent) manipulation
            3. Human data learning
            4. Multi-modal sensory data
            5. Safe to interact (torque control)
            6. Robot independent (worspace independent)
            7. Dual (multi) arm
            8. Reaching
            9. Grasping
            10. State representation
            11. Sim-to-Real
            12. Learning from Demonstration
            13. Reusable policies
            14. Explainable AI
            15. Sample efficient learning


        Tasks:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                Keywords:
                Goal: Make policies explainable (and mimick human-like behavior - if incorporating human data while TRAINING PRIMITIVES)
                Motivation:
                    - Shall let the users know what it's doing with human-interpretable information
                    - Behave in a way human brain does -> composing complicated behavior with basic structures of primitives in a hierarchical manner
                    - MCPs uses primitives that are not identifiable
                Contributions:
                    - making policy explainable
                    - Increase sample efficiency (incorporate human data while TRAINING WHOLE TASK)
                    - learn auxilary primitives that have not been told/trained (if possible)
                    - fuse policies with different action space into a whole
                    - reduce learning time compared to primitives that has same, whole action space
                    - weights coordinate each primitives: not concurrent -> execute multiple primitives at once // no latent embedding -> interpretable
                    - can be applied to policies with different action space
                    - configure proper states from observation to each primitives
                
                To_do: 
                    - Train primitives that are identified for certain tasks (from either expert trajectory or reward engineered RL scheme)
                    - Set task which requires multiple primitives:
                        Options: 
                        {
                            1. 
                            {
                                Task: Grap a glass of water and pour to a specific destination
                                Required features (primitive):
                                    - Reaching to a glass (Reaching)
                                    - Make a good grip when holding a glass (Reaching + Grasping)
                                    - Take it to a goal position when holding a glass (Reaching + Grasping + Auxilary - stabilizing glass?)
                                    - Pouring it to a goal position (Reching + Grasping + Auxilary - Pouring)
                                Thoughts:
                                    - Pouring: 
                                    - Moveit! -> variance = 0인 distribution
                                    - Optimal state representation -> No need to do these shits
                            }
                            2. 
                            {
                                Task: TBD
                                Goal: TBD
                            }
                        }
                    - State representation learning of required & orthogonal sensor data: image, pressure, joint angles
                        -> want to show that those data has some information that does not overlap with each other
                        -> check orthogonality of each sensors using [PCA? suggestions????]
                    - Provide auxilary dummy primitive networks when training in an End-to-End fashion -> do a PCA analysis for actions under states to validate the need for an extra primitive.
                        -> How to check / avoid a single auxilary network from taking over all the auxiliary primitives?
                             - by training policy with enough of aux policies and then optimize the numbers?
                        -> time: 언제 primitive가 activate 될 지??
                        -> Non-Markovian 
                    - Identify unknown primitives via (examining state/action sets from forward propagations)
                    - primitives with same action space -> MCP // primitives with different action space -> ?
                References:
                    Compositive_policy:
                        Additive:
                            - Mixture Density Networks (Bishop, 1994)
                        Multiplicative:
                            - MCP: Learning composable hierarchical control with multiplicative compositional policies (Peng, 2019)    
                    Hierarchical_policy:
                        Pure_Hierarchy
                            - FeUdal Networks for Hierarchical Reinforcement Learning (Vezhnevets, 2017)
                            - Options (Sutten, 1999)
                            - Hierarchical and interpretable skill acquisition in multi-task reinforcement learning (Shu, 2017)
                            - Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards (Li, 2019)
                            BT-RL
                                - Integrating Reinforcement Learning into Behavior Trees by Hierarchical Composition (Kartasev, 2019)
                                - QL-BT: Enhancing behaviour tree design and implementation with Q-learning (Dey, 2013)
                        Meta_Learning
                            - Learning to Coordinate Manipulation Skills Via Skill Behavior Diversification (Lee, 2020)
                            - Modular Multitask Reinforcement Learning with Policy Sketches (Andreas, 2017)
                            - Chealsea Finn paper of meta-learning
                            - MLSH (Frans, 2017)
                    Explainable_RL:
                        - Hierarchical and interpretable skill acquisition in multi-task reinforcement learning (Shu, 2017)
                    Brain_Functionality:
                        Pure_Brain:
                            - The Architecture of Complexity (Simon, 1962)
                            - Hierarchical modularity in human brain functional networks (Meunier, 2009)
                            - Functional Network Organization of the Human Brain (Power, 2011)
                        Deep_Learning_Related:
                            - Overcoming catastrophic forgetting in neural networks (Kirkpatrick, 2017)
                            - Progressive neural networks (Rusu, 2016)
                            - Sim-to-real robot learning from pixels with progressive nets (Rusu, 2017)
            }
            1-1. Sub_task: test with simpler environment
            {
                Motivation:
                    - Shall let the users know what it's doing with human-interpretable information
                    - Behave in a way human brain does -> composing complicated behavior with basic structures of primitives in a hierarchical manner
                    - MCPs uses primitives that are not identifiable
                Contributions:
                    - can be applied to policies with different action space (properly)
                    - making policy explainable by incorporating known primitives
                    - hierarchical policy

                    - Increase sample efficiency (incorporate human data while TRAINING WHOLE TASK)
                    - learn auxilary primitives that have not been told/trained (if possible) -> auxilary primitive can also achieve auxilary loss/goals to improve performance
                    - reduce learning time compared to primitives that has same, whole action space
                    - weights coordinate each primitives: not concurrent -> execute multiple primitives at once // no latent embedding -> interpretable
                        -> explicit embedding: leads to explainable AI
                    
                    - Presents correlation on std of SAC and MCP in vanilla and Hierarchical structure
                
                To Do:
                    - create tasks which needs manipulator to get involved -> different action space
                    - create cooperative task
                    - create competitive task
                
                Done (moved from ToDO):
                    - 

                References:
                    Compositive_policy:
                        Additive:
                            - Mixture Density Networks (Bishop, 1994)
                        Multiplicative:
                            - MCP: Learning composable hierarchical control with multiplicative compositional policies (Peng, 2019)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                    Hierarchical_policy:
                        Pure_Hierarchy
                            - Hierarchical and interpretable skill acquisition in multi-task reinforcement learning (Shu, 2017)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        - Goal condition is sampled from sets of goals, so the goal is explicitly comprehensible.
                                    Cons:
                                        -
                                }
                            - Data-Efficient Hierarchical Reinforcement Learning (Nachum, 2018)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        - to remedy the challenge from off-policy learning of HRL (only sequential) by off-policy correction by re-labeling
                                    Cons:
                                        - 
                                        - same state space for high/low policy + low policy taking additionl goal as a state from high
                                }
                            - Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards (Li, 2019)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            - Hierarchical Visuomotor Control of Humanoids (Merel, 2019)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                        - same state space for high/low policy + low policy taking additionl goal as a state from high
                                }
                            BT-RL
                                - QL-BT: Enhancing behaviour tree design and implementation with Q-learning (Dey, 2013)
                                - Integrating Reinforcement Learning into Behavior Trees by Hierarchical Composition (Kartasev, 2019)
                        Meta_Learning
                            - Options (Sutten, 1999)
                            - Bacon et al 2017
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            - Modular Multitask Reinforcement Learning with Policy Sketches (Andreas, 2017)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            - Feature Control as Intrinsic Motivation for Hierarchical Reinforcement Learning (Dilokthanakul, 2017) 
                                {
                                    Key:
                                        - Meta-controller provides sub-goal as a latent and an intrinsic reward at learning
                                        - Meta-controller select which layer of CNN (feature) to be used @ intrinsic reward calculation
                                    Pros:
                                        - Using sub-goals from meta-controller can work better in sparse-reward environment
                                    Cons: 
                                        - learned features from feature controlling encoder cannot be explicitly interpreted -> options from meta-controller is hard to be 
                                        - end-to-end training: can be handy but do not know its options/sub-goals explicitly.
                                        - same state space for high/low policy + low policy taking additionl goal as a state from high
                                }
                            - FeUdal Networks for Hierarchical Reinforcement Learning (Vezhnevets, 2017)
                                {
                                    Key:
                                        - Manager and Worker setting where the goal from the manager is a estimated direction for the encoded state @ Manager's side
                                        - the Manager is updated independently from the Worker in a way that the gradient of the goal is updated aligned to the direction for maximizing the Adventage of the manager
                                    Pros:
                                        - manager provides a meaningful and explicit goal for the workers (?)
                                    Cons:
                                        - if action space is large, fitting U_t from RNN of the Worker side can be hard
                                        - Still do not have a clear understanding of what the goal intends to do in a human language
                                        - same state space for high/low policy + low policy taking additionl goal as a state from high
                                }
                            - MLSH (Frans, 2017)
                            - Chealsea Finn paper of meta-learning
                            - Learning Actionable Representation with Goal-Conditioned Policies (Ghosh, 2019)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                            - Learning to Coordinate Manipulation Skills Via Skill Behavior Diversification (Lee, 2020)
                                {
                                    Key:
                                        - 
                                    Pros:
                                        -
                                    Cons:
                                        -
                                }
                    Explainable_RL:
                        - Hierarchical and interpretable skill acquisition in multi-task reinforcement learning (Shu, 2017)
                        -
                    Brain_Functionality:
                        Pure_Brain:
                            - The Architecture of Complexity (Simon, 1962)
                            - Hierarchical modularity in human brain functional networks (Meunier, 2009)
                            - Functional Network Organization of the Human Brain (Power, 2011)
                        Deep_Learning_Related:
                            - Overcoming catastrophic forgetting in neural networks (Kirkpatrick, 2017)
                            - Progressive neural networks (Rusu, 2016)
                            - Sim-to-real robot learning from pixels with progressive nets (Rusu, 2017)
                    Optimization_Algorithm:
                        - SAC
                            {
                                Key:
                                    - 
                                Pros:
                                    -
                                Cons:
                                    -
                                Questions:
                                    - Why use log_std clipping?
                            }
                        - Learning Multi-Level Hierarchies with Hindsight (Levy, 2019)
                            -> Useful for hierarchical policies in a sequential manner, but not in the case of ours
            }
            2. Title: Who cares who I am: Policy transfer between robots without fine-tuning
            {
                Keywords: 
                
                Goal: Make policies that can be transfered to different robot models without fine-tuning
                Motivation:
                    - Most of the techniques uses joint angles -> restricted to the specific configuration of a robot
                Contributions:
                    - Can transfer policy without the need of fine-tuning within the simulated env
                    - Can transfer policy to a real robot (if possible)
                Limitation:
                    - IK algorithm should be embedded
                To Do:
                    - Use actions as ∆p ∆v of an End Effecor instead of joint angles or joint velocities
                    - Use action/observation scaling with respect to the workbound of each robots (NOT FINETUNING! CAN EASILY BE DONE)
                        (if the scale of jaco : mini jaco = 10 : 1, multiply 10 to each obs/action when using the policy trained from mini jaco to apply to a jaco)
                    - transfer policy without fine-tuning and compare episode rewards at test time
                Thoughts:
                    - Put vision/PC to EE
                    - How to handle vision/PC
                    - If DOF gets to be differ?
            }   
            3. Title: Dexterous Dual-Arm Manipulation via Compositive Primitives
            {
                Keywords: 
                Goal: TBD
                Motivation: TBD
                References:
                    Hierarchical_MARL:
                        - Multi-Agent Manipulation via Locomotion using Hierarchical Sim2Real (Nachum, 2019)
            }
            4. Title: Screw-fixing
            {
                Keywords:     
                Goal: TBD
                Motivation: TBD
                To Do:
                    - time-dependent 
            }
            5. Title: Plugging with sample-efficient policy by composing primitives
            {
                Keywords:     
                Goal: TBD
                Motivation: TBD
            }
            6. Title: Fully express the intention
            {
                Keywords:     
                Goal: Fully express the intention while doing a task by proper / mixture of methods
                Motivation: 
                Notes:
                    - Feature Importance, Permutation:
                        - Determine which feature is the most import one by randomly replacing values for each feature
                        - Importance by error measurement
                        - Cons: No direction, Scale independent, Feature independent
                    - Partial Dependence Plots
                        - Determine the most import feature by 'linearly' replacing values
                        - Pros: Directions, Scale dependent
                        - Cons: Computationally expensive
                    - Global Surrogate
                    - Local Interpretable Model-agnostic Explanations (LIME)
                    - Shapley Additive exPlanations (SHAP)
                    - Filter Visualization
                    - Layer-wise Relevane Propagation (LRP)
            }
            7. State-Action Coupling to focus on states that really matters - specify elements of latent variables to focus


    Research_Details:
        Task:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                1-1. Making proper observations 
                {
                    Note: Target position of a reaching primitive - Should explicitly be known to the user
                        -> User do know where it's reaching to
                }
                1-2. Sub-primitives
                {
                    Questions:
                        1. Why have sub-primitives?
                            - To construct primitives, observation spaces for each should be matched among all.
                                Reason: each primitives shall have an idea with the full knowledge of a current state
                                Question: Doesn't weight function determine how to composite primitives?
                                Defence: 
                            - But there is no need to train each primitives with full observation spaces where they can acheive their goal with much simpler observations.
                            - pretrain sub-primitives with simple observation spaces, and modify them with full observation placeholders
                        
                        2. Did X.Peng pretrained their policies with full observation spaces?
                            - Yes (Did not trained their primitives via RL, but by LofD - simplification not required)
                    Do:
                        -> Need to select:
                            1. RL-trained sub-primitives and finetune them with full obs space
                            2. Not using sub-primitives and policy transfer with expert data directly with full obs space
                    Consequences:
                        1. if select sub-primitives:
                            Csq: Finetune twice for sub-primitives -> primitives and for primitives -> composite policy
                            Result: Complicated/Time consuming to pretrain and finetune twice
                        2.  
                    Defences for Consequences:
                        Def for 1: To avoid complicated / time consumption problem
                            - May not have to finetune for primitives -> composite policies
                            Reason: 
                                1. 
                }
                1-3. Primitive fusing 
                {    
                    Questions:
                        1. How to fuse primitives with different action space?
                            - maybe weight is all reponsible of fusing action spaces.
                            - if actions from both arm and wheels are important, then the weight will equally be distributed
                            - if actions from arm is more important than that of the wheels, then the weight will focus more on arms
                            - since weight incorporates total states, it can fully observe the space and provide decisions
                            - DRAWBACK: If one primitive gets majoritic value, then actions from other primitives (-> from other action spaces) will not be effective
                            -           Total policy is not a 'probability distribution' anymore.
                    References:
                        - Peng et al. 2019 (MCP)
                        - Shu et al. 2017 (Hierarchical and interpretable skill acquisition in multi-task reinforcement learning)
                        - Li et al. 2019 (Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards)
                }
                1-4. Brain Functionality
                {
                    References:
                        - Hierarchical modularity in human brain functional networks (Meunier, 2009)
                        {
                            - modular decomposition, measured using fMRI
                            - human brain functional networks have a hierarchical modular organization
                            - 5-largest, highest: medial occipital (visual), lateral occipital, central, parieto-frontal, fronto-temporal
                            ** Simon’s original hypothesis that hierarchy or near-decomposability of physical symbol systems is a critical design feature for their fast adaptivity to changing environmental conditions.
                            ** stable intermediate forms (component modules) -> allow the system to adapt one module at a time without risking loss of function in other, already-adapted modules
                            -> Need of auxilary network, not to destruct already-trained primitive 
                                - Overcoming catastrophic forgetting in neural networks (Kirkpatrick, 2017)
                                - Progressive neural networks (Rusu, 2016)
                                - Sim-to-real robot learning from pixels with progressive nets (Rusu, 2017)
                            ** modularity -> Can it measure similarity btw NN primitives? -> btw pretrained primitives and aux networks
                                - how well a given partition concentrates the edges within the modules based on the topological connectivity
                                - For Neural Network, ...?
                            - 
                        }
                        - The Architecture of Complexity (Simon, 1962)
                        {
                            - Nearly-decomposable systems
                            - most interactions with subsets close to them, and less with elemtns outside this subset
                            -> then, behavior part should located at the center - require fastest response with all the other processes
                        }
                        - Functional Network Organization of the Human Brain (Power, 2011)
                        {

                        }
                        - Multi-scale brain networks (Betzel, 2017)
                        - The economy of brain network organization (Bullmore, 2012)
                        - Dynamic reconfiguration of human brain networks during learning (Bassett, 2011)
                        - Structural and Functional Brain Networks: From Connections to Cognition (Park, 2013)
                        - Organization and hierarchy of the human functional brain network lead to a chain-like core (Mastrandrea, 2017)
                        - Modeling functional resting-state brain networks through neural message passing on the human connectome (Peraza-Goicolea, 2019)
                        - Deep Learning on Graph-structured Data (Lee, 2019)
                }
                1-5. Value selection
                {
                    Questions:
                        1. Does each of the pretrained primitives requires its value function to be preserved?
                            - 
                        2. Can the value function of the total policy be another new chunk of network?
                            - Maybe
                            - or, Maybe not
                    Do:
                        1. Do all:
                            - 1. Easiest: make whole new value function from scratch as a chunk of parameters
                            - 2. Hard: construct same value function for each pretrained primitives to get total sum of values, but trained from scratch
                            - 3. Harder: construct same value function with total sum of values and import parameters to train for complicated task (Fine-tunable)
                            - 4. Also Harder: same with 3 but parameter be fixed                            
                        2. For 1, get:
                            - total episodic reward
                            - convergence time
                            - environment timesteps
                }
                1-6. Algorithm Formulation
                {
                    Question:
                        1. How to formulate entropy? Just add them all by primitives?
                        2. How to initialize policy since it only guarantees the local maximum?
                }
                1-7. Baselines
                {
                    - Vanilla RL
                    - MCP with goal conditioned 1-leveled primitives
                    - HMCP
                }
                1-8. Variances from Probability Distribution
                {
                    Question:
                        1. What is the meaning of standard deviation of a primitive distribution while learning?
                            - In SAC: d
                            - In MCP: To formulate the gaussian primitive, independently influcence on the composite distribution. No info on how to optimize std, used PPO to optimize both mean and std.
                }
            }
            2. Title: Who cares who I am: Policy transfer between robots without fine-tuning
            {
            }
            6. Title: Fully express the intention
            {
                XAI
            }
    
    Conflicts:
        Task:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                Primitives - control variables can be differ wrt. tasks when using same hardware
                (e.g. reaching tasks given position is redundant for balancing while walking
                    -> motion of arm is required, but not regarding the position of the hand, but to stabilize inertia by moving shoulder joint)
                Task specific primitives:
                    - Controlling arm when reaching / balancing [Q: uses different part of the brain -> and thus, should be trained separately.]
                -> Is our brain made out of cascading weight layers of neurons? -> then, by visualizing each weights to a series of layers can explain the intention of the AI
                -> How does it incorporate external information to the existing layers of neurons?
                -> Physical memory vs Simulated memory
            } 
            
        