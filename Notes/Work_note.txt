Mobile robot + jaco mujoco model
{
    root.xml
        shared.xml 
            <asset> all the meshes with stl
            <asset> all the objects with stl + png
            <contact> excluding contacts
            <default> colors under class

        <worldbody> environments
        <worldbody> mobile robots
            <body> mocap def
            <body> links with childclass="dual_ur5_husky"
            ur5_l.xml
            ur5_r.xml

        <worldbody> objects

        <actuator> base joints

        gripper.xml
            <tendon> joints
            <actuator> tendons-joints
}

SAC policies
{
    SAC(OffPolicyRLModel(BaseRLModel))
    (
        layers: layers
        policy: LnMlpPolicy(FeedForwardPolicy(SACPolicy(BasePolicy)))
    )

    policy
        if not deterministic:
            mu = dense(mlp(flatten(obs_ph),layer_structure),ac_space)
            std = exp(clip(dense(mlp(flatten(obs_ph),layer_structure),ac_space),min=-20,max=2))
            policy = tanh(mu + normal*std)
        else:
            mu = dense(mlp(flatten(obs_ph),layer_structure),ac_space)
            policy = mu
}


aux network + fine tuning
{
    from stable_baselines.sac.policies import MlpPolicy

    def train_with_additional_layer(self, model_dir):        
        env = JacoMujocoEnv(**vars(self.args))

        # tf.graph -> Use the one in SAC_MULTI.setup_model()
        # 1. set the structure of unknown layers -> []
        # 2. define name of the unknown layers -> {'name': np.array}
        # 3. call pretrained policies and extract the structure of layers by name / store paramters in a temp buffer -> {'name': (structure -> [], parameters -> np.array)}
        # 4. compose total structure of the total policy -> []
        # 5. update model policy with the structure of the total policy
        # 6. setup a graph with variables

        # Within SAC_MULTI(OffPolicyRLModel(BaseRLModel))               common/base_class.py
        @classmethod
        @abstractmethod
        def construct_primitive_info(obs_dimension, obs_range, obs_index, act_dimension, act_range, act_index, layer_structure):
            pass

        # Within SAC_MULTI(OffPolicyRLModel)                            common/base_class.py
        @classmethod
        def construct_primitive_info(obs_dimension, obs_range: Union[str, list], obs_index, act_dimension, act_range, act_index, layer_structure) -> dict:
            '''
            Returns info of the primtive as a dictionary

            :param obs_dimension: (int) observation space dimension for the primitive
            :param obs_range: ([float, float] or [int, int] or int) observation range. If int, then range fixed to 0
            :param obs_index: ([int, ...]) list of indices of the observation for the primitive
            :param act_dimension: (int) action space dimension for the primitive
            :param act_range: ([float, float] or [int, int]) action range
            :param act_index: ([int, ...]) list of indices of the action for the primitive
            :param layer_structure: ([int, ...]) hidden layer structure of the primitive
            :return: (dict: {'obs':tuple, 'act':tuple, 'layer':list}) primitive information
            '''
            # TODO: assert Error for the wrong type
            # TODO: wrong obs_dimension size from the size of the obs_index
            if isinstance(obs_range, list):
                obs_range_max = np.array([max(obs_range)]*obs_dimension)
                obs_range_min = np.array([min(obs_range)]*obs_dimension)
                obs = (spaces.Box(obs_range_min, obs_range_max, dtype=np.float32), obs_index.sort())
            elif isinstance(obs_range, int):
                obs_range_array = np.array([0]*obs_dimension)
                obs = (spaces.Box(obs_range_array, obs_range_array, dtype=np.float32), obs_index.sort())
            else:
                raise TypeError("[ERROR]: obs_range wrong type - Should be list or int. Received {}".format(type(obs_range)))

            act_range_max = np.array([max(act_range)]*act_dimension)
            act_range_min = np.array([min(act_range)]*act_dimension)
            act = (spaces.Box(act_range_min, act_range_max, dtype=np.float32), act_index.sort())

            return {'obs': obs, 'act': act, 'layer': layer_structure}

        # NOTE: name for primitive
        # If pretrained, then 'freeze/' must be added in front of the name in order to prevent forgetting
        # else, put 'train/'
        primitives = OrderedDict()
    
        # NOTE: newly appointed primitives
        # TODO: insert info in primitives dict within the function
        primitives['train/aux1'] = construct_primitive_info(obs_dimension=6, obs_range=[-3, 3], obs_index=[0, 1, 2, 4, 5], 
                                                    act_dimension=6, act_range=[-1.4, 1.4], layer_structure=[256, 256])
        primitives['train/aux2'] = construct_primitive_info(6, [-3, 3], [3, 4, 5, 8], 
                                                    6, [-1.4, 1.4], [256, 256])
        # TODO: input space should be compared with the env.obs space
        primitives['train/weight'] = construct_primitive_info(total_obs_dim, 0, list(range(total_obs_dim)), 
                                                    number_of_primitives, [0,1], [256, 256])

        # NOTE: Pretrained primitives
        # tuple (data -> OrderedDict, param -> OrderedDict)
        # param = {'name': value (np.array)}
        primitives['freeze/reaching'] = BaseRLModel._load_from_file(policy_zip_path)
        primitives['freeze/grasping'] = BaseRLModel._load_from_file(policy_zip_path)

        policy = MlpPolicy
        self.trainer = SAC_MULTI.pretrainer_load(policy, primitives, env)

        # Within SAC_MULTI(OffPolicyRLModel(BaseRLModel))               common/base_class.py
        @classmethod
        @abstractmethod
        def pretrainer_load(cls, policy, primitives, env, **kwargs):
            """
            Construct trainer from policy structure

            :param policy: (Policy Class) class of SAC policy
            :param primitives: (dict) primitives by name to which items assigned as an info of obs/act/layer_structure
            :param env: (Gym Environment) the new environment to run the loaded model on
            :param kwargs: extra arguments to change the model when loading
            """
            raise NotImplementedError()

        # Within SAC_MULTI(OffPolicyRLModel)                            common/base_class.py
        @classmethod
        def pretrainer_load(cls, policy, primitives, env, **kwargs) -> SAC_MULTI:
            """
            Construct trainer from policy structure

            :param policy: (Policy Class) class of SAC policy
            :param primitives: (dict) primitives by name to which items assigned as an info of obs/act/layer_structure
            :param env: (Gym Environment) the new environment to run the loaded model on
            :param kwargs: extra arguments to change the model when loading
            """

            # TODO: policy_kwargs?
            if 'policy_kwargs' in kwargs and kwargs['policy_kwargs'] != data['policy_kwargs']:
                raise ValueError("The specified policy kwargs do not equal the stored policy kwargs. "
                                "Stored kwargs: {}, specified kwargs: {}".format(data['policy_kwargs'],
                                                                                kwargs['policy_kwargs']))
            # SAC_MULTI
            model = cls(policy=policy, env=None, _init_setup_model=False)
            
            # get total_obs_dim, total_act_dim
            dims = check_primitive(primitives)
            data = {'observation_space': Box(dims[0][0], dims[0][1], dtype=np.float32), 'action_space': Box(dims[1][0], dims[1][0], dtype=np.float32)}
            model.__dict__.update(data)

            # Nothing updated
            model.__dict__.update(kwargs)
            # TODO: find out what kind of keys in kwargs exists
            
            # check env is a VecEnv -> always False. (Only train our policy with 1 environment)
            model.set_env(env)
            # TODO: env.ob_space and env.ac_space should match with the one from __dict__update(data)
            # -> env.ob_space and env.ac_space should be the total/final obs and acts

            # make self.graph
            # with graph: init self.sess
            # with tf.Variable_scope('input), make:
            #   self.policy = self.policy(self.sess, self.ob_space, self.ac_space, self.layers)
            #   self.target_policy = self.policy(self.sess, self.ob_space, self.ac_space, self.layers)
            #   initialize input placeholders
            # with tf.Variable_scope('model'), make:
            #   det_policy, sto_policy, logp_pi = self.policy.make_actor(input_ph)
            #   qf1, qf2, value = self.policy.make_critics(input_ph, actions_ph)
            #   qf1_pi, qf2_pi, _ = self.policy.make_critics(input_ph, sto_policy, vf=False, reuse=True)
            # with tf.Variable_scope('target'), make:
            #   _, _, value_target = self.target_policy.make_critics(next_input_ph, qf=False)
            # with tf.Variable_scope('loss'), make:
            # policy_train_op
            # train_value_op
            # target_init_op
            # initialize Values and target network
            model.setup_custom_model(primitives)
            # TODO: 

            # from self.tf_Variables of network:
            #   make load_ops -> {'name from self.variables': (ph, tf.Variable.assign(ph))}
            # for each layer name:
            #   make feed_dict -> {'ph of the name': value of the name}
            #   make param_update_ops -> [tf.Variable.assign(ph)]
            # after building feed_dict, update_ops, run update_ops with feed_dict
            model.load_parameters(params, exact_match=False)
            # TODO: 

            # return SAC trainer
            return model
        
        # Within SAC_MULTI(OffPolicyRLModel)                            common/base_class.py
        @classmethod
        def check_primitive(self, primitives: dict) -> list:
            '''
            dims[0][0] = obs min np.array, dims[0][1] = obs max np.array
            dims[1][0] = act min np.array, dims[1][1] = act max np.array
            '''
            
            try:
                obs_dim = primitives['train/weight']['obs'][1][-1]
                obs_min_array = np.array([-float('inf')]*obs_dim)
                obs_max_array = np.array([float('inf')]*obs_dim)
                act_dim = primitives['train/weight']['act'][1][-1]
                act_min_array = np.array([-float('inf')]*act_dim)
                act_max_array = np.array([float('inf')]*act_dim)
            except Exception:
                print("[ERROR]: No primitive name train/weight. YOU MUST HAVE IT")

            for name, info_dict in primitives.items():
                obs_min_prim = info_dict['obs'][0].low.min()
                obs_max_prim = info_dict['obs'][0].high.max()
                act_min_prim = info_dict['act'][0].low.min()
                act_max_prim = info_dict['act'][0].high.max()
                for index in info_dict['obs'][1]:
                    obs_min_array[index] = obs_min_prim
                    if obs_min_array[index] not in [-float('inf'), obs_min_prim]:
                        print("[WARNING]: You are about to overwrite min bound of obs[{0}] with {1}".format(obs_min_array[index], obs_min_prim))
                    obs_max_array[index] = obs_max_prim
                    if obs_max_array[index] not in [float('inf'), obs_max_prim]:
                        print("[WARNING]: You are about to overwrite max bound of obs[{0}] with {1}".format(obs_max_array[index], obs_max_prim))
                    act_min_array[index] = act_min_prim
                    if act_min_array[index] not in [-float('inf'), act_min_prim]:
                        print("[WARNING]: You are about to overwrite min bound of obs[{0}] with {1}".format(act_min_array[index], act_min_prim))
                    act_max_array[index] = act_max_prim
                    if act_max_array[index] not in [float('inf'), act_max_prim]:
                        print("[WARNING]: You are about to overwrite max bound of obs[{0}] with {1}".format(act_max_array[index], act_max_prim))
            dims = [[obs_min_array, obs_max_array], [act_min_array, act_max_array]]

            return dims

        @classmethod
        def init_layers(layers: dict) -> dict:
            '''
            receive layers, convert to np.array with given size if list
            1. loadable_parameters = [tf.Variables with name, shape, dtype]
            2. self._param_load_ops = {'name':(ph, tf.Variable.assign(ph))}
            3. params = {'name': np.array with shape}
            4. feed_dict = {self._param_load_ops[name] -> ph : params[name] -> np.array}
            5. param_update_ops = [tf.Variable.assign(ph)]
            But to make tf.Variable, need to construct a graph with session
            '''
            # Q. Can Layer Normalized policies be mixed with unNormalized policies?
            # Assume: No Layer Normalization for just a fc policy
            # name: e.g. 'aux1/model/pi/fc0/kernel:0

            for name, item in layers.items():
                if isinstance(item, list):
                    layer_od = OrderedDict()
                    networks = ['model/pi', 'model/values_fn/vf', 'model/values_fn/qf1', 'model/values_fn/qf2', 'target/values_fn/vf']
                    for network in networks:
                        # First layer6
                        kernel_layer_name = name + "/{0}/fc{1}/kernel:0".format(network,0)
                        bias_layer_name = name + "/{0}/fc{1}/bias:0".format(network,0)
                        obs_space_size = ?
                        # TODO: replace np.zeros with random init module
                        kernel_value = np.array(np.zeros([obs_space_size, item[0]]))
                        bias_value = np.array(np.zeros([item[0]]))
                        layer_od[kernel_layer_name] = kernel_value
                        layer_od[bias_layer_name] = bias_value

                        # Hidden layer
                        for i in range(len(item)-1):
                            # append np.array
                            kernel_layer_name = name + "/{0}/fc{1}/kernel:0".format(network,i+1)
                            bias_layer_name = name + "/{0}/fc{1}/bias:0".format(network,i+1)
                            # TODO: replace np.zeros with random init module
                            kernel_value = np.array(np.zeros([item[i], item[i+1]]))
                            bias_value = np.array([item[i+1]])
                            layer_od[kernel_layer_name] = kernel_value
                            layer_od[bias_layer_name] = bias_value

                        # Output layer
                        if network == 'model/pi':
                            output = 'dense'
                            mu_kernel_layer_name = name + "/{0}/{1}/kernel:0".format(network, output)
                            mu_bias_layer_name = name + "/{0}/{1}/bias:0".format(network, output)
                            std_kernel_layer_name = name + "/{0}/{1}_1/kernel:0".format(network, output)
                            std_bias_layer_name = name + "/{0}/{1}_1/bias:0".format(network, output)
                            # TODO: replace np.zeros with random init module
                            mu_kernel_value = np.array(np.zeros([item[-1], 1]))
                            mu_bias_value = np.array([1])
                            std_kernel_value = np.array(np.zeros([item[-1], 1]))
                            std_bias_value = np.array([1])
                            layer_od[mu_kernel_layer_name] = mu_kernel_value
                            layer_od[mu_bias_layer_name] = mu_bias_value
                            layer_od[std_kernel_layer_name] = std_kernel_value
                            layer_od[std_bias_layer_name] = std_bias_value
                        else:
                            output = network.split("/")[-1]
                            kernel_layer_name = name + "/{0}/{1}/kernel:0".format(network, output)
                            bias_layer_name = name + "/{0}/{1}/bias:0".format(network, output)
                            # TODO: replace np.zeros with random init module
                            kernel_value = np.array(np.zeros([item[-1], 1]))
                            bias_value = np.array([1])
                            layer_od[kernel_layer_name] = kernel_value
                            layer_od[bias_layer_name] = bias_value        
                else:
                    pass

        # Within SAC_MULTI                                              sac_multi/sac_multi.py
        def set_env(self, env):
            """
            Checks the validity of the environment, and if it is coherent, set it as the current environment.

            :param env: (Gym Environment) The environment for learning a policy
            """
            if env is None and self.env is None:
                if self.verbose >= 1:
                    print("Loading a model without an environment, "
                        "this model cannot be trained until it has a valid environment.")
                return
            elif env is None:
                raise ValueError("Error: trying to replace the current environment with None")

            # sanity checking the environment
            assert self.observation_space == env.observation_space, \
                "Error: the environment  passed must have at least the same observation space as the model was trained on. self.obs = {0}, env.obs = {1}".format(self.observation_space, env.observation_space)
            assert self.action_space == env.action_space, \
                "Error: the environment passed must have at least the same action space as the model was trained on."
            if self._requires_vec_env:
                assert isinstance(env, VecEnv), \
                    "Error: the environment passed is not a vectorized environment, however {} requires it".format(
                        self.__class__.__name__)
                assert not self.policy.recurrent or self.n_envs == env.num_envs, \
                    "Error: the environment passed must have the same number of environments as the model was trained on." \
                    "This is due to the Lstm policy not being capable of changing the number of environments."
                self.n_envs = env.num_envs
            else:
                # for models that dont want vectorized environment, check if they make sense and adapt them.
                # Otherwise tell the user about this issue
                if isinstance(env, VecEnv):
                    if env.num_envs == 1:
                        env = _UnvecWrapper(env)
                        self._vectorize_action = True
                    else:
                        raise ValueError("Error: the model requires a non vectorized environment or a single vectorized "
                                        "environment.")
                else:
                    self._vectorize_action = False

                self.n_envs = 1

            self.env = env
            self._vec_normalize_env = unwrap_vec_normalize(env)

            # Invalidated by environment change.
            self.episode_reward = None
            self.ep_info_buf = None

        # Within SAC_MULTI(OffPolicyRLModel(BaseRLModel))               common/base_class.py
        @abstractmethod
        def setup_custom_model(self, primitives):
            pass

        # Within SAC_MULTI(OffPolicyRLModel)                            common/base_class.py
        @abstractmethod
        def setup_custom_model(self, primitives):
            pass

        # Within SAC_MULTI                                              sac_multi/sac_multi.py
        def setup_custom_model(self, primitives):
            with SetVerbosity(self.verbose):
                self.graph = tf.Graph()
                with self.graph.as_default():
                    self.set_random_seed(self.seed)
                    self.sess = tf_util.make_session(num_cpu=self.n_cpu_tf_sess, graph=self.graph)
                    self.replay_buffer = ReplayBuffer(self.buffer_size)

                    with tf.variable_scope("input", reuse=False):
                        # Create custom policy and target TF objects
                        self.policy_tf = self.policy(self.sess, self.observation_space, self.action_space, layers=self.layers,
                                                    **self.policy_kwargs)
                        self.target_policy = self.policy(self.sess, self.observation_space, self.action_space, layers=self.layers,
                                                        **self.policy_kwargs)
                        
                        # Initialize Placeholders
                        self.observations_ph = self.policy_tf.obs_ph
                        # Normalized observation for pixels
                        self.processed_obs_ph = self.policy_tf.processed_obs
                        self.next_observations_ph = self.target_policy.obs_ph
                        self.processed_next_obs_ph = self.target_policy.processed_obs
                        # None
                        self.action_target = self.target_policy.action_ph
                        self.terminals_ph = tf.placeholder(tf.float32, shape=(None, 1), name='terminals')
                        self.rewards_ph = tf.placeholder(tf.float32, shape=(None, 1), name='rewards')
                        self.actions_ph = tf.placeholder(tf.float32, shape=(None,) + self.action_space.shape,
                                                        name='actions')
                        self.learning_rate_ph = tf.placeholder(tf.float32, [], name="learning_rate_ph")

                    with tf.variable_scope("model", reuse=False):
                        # Create the policy
                        # first return value corresponds to deterministic actions
                        # policy_out corresponds to stochastic actions, used for training
                        # logp_pi is the log probability of actions taken by the policy
                        self.deterministic_action, policy_out, logp_pi = self.policy_tf.make_custom_actor(self.processed_obs_ph, primitives)
                        # Monitor the entropy of the policy,
                        # this is not used for training
                        self.entropy = tf.reduce_mean(self.policy_tf.entropy)
                        #  Use two Q-functions to improve performance by reducing overestimation bias.
                        qf1, qf2, value_fn = self.policy_tf.make_custom_critics(self.processed_obs_ph, self.actions_ph,
                                                                        create_qf=True, create_vf=True)
                        qf1_pi, qf2_pi, _ = self.policy_tf.make_custom_critics(self.processed_obs_ph,
                                                                        policy_out, create_qf=True, create_vf=False,
                                                                        reuse=True)

                        # Target entropy is used when learning the entropy coefficient
                        if self.target_entropy == 'auto':
                            # automatically set target entropy if needed
                            self.target_entropy = -np.prod(self.action_space.shape).astype(np.float32)
                        else:
                            # Force conversion
                            # this will also throw an error for unexpected string
                            self.target_entropy = float(self.target_entropy)

                        # The entropy coefficient or entropy can be learned automatically
                        # see Automating Entropy Adjustment for Maximum Entropy RL section
                        # of https://arxiv.org/abs/1812.05905
                        if isinstance(self.ent_coef, str) and self.ent_coef.startswith('auto'):
                            # Default initial value of ent_coef when learned
                            init_value = 1.0
                            if '_' in self.ent_coef:
                                init_value = float(self.ent_coef.split('_')[1])
                                assert init_value > 0., "The initial value of ent_coef must be greater than 0"

                            self.log_ent_coef = tf.get_variable('log_ent_coef', dtype=tf.float32,
                                                                initializer=np.log(init_value).astype(np.float32))
                            self.ent_coef = tf.exp(self.log_ent_coef)
                        else:
                            # Force conversion to float
                            # this will throw an error if a malformed string (different from 'auto')
                            # is passed
                            self.ent_coef = float(self.ent_coef)

                    with tf.variable_scope("target", reuse=False):
                        # Create the value network
                        _, _, value_target = self.target_policy.make_custom_critics(self.processed_next_obs_ph,
                                                                            create_qf=False, create_vf=True)
                        self.value_target = value_target

                    with tf.variable_scope("loss", reuse=False):
                        # Take the min of the two Q-Values (Double-Q Learning)
                        min_qf_pi = tf.minimum(qf1_pi, qf2_pi)

                        # Target for Q value regression
                        q_backup = tf.stop_gradient(
                            self.rewards_ph +
                            (1 - self.terminals_ph) * self.gamma * self.value_target
                        )

                        # Compute Q-Function loss
                        # TODO: test with huber loss (it would avoid too high values)
                        qf1_loss = 0.5 * tf.reduce_mean((q_backup - qf1) ** 2)
                        qf2_loss = 0.5 * tf.reduce_mean((q_backup - qf2) ** 2)

                        # Compute the entropy temperature loss
                        # it is used when the entropy coefficient is learned
                        ent_coef_loss, entropy_optimizer = None, None
                        if not isinstance(self.ent_coef, float):
                            ent_coef_loss = -tf.reduce_mean(
                                self.log_ent_coef * tf.stop_gradient(logp_pi + self.target_entropy))
                            entropy_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)

                        # Compute the policy loss
                        # Alternative: policy_kl_loss = tf.reduce_mean(logp_pi - min_qf_pi)
                        policy_kl_loss = tf.reduce_mean(self.ent_coef * logp_pi - qf1_pi)

                        # NOTE: in the original implementation, they have an additional
                        # regularization loss for the Gaussian parameters
                        # this is not used for now
                        # policy_loss = (policy_kl_loss + policy_regularization_loss)
                        policy_loss = policy_kl_loss


                        # Target for value fn regression
                        # We update the vf towards the min of two Q-functions in order to
                        # reduce overestimation bias from function approximation error.
                        v_backup = tf.stop_gradient(min_qf_pi - self.ent_coef * logp_pi)
                        value_loss = 0.5 * tf.reduce_mean((value_fn - v_backup) ** 2)

                        values_losses = qf1_loss + qf2_loss + value_loss

                        # Policy train op
                        # (has to be separate from value train op, because min_qf_pi appears in policy_loss)
                        policy_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)
                        # NOTE: params of pretrained networks should not be fine-tuned to avoid forgetting
                        policy_train_op = policy_optimizer.minimize(policy_loss, var_list=tf_util.get_trainable_vars('model/pi/train'))

                        # Value train op
                        value_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)
                        values_params = tf_util.get_trainable_vars('model/values_fn')

                        source_params = tf_util.get_trainable_vars("model/values_fn/vf")
                        target_params = tf_util.get_trainable_vars("target/values_fn/vf")

                        # Polyak averaging for target variables
                        self.target_update_op = [
                            tf.assign(target, (1 - self.tau) * target + self.tau * source)
                            for target, source in zip(target_params, source_params)
                        ]
                        # Initializing target to match source variables
                        target_init_op = [
                            tf.assign(target, source)
                            for target, source in zip(target_params, source_params)
                        ]

                        # Control flow is used because sess.run otherwise evaluates in nondeterministic order
                        # and we first need to compute the policy action before computing q values losses
                        with tf.control_dependencies([policy_train_op]):
                            train_values_op = value_optimizer.minimize(values_losses, var_list=values_params)

                            self.infos_names = ['policy_loss', 'qf1_loss', 'qf2_loss', 'value_loss', 'entropy']
                            # All ops to call during one training step
                            self.step_ops = [policy_loss, qf1_loss, qf2_loss,
                                            value_loss, qf1, qf2, value_fn, logp_pi,
                                            self.entropy, policy_train_op, train_values_op]

                            # Add entropy coefficient optimization operation if needed
                            if ent_coef_loss is not None:
                                with tf.control_dependencies([train_values_op]):
                                    ent_coef_op = entropy_optimizer.minimize(ent_coef_loss, var_list=self.log_ent_coef)
                                    self.infos_names += ['ent_coef_loss', 'ent_coef']
                                    self.step_ops += [ent_coef_op, ent_coef_loss, self.ent_coef]

                        # Monitor losses and entropy in tensorboard
                        tf.summary.scalar('policy_loss', policy_loss)
                        tf.summary.scalar('qf1_loss', qf1_loss)
                        tf.summary.scalar('qf2_loss', qf2_loss)
                        tf.summary.scalar('value_loss', value_loss)
                        tf.summary.scalar('entropy', self.entropy)
                        if ent_coef_loss is not None:
                            tf.summary.scalar('ent_coef_loss', ent_coef_loss)
                            tf.summary.scalar('ent_coef', self.ent_coef)

                        tf.summary.scalar('learning_rate', tf.reduce_mean(self.learning_rate_ph))

                    # Retrieve parameters that must be saved
                    self.params = tf_util.get_trainable_vars("model")
                    self.target_params = tf_util.get_trainable_vars("target/values_fn/vf")

                    # Initialize Variables and target network
                    with self.sess.as_default():
                        self.sess.run(tf.global_variables_initializer())
                        self.sess.run(target_init_op)

                    self.summary = tf.summary.merge_all()

        # Within MlpPolicy(FeedForwardPolicy(SACPolicy))                sac_multi/policies.py
        @abstractmethod
        def make_custom_actor(self, primitives, obs=None, reuse=False, scope="pi"):
            """
            Creates an actor object

            :param obs: (TensorFlow Tensor) The observation placeholder (can be None for default placeholder)
            :param reuse: (bool) whether or not to reuse parameters
            :param scope: (str) the scope name of the actor
            :return: (TensorFlow Tensor) the output tensor
            """
            raise NotImplementedError

        # Within MlpPolicy(FeedForwardPolicy(SACPolicy))                sac_multi/policies.py
        @abstractmethod
        def make_custom_critics(self, primitives, obs=None, action=None, reuse=False,
                        scope="values_fn", create_vf=True, create_qf=True):
            """
            Creates the two Q-Values approximator along with the Value function

            :param obs: (TensorFlow Tensor) The observation placeholder (can be None for default placeholder)
            :param action: (TensorFlow Tensor) The action placeholder
            :param reuse: (bool) whether or not to reuse parameters
            :param scope: (str) the scope name
            :param create_vf: (bool) Whether to create Value fn or not
            :param create_qf: (bool) Whether to create Q-Values fn or not
            :return: ([tf.Tensor]) Mean, action and log probability
            """
            raise NotImplementedError

        # Within MlpPolicy(FeedForwardPolicy)                           sac_multi/policies.py
        def make_custom_actor(self, primitives, obs=None, reuse=False, scope="pi"):
            if obs is None:
                obs = self.processed_obs
            for name, item in primitives.items():
                with tf.variable_scope(scope + "/" + name, reuse=reuse):
                    if self.feature_extraction == "cnn":
                        pi_h = self.cnn_extractor(obs, **self.cnn_kwargs)
                    else:
                        pi_h = tf.layers.flatten(obs)
                    
                    seive_layer = np.zeros([item['obs_dimension'], len(item['obs_index'])])
                    for i in range(len(item['obs_index'])):
                        seive_layer[item['obs_index'][i]][i] = 1
                    pi_h = tf.matmul(pi_h, seive_layer)

                    pi_h = mlp(pi_h, item['layer_structure'], self.activ_fn, layer_norm=self.layer_norm)

                    if name == weight:
                        weight = tf.layers.dense(pi_h, item['act_dimension'], activation=None)
                        # TODO: apply softmax to weight
                        #self.weight_softmax = tf.layers.dense(weight, )
                    else:
                        self.act_mu = mu_ = tf.layers.dense(pi_h, item['act_dimension'], activation=None)
                        # Important difference with SAC and other algo such as PPO:
                        # the std depends on the state, so we cannot use stable_baselines.common.distribution
                        log_std = tf.layers.dense(pi_h, item['act_dimension'], activation=None)

                if name == weight:
                    pass
                else:
                    log_std = tf.clip_by_value(log_std, LOG_STD_MIN, LOG_STD_MAX)

                    {
                        self.std = std = tf.exp(log_std)
                        # Reparameterization trick
                        pi_ = mu_ + tf.random_normal(tf.shape(mu_)) * std
                        logp_pi = gaussian_likelihood(pi_, mu_, log_std)
                        self.entropy = gaussian_entropy(log_std)
                        # MISSING: reg params for log and mu
                        # Apply squashing and account for it in the probability
                        deterministic_policy, policy, logp_pi = apply_squashing_func(mu_, pi_, logp_pi)
                        self.policy = policy
                        self.deterministic_policy = deterministic_policy
                    }
                    std = tf.exp(log_std)
                    # Reparameterization trick
                    pi_ = mu_ + tf.random_normal(tf.shape(mu_)) * std
                    logp_pi = gaussian_likelihood(pi_, mu_, log_std)
                    self.entropy = gaussian_entropy(log_std)
                    # MISSING: reg params for log and mu
                    # Apply squashing and account for it in the probability
                    deterministic_policy, policy, logp_pi = apply_squashing_func(mu_, pi_, logp_pi)
                    self.policy = policy
                    self.deterministic_policy = deterministic_policy
            


            return deterministic_policy, policy, logp_pi

        # Within MlpPolicy(FeedForwardPolicy)                           sac_multi/policies.py
        def make_custom_critics(self, primitives, obs=None, action=None, reuse=False, scope="values_fn",
                        create_vf=True, create_qf=True):
            if obs is None:
                obs = self.processed_obs

            with tf.variable_scope(scope, reuse=reuse):
                if self.feature_extraction == "cnn":
                    critics_h = self.cnn_extractor(obs, **self.cnn_kwargs)
                else:
                    critics_h = tf.layers.flatten(obs)

                if create_vf:
                    # Value function
                    with tf.variable_scope('vf', reuse=reuse):
                        vf_h = mlp(critics_h, self.value_layers, self.activ_fn, layer_norm=self.layer_norm)
                        value_fn = tf.layers.dense(vf_h, 1, name="vf")
                    self.value_fn = value_fn

                if create_qf:
                    # Concatenate preprocessed state and action
                    qf_h = tf.concat([critics_h, action], axis=-1)

                    # Double Q values to reduce overestimation
                    with tf.variable_scope('qf1', reuse=reuse):
                        qf1_h = mlp(qf_h, self.value_layers, self.activ_fn, layer_norm=self.layer_norm)
                        qf1 = tf.layers.dense(qf1_h, 1, name="qf1")

                    with tf.variable_scope('qf2', reuse=reuse):
                        qf2_h = mlp(qf_h, self.value_layers, self.activ_fn, layer_norm=self.layer_norm)
                        qf2 = tf.layers.dense(qf2_h, 1, name="qf2")

                    self.qf1 = qf1
                    self.qf2 = qf2

            return self.qf1, self.qf2, self.value_fn
        

    M. total layer structure def required.
    {
        1. def structure of aux networks by dict
        e.g: layers = {}
                layers['weight1'] = [256, 256]
                layers['weight2'] = [128, 256, 128]
                layers['aux1'] = [128, 128, 64]
                layers['aux2'] = [128, 128]

        2. stack layers by task-specific str (structure of each prims are pre-defined/pre-trained)
        e.g: 
            [['weight1','weight2'],['reaching','mobile','grasping', 'aux1', 'aux2']]
            total network:
                                        weight1     weight2
                                    reach mob grasp aux1  aux2

        3. should recognize the structure for each primitives

    }
    M. temp param ops buffer for primitives required.

    M. Action space btw auxilaries and primitives should be correlated somehow
    {
        1. task-specific auxilary:
    }
    M. Mixture density network of each composite primitives
    {
    }

    
    (for SAC or other algos)
    SAC.load(model_path, env) + M. total_policy_structure + M. temp_param_load_ops_buffer for all primitives
        model = cls(policy=data["policy"], env=None, _init_setup_model=False) + M. layer structure
        model.__dict__.update(data)
        model.__dict__.update(kwargs)
        model.set_env(env)
            compare env.action_space and policy_list action_space
            compare obs space


        model.setup_model() -> model structure setup
            initialize tf.Graph()
            variable_scope: make_actor / make_critics (task specific)


        model.load_parameters(params -> policy.zip, exact_match=False)
            BaseRLModel._setup_load_operations()
                loadable_params = SAC.get_parameter_list()
                    (loadable_params -> [tf.Variable(name, shape, dtype)]) = self.params + self.target_params (Q. self.params, self.target_params -> from where? A. from setup_model())
                placeholder = tf.ph(param.dype, param.shape)
                (self._param_load_ops -> OrderedDict()) = map(loadable_params.name, loadable_params.assign(placeholder)) : for each params by name, (ph, tf.Variable.assign(ph))
                # self._param_load_ops: dict of param names with ph, tf.variable(ph)
            for each param: sess.run of variable.assign(ph) with dict = {ph: each value}
        
        Q: model structure? A: Maybe from somewhere else which have defined self.params/target_params

    M. standard deviation @ predict -> should be alive : deterministic = False

}
    
