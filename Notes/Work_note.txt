Mobile robot + jaco mujoco model
{
    root.xml
        shared.xml 
            <asset> all the meshes with stl
            <asset> all the objects with stl + png
            <contact> excluding contacts
            <default> colors under class

        <worldbody> environments
        <worldbody> mobile robots
            <body> mocap def
            <body> links with childclass="dual_ur5_husky"
            ur5_l.xml
            ur5_r.xml

        <worldbody> objects

        <actuator> base joints

        gripper.xml
            <tendon> joints
            <actuator> tendons-joints
}

SAC policies
{
    SAC(OffPolicyRLModel(BaseRLModel))
    (
        layers: layers
        policy: LnMlpPolicy(FeedForwardPolicy(SACPolicy(BasePolicy)))
    )

    policy
        if not deterministic:
            mu = dense(mlp(flatten(obs_ph),layer_structure),ac_space)
            std = exp(clip(dense(mlp(flatten(obs_ph),layer_structure),ac_space),min=-20,max=2))
            policy = tanh(mu + normal*std)
        else:
            mu = dense(mlp(flatten(obs_ph),layer_structure),ac_space)
            policy = mu
}


aux network + fine tuning
{
    # Within main.py
        def train_with_additional_layer(self, model_dir):        
            from stable_baselines.sac.policies import MlpPolicy

            env = JacoMujocoEnv(**vars(self.args))
            # NOTE: name for primitive
            # If pretrained, then 'freeze/' must be added in front of the name in order to prevent forgetting
            # else, put 'train/'
            primitives = OrderedDict()
        
            # NOTE: newly appointed primitives
            BaseRLModel.construct_primitive_info(name='train/aux1', primitive_dict=primitives, obs_dimension=6, obs_range=[-3, 3], obs_index=[0, 1, 2, 4, 5], 
                                                        act_dimension=6, act_range=[-1.4, 1.4], act_index=[0,1,2,3], layer_structure=[256, 256])
            BaseRLModel.construct_primitive_info('train/aux2', primitives, 6, [-3, 3], [3, 4, 5, 8], 
                                                        6, [-1.4, 1.4], [4,5,6,7], [256, 128])
            BaseRLModel.construct_primitive_info('train/weight', primitives, total_obs_dim, 0, list(range(total_obs_dim)), 
                                                        number_of_primitives, [0,1], number_of_primitives, [256, 256])
    
            # NOTE: Pretrained primitives
            # tuple (data -> OrderedDict, param -> OrderedDict)
            # param = {'name': value (np.array)}
            BaseRLModel.construct_primitive_info('freeze/reaching', primitives, \
                                                loaded_policy=BaseRLModel._load_from_file(policy_zip_path))
            BaseRLModel.construct_primitive_info('freeze/grasping', primitives, \
                                                loaded_policy=BaseRLModel._load_from_file(policy_zip_path))

            policy = MlpPolicy
            self.trainer = SAC_MULTI.pretrainer_load(policy, primitives, env)

        M. total layer structure def required.
        {
            1. def structure of aux networks by dict
            e.g: layers = {}
                    layers['weight1'] = [256, 256]
                    layers['weight2'] = [128, 256, 128]
                    layers['aux1'] = [128, 128, 64]
                    layers['aux2'] = [128, 128]

            2. stack layers by task-specific str (structure of each prims are pre-defined/pre-trained)
            e.g: 
                [['weight1','weight2'],['reaching','mobile','grasping', 'aux1', 'aux2']]
                total network:
                                            weight1     weight2
                                        reach mob grasp aux1  aux2

            3. should recognize the structure for each primitives
        }
        M. temp param ops buffer for primitives required.
        M. Action space btw auxilaries and primitives should be correlated somehow
        {
            1. task-specific auxilary:
        }
        M. Mixture density network of each composite primitives
        {
        }
        M. standard deviation @ predict -> should be alive : deterministic = False

    # Within SAC_MULTI(OffPolicyRLModel(BaseRLModel))               common/base_class.py
        @classmethod
        def construct_primitive_info(cls, name, primitive_dict, obs_dimension, obs_range: Union[str, list], obs_index, act_dimension, act_range, act_index: Union[str, list], layer_structure, loaded_policy=None) -> dict:
            '''
            Returns info of the primtive as a dictionary

            :param name: (str) name of the primitive
            :param primitive_dict: (dict) primitive dictionary in which to store data
            :param obs_dimension: (int) observation space dimension for the primitive
            :param obs_range: ([float, float] or [int, int] or int) observation range. If int, then range fixed to 0
            :param obs_index: ([int, ...]) list of indices of the observation for the primitive
            :param act_dimension: (int) action space dimension for the primitive
            :param act_range: ([float, float] or [int, int]) action range
            :param act_index: ([int, ...] or int) list of indices of the action for the primitive. If int, then action_index_array = [0:act_index]
            :param layer_structure: ([int, ...]) hidden layer structure of the primitive
            :param loaded_policy: ((dict, dict)) tuple of data and parameters for pretrained policy.zip
            :return: (dict: {'obs':tuple, 'act':tuple, 'layer':list}) primitive information
            '''
            if isinstance(loaded_policy, type(None)):
                assert obs_dimension == len(obs_index), '\033[91m[ERROR]: obs_dimension mismatch with the length of obs_index.\
                                                        obs_dimension = {0}, len(obs_index) = {1}\033[0m'.format(obs_dimension, len(obs_index))

                if isinstance(obs_range, list):
                    obs_range_max = np.array([max(obs_range)]*obs_dimension)
                    obs_range_min = np.array([min(obs_range)]*obs_dimension)
                    obs = (spaces.Box(obs_range_min, obs_range_max, dtype=np.float32), obs_index.sort())
                elif isinstance(obs_range, int):
                    obs_range_array = np.array([0]*obs_dimension)
                    obs = (spaces.Box(obs_range_array, obs_range_array, dtype=np.float32), obs_index.sort())
                else:
                    raise TypeError("\033[91m[ERROR]: obs_range wrong type - Should be a list or an int. Received {0}\033[0m".format(type(obs_range)))
                
                if isinstance(act_index, list):
                    assert act_dimension == len(act_index), '\033[91m[ERROR]: act_dimension mismatch with the length of act_index.\
                                                        act_dimension = {0}, len(act_index) = {1}\033[0m'.format(act_dimension, len(act_index))
                elif isinstance(act_index, int):
                    act_index = list(range(act_index))
                else:
                    raise TypeError("\033[91m[ERROR]: act_index wrong type, should be a list or an int. Received {0}\033[0m".format(type(act_index)))
                act_range_max = np.array([max(act_range)]*act_dimension)
                act_range_min = np.array([min(act_range)]*act_dimension)
                act = (spaces.Box(act_range_min, act_range_max, dtype=np.float32), act_index.sort())
                
            elif isinstance(loaded_policy, tuple):
                data_dict, param_dict = loaded_policy

                obs_box = data_dict['observation_space']
                act_box = data_dict['action_space']
                assert len(obs_index) == obs_box.shape[0], '\033[91m[ERROR]: Loaded observation dimension mismatch with length of obs_index.\
                                                        obs_dimension = {0}, len(obs_index) = {1}\033[0m'.format(obs_box.shape[0], len(obs_index))
                assert len(act_index) == act_box.shape[0], '\033[91m[ERROR]: Loaded action dimension mismatch with length of act_index.\
                                                        act_dimension = {0}, len(act_index) = {1}\033[0m'.format(act_box.shape[0], len(act_index))
                obs = (obs_box, obs_index.sort())
                act = (act_box, act_index.sort())

                if 'pretrained_param' not in primitive_dict.keys():
                    primitive_dict['pretrained_param'] = ([],{})
                updated_name, updated_param_dict = cls.loaded_policy_name_update(name, param_dict)
                primitive_dict['pretrained_param'][0] += updated_name
                primitive_dict['pretrained_param'][1] = {**primitive_dict['pretrained_param'][1], **updated_param_dict}

                layer_structure = cls.get_policy_layer_structure((obs, act), param_dict)
            else:
                raise TypeError("\033[91m[ERROR]: loaded_policy wrong type - Should be None or a tuple. Received {0}\033[0m".format(type(loaded_policy)))

            primitive_dict[name] = {'obs': obs, 'act': act, 'layer': layer_structure}

    # Within SAC_MULTI(OffPolicyRLModel(BaseRLModel))               common/base_class.py
        @staticmethod
        def loaded_policy_name_update(primitive_name, loaded_policy_dict):
            '''
            Concatenate name of each layers with name of the primitive

            :param name: (str) name of the primitive
            :param loaded_policy_dict: (dict) Dictionary of parameters of layers by name
            :return: (list, dict) List consisting names of layers with specified primitive
                                Dictionary of parameters by updated names
            '''
            layer_name_list = []
            layer_param_dict = {}
            for name, value in loaded_policy_dict.items():
                name_elem = name.split("/")
                assert 'LayerNorm' not in name_elem, "\033[91m[ERROR]: LayerNormalized policy is not supported for now. Try to load primitives with unnormalized layers\033[0m"

                if 'pi' in name_elem:
                    insert_index = 2
                elif 'values_fn' in name_elem:
                    insert_index = 3
                else:
                    insert_index = 1
                updated_name = '/'.join(name_elem.insert(insert_index, primitive_name))
                layer_name_list.append(updated_name)
                layer_param_dict[updated_name] = value

            return layer_name_list, layer_param_dict
    
    # Within SAC_MULTI(OffPolicyRLModel(BaseRLModel))               common/base_class.py
        @staticmethod
        def get_policy_layer_structure((obs, act), loaded_policy_dict):
            '''
            Return layer structure of the policy from parameter dictionary

            :param obs/act: ((tuple, tuple)) Tuple containing info of observation and action
            :param loaded_policy_dict: (dict) Dictionary of parameters of layers by name
            :return: (list) Layer structure of the policy
            '''
            obs_dim = len(obs[1])
            act_dim = len(act[1])
            primitive_layer_structure = []
            for name, value in loaded_policy_dict.items():
                if name.find("pi/fc") > -1:
                    if name.find("fc0/kernel") > -1:
                        assert obs_dim == value.shape[0], "\033[91m[ERROR/Loaded Primitive]: Observation input of param shape does not match with the observation box. Potential corruption\033[0m"
                    if name.find("bias") > -1:
                        primitive_layer_structure.append(value.shape[0])

            return primitive_layer_structure

    # Within SAC_MULTI(OffPolicyRLModel(BaseRLModel))               common/base_class.py
        @classmethod
        @abstractmethod
        def pretrainer_load(cls, policy, primitives, env, **kwargs):
            """
            Construct trainer from policy structure

            :param policy: (Policy Class) class of SAC policy
            :param primitives: (dict) primitives by name to which items assigned as an info of obs/act/layer_structure
            :param env: (Gym Environment) the new environment to run the loaded model on
            :param kwargs: extra arguments to change the model when loading
            """
            raise NotImplementedError()

    # Within SAC_MULTI(OffPolicyRLModel)                            common/base_class.py
        @classmethod
        def pretrainer_load(cls, policy, primitives, env, **kwargs) -> SAC_MULTI:
            """
            Construct trainer from policy structure

            :param policy: (Policy Class) class of SAC policy
            :param primitives: (dict) primitives by name to which items assigned as an info of obs/act/layer_structure
            :param env: (Gym Environment) the new environment to run the loaded model on
            :param kwargs: extra arguments to change the model when loading
            """

            # TODO: policy_kwargs?
            if 'policy_kwargs' in kwargs and kwargs['policy_kwargs'] != data['policy_kwargs']:
                raise ValueError("The specified policy kwargs do not equal the stored policy kwargs. "
                                "Stored kwargs: {}, specified kwargs: {}".format(data['policy_kwargs'],
                                                                                kwargs['policy_kwargs']))
            
            # model = SAC_MULTI
            model = cls(policy=policy, env=None, _init_setup_model=False)

            # Check the existence of 'train/weight' in primitives
            cls.weight_check(primitives)
            
            # get total_obs_bound, total_act_bound
            ranges = cls.range_primitive(primitives)
            data = {'observation_space': Box(ranges[0][0], ranges[0][1], dtype=np.float32), 'action_space': Box(ranges[1][0], ranges[1][1], dtype=np.float32)}
            model.__dict__.update(data)

            # Nothing updated
            model.__dict__.update(kwargs)
            # TODO: find out what kind of keys in kwargs exists
            
            # check env is a VecEnv -> always False. (Only train our policy with 1 environment)
            model.set_env(env)
            # TODO: env.ob_space and env.ac_space should match with the one from __dict__update(data)
            # -> env.ob_space and env.ac_space should be the total/final obs and acts

            # make self.graph
            # with graph: init self.sess
            # with tf.Variable_scope('input), make:
            #   self.policy = self.policy(self.sess, self.ob_space, self.ac_space, self.layers)
            #   self.target_policy = self.policy(self.sess, self.ob_space, self.ac_space, self.layers)
            #   initialize input placeholders
            # with tf.Variable_scope('model'), make:
            #   det_policy, sto_policy, logp_pi = self.policy.make_actor(input_ph)
            #   qf1, qf2, value = self.policy.make_critics(input_ph, actions_ph)
            #   qf1_pi, qf2_pi, _ = self.policy.make_critics(input_ph, sto_policy, vf=False, reuse=True)
            # with tf.Variable_scope('target'), make:
            #   _, _, value_target = self.target_policy.make_critics(next_input_ph, qf=False)
            # with tf.Variable_scope('loss'), make:
            # policy_train_op
            # train_value_op
            # target_init_op
            # initialize Values and target network
            model.setup_custom_model(primitives)

            # from self.tf_Variables of network:
            #   make load_ops -> {'name from self.variables': (ph, tf.Variable.assign(ph))}
            # for each layer name:
            #   make feed_dict -> {'ph of the name': value of the name}
            #   make param_update_ops -> [tf.Variable.assign(ph)]
            # after building feed_dict, update_ops, run update_ops with feed_dict
            # TODO: if primitives with name starts with 'freeze' does not have pretrained params,
            #       raise Error.
            model.load_parameters(params, exact_match=False)

            # return SAC trainer
            return model
    
    # Within SAC_MULTI(OffPolicyRLModel)                            common/base_class.py
        @staticmethod
        def weight_check(primitives: dict)
            '''
            Check the existence of 'train/weight' in primitive dict

            :param primitives: (dict) obs/act/structure info of primitives
            '''
            assert 'train/weight' in primitives.keys(), '\033[91m[ERROR]: No primitive name "train/weight". YOU MUST HAVE IT\033[0m'

    # Within SAC_MULTI(OffPolicyRLModel)                            common/base_class.py
        @staticmethod
        def range_primitive(primitives: dict) -> list:
            '''
            Return range bounds of total observation/action space

            :param primitives: (dict) obs/act/structure info of primitives
            :return: ([2x2 np.array]): 
                dims[0][0] = obs min np.array, dims[0][1] = obs max np.array
                dims[1][0] = act min np.array, dims[1][1] = act max np.array
            '''
            obs_dim = primitives['train/weight']['obs'][1][-1] + 1  # dimension = last index + 1
            obs_min_array = np.array([-float('inf')]*obs_dim)
            obs_max_array = np.array([float('inf')]*obs_dim)

            act_dim = 0
            for name, info_dict in primitives.items():
                if name != 'train/weight'
                    act_dim = max(act_dim, info_dict['act'][1][-1]+1)
            act_min_array = np.array([-float('inf')]*act_dim)
            act_max_array = np.array([float('inf')]*act_dim)

            for name, info_dict in primitives.items():
                # TODO: differenciate min/max bounds of each dimension of obs/act within a single primitive
                obs_min_prim = info_dict['obs'][0].low.min()
                obs_max_prim = info_dict['obs'][0].high.max()
                act_min_prim = info_dict['act'][0].low.min()
                act_max_prim = info_dict['act'][0].high.max()
                for index in info_dict['obs'][1]:
                    obs_min_array[index] = obs_min_prim
                    if obs_min_array[index] not in [-float('inf'), obs_min_prim]:
                        print("[WARNING]: You are about to overwrite min bound of obs[{0}] with {1}".format(obs_min_array[index], obs_min_prim))
                    obs_max_array[index] = obs_max_prim
                    if obs_max_array[index] not in [float('inf'), obs_max_prim]:
                        print("[WARNING]: You are about to overwrite max bound of obs[{0}] with {1}".format(obs_max_array[index], obs_max_prim))
                    act_min_array[index] = act_min_prim
                    if act_min_array[index] not in [-float('inf'), act_min_prim]:
                        print("[WARNING]: You are about to overwrite min bound of obs[{0}] with {1}".format(act_min_array[index], act_min_prim))
                    act_max_array[index] = act_max_prim
                    if act_max_array[index] not in [float('inf'), act_max_prim]:
                        print("[WARNING]: You are about to overwrite max bound of obs[{0}] with {1}".format(act_max_array[index], act_max_prim))
            ranges = [[obs_min_array, obs_max_array], [act_min_array, act_max_array]]

            return ranges

    # Within SAC_MULTI(OffPolicyRLModel(BaseRLModel))               common/base_class.py
        @abstractmethod
        def setup_custom_model(self, primitives):
            pass

    # Within SAC_MULTI(OffPolicyRLModel)                            common/base_class.py
        @abstractmethod
        def setup_custom_model(self, primitives):
            pass

    # Within SAC_MULTI                                              sac_multi/sac_multi.py
        def setup_custom_model(self, primitives):
            with SetVerbosity(self.verbose):
                self.graph = tf.Graph()
                with self.graph.as_default():
                    self.set_random_seed(self.seed)
                    self.sess = tf_util.make_session(num_cpu=self.n_cpu_tf_sess, graph=self.graph)
                    self.replay_buffer = ReplayBuffer(self.buffer_size)

                    with tf.variable_scope("input", reuse=False):
                        # Create custom policy and target TF objects
                        self.policy_tf = self.policy(self.sess, self.observation_space, self.action_space, layers=self.layers,
                                                    **self.policy_kwargs)
                        self.target_policy = self.policy(self.sess, self.observation_space, self.action_space, layers=self.layers,
                                                        **self.policy_kwargs)
                        
                        assert not self.policy_tf.layer_norm, "\033[91m[ERROR]: LayerNormalized policy is not supported for now. Try to train policy with unnormalized layers\033[0m"
                        
                        # Initialize Placeholders
                        self.observations_ph = self.policy_tf.obs_ph
                        # Normalized observation for pixels
                        self.processed_obs_ph = self.policy_tf.processed_obs
                        self.next_observations_ph = self.target_policy.obs_ph
                        self.processed_next_obs_ph = self.target_policy.processed_obs
                        # None
                        self.action_target = self.target_policy.action_ph
                        self.terminals_ph = tf.placeholder(tf.float32, shape=(None, 1), name='terminals')
                        self.rewards_ph = tf.placeholder(tf.float32, shape=(None, 1), name='rewards')
                        self.actions_ph = tf.placeholder(tf.float32, shape=(None,) + self.action_space.shape,
                                                        name='actions')
                        self.learning_rate_ph = tf.placeholder(tf.float32, [], name="learning_rate_ph")

                    with tf.variable_scope("model", reuse=False):
                        # Create the policy
                        # first return value corresponds to deterministic actions
                        # policy_out corresponds to stochastic actions, used for training
                        # logp_pi is the log probability of actions taken by the policy
                        self.deterministic_action, policy_out, logp_pi = self.policy_tf.make_custom_actor(self.processed_obs_ph, primitives)
                        # Monitor the entropy of the policy,
                        # this is not used for training
                        self.entropy = tf.reduce_mean(self.policy_tf.entropy)
                        #  Use two Q-functions to improve performance by reducing overestimation bias.
                        qf1, qf2, value_fn = self.policy_tf.make_custom_critics(self.processed_obs_ph, self.actions_ph,
                                                                        create_qf=True, create_vf=True)
                        qf1_pi, qf2_pi, _ = self.policy_tf.make_custom_critics(self.processed_obs_ph,
                                                                        policy_out, create_qf=True, create_vf=False,
                                                                        reuse=True)

                        # Target entropy is used when learning the entropy coefficient
                        if self.target_entropy == 'auto':
                            # automatically set target entropy if needed
                            self.target_entropy = -np.prod(self.action_space.shape).astype(np.float32)
                        else:
                            # Force conversion
                            # this will also throw an error for unexpected string
                            self.target_entropy = float(self.target_entropy)

                        # The entropy coefficient or entropy can be learned automatically
                        # see Automating Entropy Adjustment for Maximum Entropy RL section
                        # of https://arxiv.org/abs/1812.05905
                        if isinstance(self.ent_coef, str) and self.ent_coef.startswith('auto'):
                            # Default initial value of ent_coef when learned
                            init_value = 1.0
                            if '_' in self.ent_coef:
                                init_value = float(self.ent_coef.split('_')[1])
                                assert init_value > 0., "The initial value of ent_coef must be greater than 0"

                            self.log_ent_coef = tf.get_variable('log_ent_coef', dtype=tf.float32,
                                                                initializer=np.log(init_value).astype(np.float32))
                            self.ent_coef = tf.exp(self.log_ent_coef)
                        else:
                            # Force conversion to float
                            # this will throw an error if a malformed string (different from 'auto')
                            # is passed
                            self.ent_coef = float(self.ent_coef)

                    with tf.variable_scope("target", reuse=False):
                        # Create the value network
                        _, _, value_target = self.target_policy.make_custom_critics(self.processed_next_obs_ph,
                                                                            create_qf=False, create_vf=True)
                        self.value_target = value_target

                    with tf.variable_scope("loss", reuse=False):
                        # Take the min of the two Q-Values (Double-Q Learning)
                        min_qf_pi = tf.minimum(qf1_pi, qf2_pi)

                        # Target for Q value regression
                        q_backup = tf.stop_gradient(
                            self.rewards_ph +
                            (1 - self.terminals_ph) * self.gamma * self.value_target
                        )

                        # Compute Q-Function loss
                        # TODO: test with huber loss (it would avoid too high values)
                        qf1_loss = 0.5 * tf.reduce_mean((q_backup - qf1) ** 2)
                        qf2_loss = 0.5 * tf.reduce_mean((q_backup - qf2) ** 2)

                        # Compute the entropy temperature loss
                        # it is used when the entropy coefficient is learned
                        ent_coef_loss, entropy_optimizer = None, None
                        if not isinstance(self.ent_coef, float):
                            ent_coef_loss = -tf.reduce_mean(
                                self.log_ent_coef * tf.stop_gradient(logp_pi + self.target_entropy))
                            entropy_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)

                        # Compute the policy loss
                        # Alternative: policy_kl_loss = tf.reduce_mean(logp_pi - min_qf_pi)
                        policy_kl_loss = tf.reduce_mean(self.ent_coef * logp_pi - qf1_pi)

                        # NOTE: in the original implementation, they have an additional
                        # regularization loss for the Gaussian parameters
                        # this is not used for now
                        # policy_loss = (policy_kl_loss + policy_regularization_loss)
                        policy_loss = policy_kl_loss


                        # Target for value fn regression
                        # We update the vf towards the min of two Q-functions in order to
                        # reduce overestimation bias from function approximation error.
                        v_backup = tf.stop_gradient(min_qf_pi - self.ent_coef * logp_pi)
                        value_loss = 0.5 * tf.reduce_mean((value_fn - v_backup) ** 2)

                        values_losses = qf1_loss + qf2_loss + value_loss

                        # Policy train op
                        # (has to be separate from value train op, because min_qf_pi appears in policy_loss)
                        policy_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)
                        # NOTE: params of pretrained networks should not be fine-tuned to avoid forgetting
                        # TODO Q: If not contained in the train_op, will gradients of these variables be excluded?
                        policy_train_op = policy_optimizer.minimize(policy_loss, var_list=tf_util.get_trainable_vars('model/pi/train'))

                        # Value train op
                        value_optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph)
                        values_params = tf_util.get_trainable_vars('model/values_fn')

                        source_params = tf_util.get_trainable_vars("model/values_fn/vf")
                        target_params = tf_util.get_trainable_vars("target/values_fn/vf")

                        # Polyak averaging for target variables
                        self.target_update_op = [
                            tf.assign(target, (1 - self.tau) * target + self.tau * source)
                            for target, source in zip(target_params, source_params)
                        ]
                        # Initializing target to match source variables
                        target_init_op = [
                            tf.assign(target, source)
                            for target, source in zip(target_params, source_params)
                        ]

                        # Control flow is used because sess.run otherwise evaluates in nondeterministic order
                        # and we first need to compute the policy action before computing q values losses
                        with tf.control_dependencies([policy_train_op]):
                            train_values_op = value_optimizer.minimize(values_losses, var_list=values_params)

                            self.infos_names = ['policy_loss', 'qf1_loss', 'qf2_loss', 'value_loss', 'entropy']
                            # All ops to call during one training step
                            self.step_ops = [policy_loss, qf1_loss, qf2_loss,
                                            value_loss, qf1, qf2, value_fn, logp_pi,
                                            self.entropy, policy_train_op, train_values_op]

                            # Add entropy coefficient optimization operation if needed
                            if ent_coef_loss is not None:
                                with tf.control_dependencies([train_values_op]):
                                    ent_coef_op = entropy_optimizer.minimize(ent_coef_loss, var_list=self.log_ent_coef)
                                    self.infos_names += ['ent_coef_loss', 'ent_coef']
                                    self.step_ops += [ent_coef_op, ent_coef_loss, self.ent_coef]

                        # Monitor losses and entropy in tensorboard
                        tf.summary.scalar('policy_loss', policy_loss)
                        tf.summary.scalar('qf1_loss', qf1_loss)
                        tf.summary.scalar('qf2_loss', qf2_loss)
                        tf.summary.scalar('value_loss', value_loss)
                        tf.summary.scalar('entropy', self.entropy)
                        for name, value in primitives.items():
                            pi_data = tf_util.get_trainable_vars('model/pi/'+name)
                            #val_data = tf_util.get_trainable_vars('model/pi/'+name)
                            tf.summary.histogram('model/pi/'+name.split('/')[1], pi_data)
                            #tf.summary.histogram('model/values_fn/vf/'+name.split('/')[1], val_data)
                        if ent_coef_loss is not None:
                            tf.summary.scalar('ent_coef_loss', ent_coef_loss)
                            tf.summary.scalar('ent_coef', self.ent_coef)

                        tf.summary.scalar('learning_rate', tf.reduce_mean(self.learning_rate_ph))

                    # Retrieve parameters that must be saved
                    self.params = tf_util.get_trainable_vars("model")
                    self.target_params = tf_util.get_trainable_vars("target/values_fn/vf")

                    # Initialize Variables and target network
                    with self.sess.as_default():
                        self.sess.run(tf.global_variables_initializer())
                        self.sess.run(target_init_op)

                    self.summary = tf.summary.merge_all()

    # Within MlpPolicy(FeedForwardPolicy(SACPolicy))                sac_multi/policies.py
        @abstractmethod
        def make_custom_actor(self, primitives, obs=None, reuse=False, scope="pi"):
            """
            Creates an actor object

            :param obs: (TensorFlow Tensor) The observation placeholder (can be None for default placeholder)
            :param reuse: (bool) whether or not to reuse parameters
            :param scope: (str) the scope name of the actor
            :return: (TensorFlow Tensor) the output tensor
            """
            raise NotImplementedError

    # Within MlpPolicy(FeedForwardPolicy(SACPolicy))                sac_multi/policies.py
        @abstractmethod
        def make_custom_critics(self, primitives, obs=None, action=None, reuse=False,
                        scope="values_fn", create_vf=True, create_qf=True):
            """
            Creates the two Q-Values approximator along with the Value function

            :param obs: (TensorFlow Tensor) The observation placeholder (can be None for default placeholder)
            :param action: (TensorFlow Tensor) The action placeholder
            :param reuse: (bool) whether or not to reuse parameters
            :param scope: (str) the scope name
            :param create_vf: (bool) Whether to create Value fn or not
            :param create_qf: (bool) Whether to create Q-Values fn or not
            :return: ([tf.Tensor]) Mean, action and log probability
            """
            raise NotImplementedError

    # Within MlpPolicy(FeedForwardPolicy)                           sac_multi/policies.py
        def make_custom_actor(self, primitives, obs=None, reuse=False, scope="pi"):
            if obs is None:
                obs = self.processed_obs
            mu_array = []
            log_std_array = []
            self.entropy = 0

            for name, item in primitives.items():
                # When the primitive is not pretrained
                if isinstance(item, dict):
                    with tf.variable_scope(scope + "/" + name, reuse=reuse):
                        if self.feature_extraction == "cnn":
                            pi_h = self.cnn_extractor(obs, **self.cnn_kwargs)
                        else:
                            pi_h = tf.layers.flatten(obs)
                        
                        #------------- Input observation seiving layer -------------#
                        seive_layer = np.zeros([item['obs'][0].shape[0], len(item['obs'][1])])
                        for i in range(len(item['obs'][1])):
                            seive_layer[item['obs'][1][i]][i] = 1
                        pi_h = tf.matmul(pi_h, seive_layer)
                        #------------- Observation seiving layer End -------------#

                        pi_h = mlp(pi_h, item['layer'], self.activ_fn, layer_norm=self.layer_norm)

                        if name == 'train/weight':
                            weight = tf.layers.dense(pi_h, item['act_dimension'], activation='softmax')
                        else:
                            mu_ = tf.layers.dense(pi_h, item['act_dimension'], activation=None)
                            mu_array.append(mu_)

                            # Important difference with SAC and other algo such as PPO:
                            # the std depends on the state, so we cannot use stable_baselines.common.distribution
                            log_std = tf.layers.dense(pi_h, item['act_dimension'], activation=None)

                    if name != 'train/weight':
                        log_std = tf.clip_by_value(log_std, LOG_STD_MIN, LOG_STD_MAX)
                        log_std_array.append(log_std)
                        '''
                            self.std = std = tf.exp(log_std)
                            # Reparameterization trick
                            pi_ = mu_ + tf.random_normal(tf.shape(mu_)) * std
                            logp_pi = gaussian_likelihood(pi_, mu_, log_std)
                            self.entropy = gaussian_entropy(log_std)
                            # MISSING: reg params for log and mu
                            # Apply squashing and account for it in the probability
                            deterministic_policy, policy, logp_pi = apply_squashing_func(mu_, pi_, logp_pi)
                            self.policy = policy
                            self.deterministic_policy = deterministic_policy
                        '''
                        std = tf.exp(log_std)

                        '''
                            # Reparameterization trick
                            #pi_ = mu_ + tf.random_normal(tf.shape(mu_)) * std
                            #logp_pi = gaussian_likelihood(pi_, mu_, log_std)
                        '''
                        self.entropy += gaussian_entropy(log_std)
                else:
                    raise TypeError("\033[91m[ERROR]: Primitive type error. Received: {0}, Should be one of 'dict' or 'tuple'.\033[0m".format(type(item)))
                
            # Reparameterization trick for MCP
            pi_MCP, mu_MCP, log_std_MCP = fuse_networks_MCP(mu_array, log_std_array, weight)
            logp_pi = gaussian_likelihood(pi_MCP, mu_MCP, log_std_MCP)
            self.std = tf.exp(log_std_MCP)
            self.policy_train = pi_MCP
            self.deterministic_policy_train = self.act_mu = mu_MCP

            # policies with squashing func at test time
            # TODO: Need to check if these variables are used @ training time
            deterministic_policy, policy, logp_pi = apply_squashing_func(mu_MCP, pi_MCP, logp_pi)
            self.policy = policy
            self.deterministic_policy = deterministic_policy

            return deterministic_policy, policy, logp_pi

    # Within MlpPolicy(FeedForwardPolicy)                           sac_multi/policies.py
    def make_custom_critics(self, primitives, obs=None, action=None, reuse=False, scope="values_fn",
                    create_vf=True, create_qf=True):
        if obs is None:
            obs = self.processed_obs

        with tf.variable_scope(scope, reuse=reuse):
            if self.feature_extraction == "cnn":
                critics_h = self.cnn_extractor(obs, **self.cnn_kwargs)
            else:
                critics_h = tf.layers.flatten(obs)

            if create_vf:
                # Value function
                with tf.variable_scope('vf', reuse=reuse):
                    vf_h = mlp(critics_h, self.value_layers, self.activ_fn, layer_norm=self.layer_norm)
                    value_fn = tf.layers.dense(vf_h, 1, name="vf")
                self.value_fn = value_fn

            if create_qf:
                # Concatenate preprocessed state and action
                qf_h = tf.concat([critics_h, action], axis=-1)

                # Double Q values to reduce overestimation
                with tf.variable_scope('qf1', reuse=reuse):
                    qf1_h = mlp(qf_h, self.value_layers, self.activ_fn, layer_norm=self.layer_norm)
                    qf1 = tf.layers.dense(qf1_h, 1, name="qf1")

                with tf.variable_scope('qf2', reuse=reuse):
                    qf2_h = mlp(qf_h, self.value_layers, self.activ_fn, layer_norm=self.layer_norm)
                    qf2 = tf.layers.dense(qf2_h, 1, name="qf2")

                self.qf1 = qf1
                self.qf2 = qf2

        return self.qf1, self.qf2, self.value_fn
    
    # Within                                                        sac_multi/policies.py
    def fuse_networks_MCP(mu_array, log_std_array, weight):
        """
        Fuse distributions of policy into a MCP fashion

        :param mu_array: ([tf.Tensor]) List of means
        :param log_std_array: ([tf.Tensor]) List of log of the standard deviations
        :param weight: (tf.Tensor) Weight tensor of each primitives
        :return: ([tf.Tensor]) Samples of fused policy, fused mean, and fused standard deviations
        """

        return pi_MCP, mu_MCP, log_std_MCP

}