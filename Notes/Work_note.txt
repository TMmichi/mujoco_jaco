Mobile robot + jaco mujoco model
{
    root.xml
        shared.xml 
            <asset> all the meshes with stl
            <asset> all the objects with stl + png
            <contact> excluding contacts
            <default> colors under class

        <worldbody> environments
        <worldbody> mobile robots
            <body> mocap def
            <body> links with childclass="dual_ur5_husky"
            ur5_l.xml
            ur5_r.xml

        <worldbody> objects

        <actuator> base joints

        gripper.xml
            <tendon> joints
            <actuator> tendons-joints
}

SAC policies
{
    SAC(OffPolicyRLModel(BaseRLModel))
    (
        layers: layers
        policy: LnMlpPolicy(FeedForwardPolicy(SACPolicy(BasePolicy)))
    )

    policy
        if not deterministic:
            mu = dense(mlp(flatten(obs_ph),layer_structure),ac_space)
            std = exp(clip(dense(mlp(flatten(obs_ph),layer_structure),ac_space),min=-20,max=2))
            policy = tanh(mu + normal*std)
        else:
            mu = dense(mlp(flatten(obs_ph),layer_structure),ac_space)
            policy = mu
}


aux network + fine tuning
{
    from stable_baselines.sac.policies import MlpPolicy

    def train_with_additional_layer(self, model_dir):        
        env = JacoMujocoEnv(**vars(self.args))

        # tf.graph -> Use the one in SAC_MULTI.setup_model()
        # 1. set the structure of unknown layers -> []
        # 2. define name of the unknown layers -> {'name': np.array}
        # 3. call pretrained policies and extract the structure of layers by name / store paramters in a temp buffer -> {'name': (structure -> [], parameters -> np.array)}
        # 4. compose total structure of the total policy -> []
        # 5. update model policy with the structure of the total policy
        # 6. setup a graph with variables
        # 7. 
        # 
        def init_layers(layers: dict) -> dict:
            '''
            receive layers, convert to np.array with given size if list
            1. loadable_parameters = [tf.Variables with name, shape, dtype]
            2. self._param_load_ops = {'name':(ph, tf.Variable.assign(ph))}
            3. params = {'name': np.array with shape}
            4. feed_dict = {self._param_load_ops[name] -> ph : params[name] -> np.array}
            5. param_update_ops = [tf.Variable.assign(ph)]
            But to make tf.Variable, need to construct a graph with session
            '''
            # Q. Can Layer Normalized policies be mixed with unNormalized policies?
            # Assume: No Layer Normalization for just a fc policy
            # name: e.g. 'aux1/model/pi/fc0/kernel:0

            for name, item in layers.items():
                if isinstance(item, list):
                    layer_od = OrderedDict()
                    networks = ['model/pi', 'model/values_fn/vf', 'model/values_fn/qf1', 'model/values_fn/qf2', 'target/values_fn/vf']
                    for network in networks:
                        # First layer6
                        kernel_layer_name = name + "/{0}/fc{1}/kernel:0".format(network,0)
                        bias_layer_name = name + "/{0}/fc{1}/bias:0".format(network,0)
                        obs_space_size = ?
                        # TODO: replace np.zeros with random init module
                        kernel_value = np.array(np.zeros([obs_space_size, item[0]]))
                        bias_value = np.array(np.zeros([item[0]]))
                        layer_od[kernel_layer_name] = kernel_value
                        layer_od[bias_layer_name] = bias_value

                        # Hidden layer
                        for i in range(len(item)-1):
                            # append np.array
                            kernel_layer_name = name + "/{0}/fc{1}/kernel:0".format(network,i+1)
                            bias_layer_name = name + "/{0}/fc{1}/bias:0".format(network,i+1)
                            # TODO: replace np.zeros with random init module
                            kernel_value = np.array(np.zeros([item[i], item[i+1]]))
                            bias_value = np.array([item[i+1]])
                            layer_od[kernel_layer_name] = kernel_value
                            layer_od[bias_layer_name] = bias_value

                        # Output layer
                        if network == 'model/pi':
                            output = 'dense'
                            mu_kernel_layer_name = name + "/{0}/{1}/kernel:0".format(network, output)
                            mu_bias_layer_name = name + "/{0}/{1}/bias:0".format(network, output)
                            std_kernel_layer_name = name + "/{0}/{1}_1/kernel:0".format(network, output)
                            std_bias_layer_name = name + "/{0}/{1}_1/bias:0".format(network, output)
                            # TODO: replace np.zeros with random init module
                            mu_kernel_value = np.array(np.zeros([item[-1], 1]))
                            mu_bias_value = np.array([1])
                            std_kernel_value = np.array(np.zeros([item[-1], 1]))
                            std_bias_value = np.array([1])
                            layer_od[mu_kernel_layer_name] = mu_kernel_value
                            layer_od[mu_bias_layer_name] = mu_bias_value
                            layer_od[std_kernel_layer_name] = std_kernel_value
                            layer_od[std_bias_layer_name] = std_bias_value
                        else:
                            output = network.split("/")[-1]
                            kernel_layer_name = name + "/{0}/{1}/kernel:0".format(network, output)
                            bias_layer_name = name + "/{0}/{1}/bias:0".format(network, output)
                            # TODO: replace np.zeros with random init module
                            kernel_value = np.array(np.zeros([item[-1], 1]))
                            bias_value = np.array([1])
                            layer_od[kernel_layer_name] = kernel_value
                            layer_od[bias_layer_name] = bias_value        
                else:
                    pass


        # TODO: Need to decide how to property reserve input placeholders for each primitives
        layers = {}
        layers['weight'] = [256, 256]   # list
        layers['aux1'] = [256, 256]     # list
        layers['aux2'] = [128, 128]     # list

        # tuple (data -> OrderedDict, param -> OrderedDict)
        # param = {'name': value (np.array)}
        layers['reaching'] = BaseRLModel._load_from_file(policy_zip_path)
        layers['grasping'] = BaseRLModel._load_from_file(policy_zip_path)

        policy = MlpPolicy
        policy_dict = init_layers(layers)
        self.trainer = SAC_MULTI.init_with_structure(policy, policy_dict, env)

        @classmethod
        def init_with_structure(cls, policy, policy_dict, env, **kwargs) -> SAC_MULTI:
            """
            Construct trainer from policy structure
            :param policy: (Policy Class) class of SAC policy
            :param policy_dict: (dict) structure of the policy
            :param env: (Gym Environment) the new environment to run the loaded model on
            :param kwargs: extra arguments to change the model when loading
            """

            # TODO: policy_kwargs?
            if 'policy_kwargs' in kwargs and kwargs['policy_kwargs'] != data['policy_kwargs']:
                raise ValueError("The specified policy kwargs do not equal the stored policy kwargs. "
                                "Stored kwargs: {}, specified kwargs: {}".format(data['policy_kwargs'],
                                                                                kwargs['policy_kwargs']))

            # SAC_MULTI
            model = cls(policy=policy, env=None, _init_setup_model=False)

            # NOTE: Once loaded, type of policy is fixed
            model.__dict__.update(data) 
            # TODO: self.obs_space, self.act_space should be updated here
            # obs_space and act_space -> Box, but should be separable

            # Nothing updated
            model.__dict__.update(kwargs)
            
            # check env is a VecEnv -> always False. (Only train our policy with 1 environment)
            model.set_env(env)
            # TODO: env.ob_space and env.ac_space should match with the one from __dict__update(data)

            # make self.graph
            # with graph: init self.sess
            # with tf.Variable_scope('input), make:
            #   self.policy = self.policy(self.sess, self.ob_space, self.ac_space, self.layers)
            #   self.target_policy = self.policy(self.sess, self.ob_space, self.ac_space, self.layers)
            #   initialize input placeholders
            # with tf.Variable_scope('model'), make:
            #   det_policy, sto_policy, logp_pi = self.policy.make_actor(input_ph)
            #   qf1, qf2, value = self.policy.make_critics(input_ph, actions_ph)
            #   qf1_pi, qf2_pi, _ = self.policy.make_critics(input_ph, sto_policy, vf=False, reuse=True)
            # with tf.Variable_scope('target'), make:
            #   _, _, value_target = self.target_policy.make_critics(next_input_ph, qf=False)
            # with tf.Variable_scope('loss'), make:
            # policy_train_op
            # train_value_op
            # target_init_op
            # initialize Values and target network
            model.setup_model()
            # TODO: 

            # from self.tf_Variables of network:
            #   make load_ops -> {'name from self.variables': (ph, tf.Variable.assign(ph))}
            # for each layer name:
            #   make feed_dict -> {'ph of the name': value of the name}
            #   make param_update_ops -> [tf.Variable.assign(ph)]
            # after building feed_dict, update_ops, run update_ops with feed_dict
            model.load_parameters(params, exact_match=False)
            # TODO: 

            # return SAC trainer
            return model


    M. total layer structure def required.
    {
        1. def structure of aux networks by dict
        e.g: layers = {}
                layers['weight1'] = [256, 256]
                layers['weight2'] = [128, 256, 128]
                layers['aux1'] = [128, 128, 64]
                layers['aux2'] = [128, 128]

        2. stack layers by task-specific str (structure of each prims are pre-defined/pre-trained)
        e.g: 
            [['weight1','weight2'],['reaching','mobile','grasping', 'aux1', 'aux2']]
            total network:
                                        weight1     weight2
                                    reach mob grasp aux1  aux2

        3. should recognize the structure for each primitives

    }
    M. temp param ops buffer for primitives required.

    M. Action space btw auxilaries and primitives should be correlated somehow
    {
        1. task-specific auxilary:
    }
    M. Mixture density network of each composite primitives
    {
    }

    
    (for SAC or other algos)
    SAC.load(model_path, env) + M. total_policy_structure + M. temp_param_load_ops_buffer for all primitives
        model = cls(policy=data["policy"], env=None, _init_setup_model=False) + M. layer structure
        model.__dict__.update(data)
        model.__dict__.update(kwargs)
        model.set_env(env)
            compare env.action_space and policy_list action_space
            compare obs space


        model.setup_model() -> model structure setup
            initialize tf.Graph()
            variable_scope: make_actor / make_critics (task specific)


        model.load_parameters(params -> policy.zip, exact_match=False)
            BaseRLModel._setup_load_operations()
                loadable_params = SAC.get_parameter_list()
                    (loadable_params -> [tf.Variable(name, shape, dtype)]) = self.params + self.target_params (Q. self.params, self.target_params -> from where? A. from setup_model())
                placeholder = tf.ph(param.dype, param.shape)
                (self._param_load_ops -> OrderedDict()) = map(loadable_params.name, loadable_params.assign(placeholder)) : for each params by name, (ph, tf.Variable.assign(ph))
                # self._param_load_ops: dict of param names with ph, tf.variable(ph)
            for each param: sess.run of variable.assign(ph) with dict = {ph: each value}
        
        Q: model structure? A: Maybe from somewhere else which have defined self.params/target_params

    M. standard deviation @ predict -> should be alive : deterministic = False

}
    
