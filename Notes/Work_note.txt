problem: logp_pi gets huge

buffer = [(s0,a0), (s1,a1), ..., (sT-1, aT-1), (sT)]
    where a0, a1, ..., aT-1 from policy histories

func appx for: 
    Q_theta, pi_phi, V_psi

V_psi - approximates the soft value
      - no need to include a separate appx for the state: E at~pi[Q - logpi]
      - but, using separate appx stabilizes training
      - appx by minimizing residual error wrt (states) from buffer: V(st) - E at~pi[Q(at,st)) - logpi(at|st)]
      - gradients: (states) from buffer, but (actions) from current policy

Q_theta - appx by minimizing bellman residual wrt (states, actions) from buffer: Q_theta - (r + gamma*E st+1~p[V_phi(st+1)])
        - gradients: (states, actions) from ?

pi_phi - directly minimizing wrt (states) from buffer: KLdiv(pi_phi|exp(Q_theta)/Z_theta(st))
       - Use lower variance estimator <- apply reparameterization trick: at = f_phi(eps;st)
       - minimize (states) from buffer, eps~N [logp_phi(f_phi(eps;st)|st) - Q_theta]
       - gradients: del_phi logpi + (del_at logpi - del_at Q) * del_phi f_phi

evaluation + improvement -> optimizing both networks with SGD

