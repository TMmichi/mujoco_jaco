Note

    Problem_specification:
        Keywords:
            1. Dexterous manipulation
            2. Strategic (time-dependent) manipulation
            3. Human data learning
            4. Multi-modal sensory data
            5. Safe to interact (torque control)
            6. Robot independent (worspace independent)
            7. Dual (multi) arm
            8. Reaching
            9. Grasping
            10. State representation
            11. Sim-to-Real
            12. Learning from Demonstration
            13. Reusable policies
            14. Explainable AI
            15. Sample efficient learning


        Tasks:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                Keywords:
                Goal: Make policies explainable (and mimick human-like behavior - if incorporating human data while TRAINING PRIMITIVES)
                Motivation:
                    - Shall let the users know what it's doing with human-interpretable information
                    - Behave in a way human brain does -> composing complicated behavior with basic structures of primitives
                        ref required
                    - MCPs uses primitives that are not identifiable
                    (WILL NOT USE: - most of the techniques uses joint angles -> restricted to the specific configuration of a robot)
                Contributions:
                    - making policy explainable
                    - Increase sample efficiency (incorporate human data while TRAINING WHOLE TASK)
                    - learn auxilary primitives that have not been told/trained (if possible)
                    - fuse policies with different action space into a whole
                    - reduce learning time compared to primitives that has same, whole action space
                To_do: 
                    - Train primitives that are identified for certain tasks (from either expert trajectory or reward engineered RL scheme)
                    - Set task which requires multiple primitives:
                        Options: 
                        {
                            1. 
                            {
                                Task: Grap a glass of water and pour to a specific destination
                                Required features (primitive):
                                    - Reaching to a glass (Reaching)
                                    - Make a good grip when holding a glass (Reaching + Grasping)
                                    - Take it to a goal position when holding a glass (Reaching + Grasping + Auxilary - stabilizing glass?)
                                    - Pouring it to a goal position (Reching + Grasping + Auxilary - Pouring)
                                Thoughts:
                                    - Pouring: 
                                    - Moveit! -> variance = 0인 distribution
                                    - Optimal state representation -> No need to do these shits
                            }
                            2. 
                            {
                                Task: TBD
                                Goal: TBD
                            }
                        }
                    - State representation learning of required & orthogonal sensor data: image, pressure, joint angles
                        -> want to show that those data has some information that does not overlap with each other
                        -> check orthogonality of each sensors using [PCA? suggestions????]
                    - Provide auxilary dummy primitive networks when training in an End-to-End fashion -> do a PCA analysis for actions under states to validate the need for an extra primitive.
                        -> How to check / avoid a single auxilary network from taking over all the auxiliary primitives?
                             - by training policy with enough of aux policies and then optimize the numbers?
                        -> time: 언제 primitive가 activate 될 지??
                        -> Non-Markovian 
                    - Identify unknown primitives via (examining state/action sets from forward propagations)
                    - primitives with same action space -> MCP // primitives with different action space -> ?
                References:
                    - MLSH (Frans, 2017)
                    - FeUdal Networks for Hierarchical Reinforcement Learning (2017)
                    - Modular Multitask Reinforcement Learning with Policy Sketches (2017)
                    - MCP (Peng, 2019)
                    - Options (Sutten, 1999)
                    # If using LofD for primitives:
                    - Learning Agile Robotic Locomotion Skills by Imitating Animals (Peng, 2020)
            }
            2. Title: Who cares who I am: Policy transfer between robots without fine-tuning
            {
                Keywords: 
                
                Goal: Make policies that can be transfered to different robot models without fine-tuning
                Motivation:
                    - Most of the techniques uses joint angles -> restricted to the specific configuration of a robot
                Contributions:
                    - Can transfer policy without the need of fine-tuning within the simulated env
                    - Can transfer policy to a real robot (if possible)
                Limitation:
                    - IK algorithm should be embedded
                To Do:
                    - Use actions as ∆p ∆v of an End Effecor instead of joint angles or joint velocities
                    - Use action/observation scaling with respect to the workbound of each robots (NOT FINETUNING! CAN EASILY BE DONE)
                        (if the scale of jaco : mini jaco = 10 : 1, multiply 10 to each obs/action when using the policy trained from mini jaco to apply to a jaco)
                    - transfer policy without fine-tuning and compare episode rewards at test time
                Thoughts:
                    - Put vision/PC to EE
                    - How to handle vision/PC
                    - If DOF gets to be differ?
            }   
            3. Title: Dexterous Dual-Arm Manipulation via Compositive Primitives
            {
                Keywords: 
                Goal: TBD
                Motivation: TBD
            }
            4. Title: Screw-fixing
            {
                Keywords:     
                Goal: TBD
                Motivation: TBD
                To Do:
                    - time-dependent 
            }
            5. Title: Plugging with sample-efficient policy by composing primitives
            {
                Keywords:     
                Goal: TBD
                Motivation: TBD
            }
            6. Title: Fully express the intention
            {
                Keywords:     
                Goal: Fully express the intention while doing a task by proper / mixture of methods
                Motivation: 
                Notes:
                    - Feature Importance, Permutation:
                        - Determine which feature is the most import one by randomly replacing values for each feature
                        - Importance by error measurement
                        - Cons: No direction, Scale independent, Feature independent
                    - Partial Dependence Plots
                        - Determine the most import feature by 'linearly' replacing values
                        - Pros: Directions, Scale dependent
                        - Cons: Computationally expensive
                    - Global Surrogate
                    - Local Interpretable Model-agnostic Explanations (LIME)
                    - Shapley Additive exPlanations (SHAP)
                    - Filter Visualization
                    - Layer-wise Relevane Propagation (LRP)
            }


    Research_Details:
        Task:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                1-1. Making proper observations 
                {
                    Note: Target position of a reaching primitive - Should explicitly be known to the user
                        -> User do know where it's reaching to
                }
                1-2. Sub-primitives 
                {
                    Questions:
                        1. Why have sub-primitives?
                            - To construct primitives, observation spaces for each should be matched among all.
                                Reason: each primitives shall have an idea with the full knowledge of a current state
                                Question: Doesn't weight function determine how to composite primitives?
                                Defence: 
                            - But there is no need to train each primitives with full observation spaces where they can acheive their goal with much simpler observations.
                            - pretrain sub-primitives with simple observation spaces, and modify them with full observation placeholders
                        
                        2. Did X.Peng pretrained their policies with full observation spaces?
                            - Yes (Did not trained their primitives via RL, but by LofD - simplification not required)
                    Do:
                        -> Need to select:
                            1. RL-trained sub-primitives and finetune them with full obs space
                            2. Not using sub-primitives and policy transfer with expert data directly with full obs space
                    Consequences:
                        1. if select sub-primitives:
                            Csq: Finetune twice for sub-primitives -> primitives and for primitives -> composite policy
                            Result: Complicated/Time consuming to pretrain and finetune twice
                        2.  
                    Defences for Consequences:
                        Def for 1: To avoid complicated / time consumption problem
                            - May not have to finetune for primitives -> composite policies
                            Reason: 
                                1. 
                }
                1-3. Primitive fusing 
                {    
                    Questions:
                        1. How to fuse primitives with different action space?
                            - maybe weight is all reponsible of fusing action spaces.
                            - if actions from both arm and wheels are important, then the weight will equally be distributed
                            - if actions from arm is more important than that of the wheels, then the weight will focus more on arms
                            - since weight incorporates total states, it can fully observe the space and provide decisions
                }
                1-4. Brain Functionality
                {
                    References:
                        - Hierarchical modularity in human brain functional networks (Meunier, 2009)
                        {
                            - modular decomposition, measured using fMRI
                            - human brain functional networks have a hierarchical modular organization
                            - 5-largest, highest: medial occipital (visual), lateral occipital, central, parieto-frontal, fronto-temporal
                            ** Simon’s original hypothesis that hierarchy or near-decomposability of physical symbol systems is a critical design feature for their fast adaptivity to changing environmental conditions.
                            ** stable intermediate forms (component modules) -> allow the system to adapt one module at a time without risking loss of function in other, already-adapted modules
                            -> Need of auxilary network, not to destruct already-trained primitive 
                                - Overcoming catastrophic forgetting in neural networks Kirkpatrick, 2017)
                                - Progressive neural networks (Rusu, 2016)
                                - Sim-to-real robot learning from pixerls with progressive nets (Rusu, 2017)
                            ** modularity -> Can it measure similarity btw NN primitives? -> btw pretrained primitives and aux networks
                                - how well a given partition concentrates the edges within the modules based on the topological connectivity
                                - For Neural Network, ...?
                            - 
                        }
                        - The Architecture of Complexity (Simon, 1962)
                        {
                            - Nearly-decomposable systems
                            - most interactions with subsets close to them, and less with elemtns outside this subset
                            -> then, behavior part should located at the center - require fastest response with all the other processes
                        }
                        - Functional Network Organization of the Human Brain (Power, 2011)
                        {

                        }
                        - Multi-scale brain networks (Betzel, 2017)
                        - The economy of brain network organization (Bullmore, 2012)
                        - Dynamic reconfiguration of human brain networks during learning (Bassett, 2011)
                        - Structural and Functional Brain Networks: From Connections to Cognition (Park, 2013)
                        - Organization and hierarchy of the human functional brain network lead to a chain-like core (Mastrandrea, 2017)
                        - Modeling functional resting-state brain networks through neural message passing on the human connectome (Peraza-Goicolea, 2019)
                        - Deep Learning on Graph-structured Data (Lee, 2019)
                }
                1-5. Value selection
                {
                    Questions:
                        1. Does each of the pretrained primitives requires its value function to be preserved?
                            - 
                        2. Can the value function of the total policy be another new chunk of network?
                            - Maybe
                            - or, Maybe not
                    Do:
                        1. Do all:
                            - 1. Easiest: make whole new value function from scratch as a chunk of parameters
                            - 2. Hard: construct same value function for each pretrained primitives to get total sum of values, but trained from scratch
                            - 3. Harder: construct same value function with total sum of values and import parameters to train for complicated task (Fine-tunable)
                            - 4. Also Harder: same with 3 but parameter be fixed                            
                        2. For 1, get:
                            - total episodic reward
                            - convergence time
                            - environment timesteps
                }
                1-6. Algorithm Formulation
                {
                    Question:
                        1. How to formulate entropy? Just add them all by primitives?
                }
            }
            2. Title: Who cares who I am: Policy transfer between robots without fine-tuning
            {
            }
            6. Title: Fully express the intention
            {
                XAI
            }
    
    Conflicts:
        Task:
            1. Title: Guess What I'm doing: Manipulation of a robot arm with known primitives (XAI related)
            {
                Primitives - control variables can be differ wrt. tasks when using same hardware
                (e.g. reaching tasks given position is redundant for balancing while walking
                    -> motion of arm is required, but not regarding the position of the hand, but to stabilize inertia by moving shoulder joint)
                Task specific primitives:
                    - Controlling arm when reaching / balancing [Q: uses different part of the brain -> and thus, should be trained separately.]
                -> Is our brain made out of cascading weight layers of neurons? -> then, by visualizing each weights to a series of layers can explain the intention of the AI
                -> How does it incorporate external information to the existing layers of neurons?
                -> Physical memory vs Simulated memory
            } 
            
        