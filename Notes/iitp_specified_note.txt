1. 사물을 다루는 것 같은 단순한 작업에서 조차
2. 다양한 감각을 처리
3. 지각과 행동간 상호 작용을 통해 행동 조종을 수행함

1. 데이터 수집
2. generalized 센서/행동 패턴 식별
3. 조작 작업의 패턴으로부터 역강화학습 기술 사용해 보상함수 도출

작업 설정
작업에 맞는 사람 데이터 수집
데이터 수집해서 State representation으로 패턴 인식 (그냥 feature vector extraction)
policy는 HRL로 설정
feature vector를 input으로 하는 primitive policy를 behavior cloning으로 tuning
근데 여기서 primitive는 작업에 도움이 될 것 같은것만 쓰지 말고, 이것 저것 다 학습시켜서 한 번에 때려넣고 학습. coordinator가 알아서 상관 없는 primitive는 안 쓸 것. (예상)

    cf) beta policy는 안 쓸 예정. 지금 reward over/under estimation보단 학습의 안정성이 훨씬 더 필요한 상황.
    gaussian policy를 쓰게 되면, Adam optimzer를 사용하는 것 만으로도 Natural Gradient 방향임. (logp gradient의 2-norm에 대한 moving average를 계산하기 때문에, FIM의 diagonal term을 반영하는 효과가 있는데, Gaussian distribution은 off-diagonal term이 0이기 때문에 똑같게 됨)

전체 작업을 HRL로 쌓아서 behavior cloning 또 돌림, 이 때 primitive는 freeze 하고, coordinator과 aux net만 학습. aux net의 action bound는 restrict해서 보정 하는 정도로만.
feature vector를 argument로 하는 reward function 정형화하여 parameter tuning
이 reward function으로 vanilla policy 상요하여 RL, 그냥 sparse reward scheme이랑 비교. 당연히 잘 됨.

끝.


beta policy를 사용했을 때 NaN이 나는 이유: action은 