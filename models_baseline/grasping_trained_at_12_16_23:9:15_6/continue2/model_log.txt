Layers:
	policy:	[64, 64]
	value:	[64, 64]

gripper angle merged into 1 (pre-training)
touch sensor improved than the pre-training session (continue1)
transfer learning. Only value parameters from pre-training session updated.

Characteristics at 1703:
    Does not show a good performance at the initialization (obvious, because the policy is randomly initialized)
    But it seems to be acting better as it adopts the value, and got out of the stucked trajectory.
    Need: reward comparison to the randomly initialized policy/value net
    
