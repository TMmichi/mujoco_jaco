Layers:
	policy:	[64, 64]
	value:	[64, 64]

gripper angle merged into 1 (pre-training)
touch sensor improved than the pre-training session (continue1)
transfer learning. Only value parameters from pre-training session updated -> from trial6, policy 36350

Characteristics at 1703:
    Does not show a good performance at the initialization (obvious, because the policy is randomly initialized)
    But it seems to be acting better as it adopts the value, and got out of the stucked trajectory.
    Need: reward comparison to the randomly initialized policy/value net
   
Characteristics at 4487:
    Not sure if it is reaching out for the 'object' or for the 'position that object used to be'
    Need: object initialization with more noise
