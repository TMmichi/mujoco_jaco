Layers:
	policy:	[512, 256, 256]
	value:	[512, 256, 256]

trial:		46
action:		linear
tolerance:		0.1
total time steps:		50000000
n_robots:		1
n_targets:		1
episode_length:		2000
reward_method:		target
observation_method:		absolute
Additional Info:		Reset from random initial pos
                    		Agent rotates a bit less
                    		Target stay in position
                    		shallower network
                    		Positive reward
                    		Initial pose a bit inward
                    		PPO MPI
                    		using tanh to squash action
                    		learning_rate: 2e-4, gamma:0.995, n_steps:1024, nminibatches:256, cliprange:0.3
                    		Beta policy with alpha, beta > 1 + mean -> mode
                    		Alpha, beta exponentialized, scaled by 5 within exp
                    		init_scale to 2
                    		alpha and beta from mu and sigma
