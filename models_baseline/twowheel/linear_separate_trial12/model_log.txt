Layers:
	policy:	[256, 256, 128]
	value:	[256, 256, 128]

trial:		12
action:		linear
tolerance:		0.1
total time steps:		10000000
n_robots:		1
n_targets:		1
episode_length:		2000
reward_method:		target
observation_method:		absolute
ent_coef:		0.001
Additional Info:		Does the scaled reward affects scale of mu and log std? -> reward to 0.01*
                      		Does clipping log_std causes any problem when training? e.g. gradient diverging,..
                     		No Squashing
	-> Gradient exploded
