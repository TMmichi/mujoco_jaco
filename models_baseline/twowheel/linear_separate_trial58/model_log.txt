Layers:
	policy:	[512, 256, 256]
	value:	[512, 256, 256]

trial:		58
action:		linear
tolerance:		0.1
total time steps:		10000000
n_robots:		1
n_targets:		1
episode_length:		2000
reward_method:		sparse
observation_method:		absolute
ent_coef:		auto
Additional Info:		Reset from random initial pos
                        		Agent roates a bit less
                        		Target also moves (randomly)
                        		A shaped network
                        		Positive Sparse reward
                        		Initial pose a bit inward
                        		SAC MPI
                        		Policy changed from the Gaussian to Beta
                        		batch_size:2048, buffer_size: 1000000, gamma:0.995, learning_rate: 1e-5
                        		Non-lin: Swish
                        		Train frequency 1 -> 10
                        		weights no biases
