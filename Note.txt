Note

    Problem specification:
        Keywords:
            1. Dexterous manipulation
            2. Strategic (time-dependent) manipulation
            3. Human data learning
            4. Multi-modal sensory data
            5. Safe to interact (torque control)
            6. Robot independent (worspace independent)
            7. Dual (multi) arm
            8. Reaching
            9. Grasping
            10. State representation
            11. Sim-to-Real
            12. Learning from Demonstration
            13. Reusable policies
            14. Explainable actions from policy
            15. Sample efficient learning


        Tasks:
            1. {
                Keywords:
                Title: Guess What I'm doing: Manipulation of a robot arm with known primitives
                Goal: Make policies explainable (and mimick human-like behavior - if incorporating human data while TRAINING PRIMITIVES)
                Motivation:
                    - Shall let the users know what it's doing with human-interpretable information
                    - Behave in a way human brain does -> composing complicated behavior with basic structures of primitives
                        -> ref required
                    - MCPs uses primitives that are not identifiable
                    (WILL NOT USE: - most of the techniques uses joint angles -> restricted to the specific configuration of a robot)
                Contributions:
                    - making policy explainable
                    - Increase sample efficiency (incorporate human data while TRAINING WHOLE TASK)
                    - learn auxilary primitives that have not been told/trained (if possible)
                To do: 
                    - Train primitives that are identified for certain tasks (from either expert trajectory or reward engineered RL scheme)
                    - Set task which requires multiple primitives:
                        Options: {
                            1. {
                                Task: Grap a glass of water and pour to a specific destination
                                Required features (primitive):
                                    - Reaching to a glass (Reaching)
                                    - Make a good grip when holding a glass (Reaching + Grasping)
                                    - Take it to a goal position when holding a glass (Reaching + Grasping + Auxilary - stabilizing glass?)
                                    - Pouring it to a goal position (Reching + Grasping + Auxilary - Pouring)
                            }
                            2. {
                                Task: TBD
                                Goal: TBD
                            }
                        }
                    - State representation learning of required & orthogonal sensor data: image, pressure, joint angles
                        -> want to show that those data has some information that does not overlap with each other
                        -> check orthogonality of each sensors using [PCA? suggestions????]
                    - Provide auxilary dummy primitive networks when training in an End-to-End fashion -> do a PCA analysis for actions under states to validate the need for an extra primitive.
                        -> How to check / avoid a single auxilary network from taking over all the auxiliary primitives?
                             - by training policy with enough of aux policies and then optimize the numbers?
                    - Identify unknown primitives via (examining state/action sets from forward propagations)
                References:
                    - MLSH (Frans, 2017)
                    - MCP (Peng, 2019)
                    - Options (Sutten, 1999)
            }
            2. {
                Keywords: 
                Title: Who cares who I am: Policy transfer between robots without fine-tuning
                Goal: Make policies that can be transfered to different robot models without fine-tuning
                Motivation:
                    - Most of the techniques uses joint angles -> restricted to the specific configuration of a robot
                Contributions:
                    - Can transfer policy without the need of fine-tuning within the simulated env
                    - Can transfer policy to a real robot (if possible)
                To Do:
                    - Use actions as ∆p ∆v of an End Effecor instead of joint angles or joint velocities
                    - Use action/observation scaling with respect to the workbound of each robots (NOT FINETUNING! CAN EASILY BE DONE)
                        (if the scale of jaco : mini jaco = 10 : 1, multiply 10 to each obs/action when using the policy trained from mini jaco to apply to a jaco)
                    - transfer policy without fine-tuning and compare episode rewards at test time
            }
            3. {
                Keywords: 
                Title: Screw-fixing
                Goal: TBD
                Motivation: TBD
            }
            4. {
                Keywords: 
                Title: Plugging with sample-efficient policy by composing primitives
                Goal: TBD
                Motivation: TBD
            }


    Research Details
        Task:
            1. {
                Making proper observations
                    Note: Target position of a reaching primitive - Should explicitly be known to the user
                            -> User do know where it's reaching to

                Sub-primitives
                    Questions:
                        1. Why have sub-primitives?
                            - To construct primitives, observation spaces for each should be matched among all.
                                Reason: each primitives shall have an idea with the full knowledge of a current state
                                Question: Doesn't weight network determine how to composite primitives?
                                Defence: 
                            - But there is no need to train each primitives with full observation spaces
                            where they can acheive their goal with much simpler observations.
                            - pretrain sub-primitives with simple observation spaces, and modify them with 
                            full observation placeholders
                        
                        2. Did X.Peng pretrained their policies with full observation spaces?
                            - Yes (Did not trained their primitives via RL, but by LofD - simplification not required)

                    Actions:
                        -> Need to select:
                            1. RL-trained sub-primitives and finetune them with full obs space
                            2. Not using sub-primitives and policy transfer with expert data directly with full obs space
                    
                    Consequences:
                        1. if select sub-primitives:
                            Csq: Finetune twice for sub-primitives -> primitives and for primitives -> composite policy
                            Result: Complicated/Time consuming to pretrain and finetune twice
                        2.  
                    
                    Defences for Consequences:
                        Def for 1: To avoid complicated / time consumption problem
                            - May not have to finetune for primitives -> composite policies
                            Reason: 
                                1. 

            }
            2. {
                
            }          
    